<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>qudi_hira_analysis.io_handler API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>qudi_hira_analysis.io_handler</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import ast
import datetime
import inspect
import itertools
import pickle
from functools import wraps
from pathlib import Path
from typing import Callable

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import pySPM


class IOHandler:
    &#34;&#34;&#34; Handle all read and write operations. &#34;&#34;&#34;

    def __init__(self, base_read_path: Path = None, base_write_path: Path = None):
        super().__init__()
        self.base_read_path = base_read_path
        self.base_write_path = base_write_path

    @staticmethod
    def _add_base_read_path(func: Callable) -&gt; Callable:
        &#34;&#34;&#34;
        Decorator to add the `base_read_path` to the filepath if it is not None

        Args:
            func: Function to be decorated

        Returns:
            Decorated function
        &#34;&#34;&#34;

        @wraps(func)
        def wrapper(self, filepath: Path, **kwargs):
            if self.base_read_path:
                filepath = self.base_read_path / filepath
            return func(self, filepath, **kwargs)

        return wrapper

    @staticmethod
    def _add_base_write_path(func: Callable) -&gt; Callable:
        &#34;&#34;&#34;
        Decorator to add the `base_write_path` to the filepath if it is not None

        Args:
            func: Function to be decorated

        Returns:
            Decorated function
        &#34;&#34;&#34;

        @wraps(func)
        def wrapper(self, filepath: Path, **kwargs):
            if self.base_write_path:
                filepath = self.base_write_path / filepath
            filepath.parent.mkdir(exist_ok=True)
            return func(self, filepath, **kwargs)

        return wrapper

    @staticmethod
    def _check_extension(ext: str) -&gt; Callable:
        &#34;&#34;&#34;
        Decorator to check the extension of the filepath is correct

        Args:
            ext: Extension to check for

        Returns:
            Decorated function
        &#34;&#34;&#34;

        def decorator(func: Callable) -&gt; Callable:
            @wraps(func)
            def wrapper(self, filepath: Path, **kwargs) -&gt; Callable:
                if filepath.suffix == ext:
                    return func(self, filepath, **kwargs)
                elif filepath.suffix == &#34;&#34;:
                    return func(self, filepath.with_suffix(ext), **kwargs)
                else:
                    raise IOError(f&#34;Invalid extension &#39;{filepath.suffix}&#39; in &#39;{filepath}&#39;, extension should be &#39;{ext}&#39;&#34;)

            return wrapper

        return decorator

    @_add_base_read_path
    @_check_extension(&#34;.dat&#34;)
    def read_qudi_parameters(self, filepath: Path) -&gt; dict:
        &#34;&#34;&#34;Extract parameters from a qudi dat file.

        Args:
            filepath: Path to the qudi .dat file

        Returns:
            Dictionary of parameters
        &#34;&#34;&#34;
        params = {}
        with open(filepath) as file:
            for line in file:
                if line == &#39;#=====\n&#39;:
                    break
                else:
                    # noinspection PyBroadException
                    try:
                        # Remove # from beginning of lines
                        line = line[1:]
                        if line.count(&#34;:&#34;) == 1:
                            # Add params to dictionary
                            label, value = line.split(&#34;:&#34;)
                            if value != &#34;\n&#34;:
                                params[label] = ast.literal_eval(inspect.cleandoc(value))
                        elif line.count(&#34;:&#34;) == 3:
                            # Handle files with timestamps in them
                            label = line.split(&#34;:&#34;)[0]
                            timestamp_str = &#34;&#34;.join(line.split(&#34;:&#34;)[1:]).strip()
                            datetime_str = datetime.datetime.strptime(timestamp_str, &#34;%d.%m.%Y %Hh%Mmin%Ss&#34;).replace(
                                tzinfo=datetime.timezone.utc)
                            params[label] = datetime_str
                    except Exception as _:
                        pass
        return params

    @_add_base_read_path
    @_check_extension(&#34;.dat&#34;)
    def read_into_dataframe(self, filepath: Path) -&gt; pd.DataFrame:
        &#34;&#34;&#34;Read a qudi data file into a pandas DataFrame for analysis.

        Args:
            filepath: Path to the qudi data file

        Returns:
            DataFrame containing the data from the qudi data file
        &#34;&#34;&#34;
        with open(filepath) as handle:
            # Generate column names for DataFrame by parsing the file
            *_comments, names = itertools.takewhile(lambda line: line.startswith(&#39;#&#39;), handle)
            names = names[1:].strip().split(&#34;\t&#34;)
        return pd.read_csv(filepath, names=names, comment=&#34;#&#34;, sep=&#34;\t&#34;)

    @_add_base_read_path
    def read_csv(self, filepath: Path, **kwargs) -&gt; pd.DataFrame:
        &#34;&#34;&#34; Read a csv file into a pandas DataFrame. &#34;&#34;&#34;
        return pd.read_csv(filepath, **kwargs)

    @_add_base_read_path
    def read_excel(self, filepath: Path, **kwargs) -&gt; pd.DataFrame:
        &#34;&#34;&#34; Read a csv file into a pandas DataFrame. &#34;&#34;&#34;
        return pd.read_excel(filepath, **kwargs)

    @_add_base_read_path
    @_check_extension(&#34;.dat&#34;)
    def read_confocal_into_dataframe(self, filepath: Path) -&gt; pd.DataFrame:
        &#34;&#34;&#34; Read a qudi confocal data file into a pandas DataFrame for analysis. &#34;&#34;&#34;
        confocal_params = self.read_qudi_parameters(filepath)
        data = self.read_into_ndarray(filepath, delimiter=&#34;\t&#34;)
        # Use the confocal parameters to generate the index and columns for the DataFrame
        index = np.linspace(
            confocal_params[&#39;X image min (m)&#39;],
            confocal_params[&#39;X image max (m)&#39;],
            data.shape[0]
        )
        columns = np.linspace(
            confocal_params[&#39;Y image min&#39;],
            confocal_params[&#39;Y image max&#39;],
            data.shape[1]
        )
        df = pd.DataFrame(data, index=index, columns=columns)
        # Sort the index to get origin (0, 0) in the lower left corner of the DataFrame
        df.sort_index(axis=0, ascending=False, inplace=True)
        return df

    @_add_base_read_path
    def read_into_ndarray(self, filepath: Path, **kwargs) -&gt; np.ndarray:
        &#34;&#34;&#34; Read a file into a numpy ndarray. &#34;&#34;&#34;
        return np.genfromtxt(filepath, **kwargs)

    @_add_base_read_path
    def read_into_ndarray_transposed(self, filepath: Path, **kwargs) -&gt; np.ndarray:
        &#34;&#34;&#34; Read a file into a transposed numpy ndarray. &#34;&#34;&#34;
        return np.genfromtxt(filepath, **kwargs).T

    @_add_base_read_path
    @_check_extension(&#34;.pys&#34;)
    def read_pys(self, filepath: Path) -&gt; dict:
        &#34;&#34;&#34; Read raw .pys data files into a dictionary. &#34;&#34;&#34;
        byte_dict = np.load(str(filepath), encoding=&#34;bytes&#34;, allow_pickle=True)
        # Convert byte string keys to normal strings
        return {key.decode(&#39;utf8&#39;): byte_dict.get(key) for key in byte_dict.keys()}

    @_add_base_read_path
    @_check_extension(&#34;.pkl&#34;)
    def read_pkl(self, filepath: Path) -&gt; dict:
        &#34;&#34;&#34; Read pickle files into a dictionary. &#34;&#34;&#34;
        with open(filepath, &#39;rb&#39;) as f:
            file = pickle.load(f)
        return file

    @_add_base_read_path
    @_check_extension(&#34;.dat&#34;)
    def read_nanonis_data(self, filepath: Path) -&gt; pd.DataFrame:
        &#34;&#34;&#34;Read data from a Nanonis .dat file.

        Args:
            filepath: Path to the Nanonis .dat file.

        Returns:
            DataFrame of data.
        &#34;&#34;&#34;
        skip_rows = 0
        with open(filepath) as dat_file:
            for num, line in enumerate(dat_file, 1):
                if &#34;[DATA]&#34; in line:
                    # Find number of rows to skip when extracting data
                    skip_rows = num
                    break
                if &#34;#=====&#34; in line:
                    skip_rows = num
                    break
        df = pd.read_table(filepath, sep=&#34;\t&#34;, skiprows=skip_rows)
        return df

    @_add_base_read_path
    @_check_extension(&#34;.dat&#34;)
    def read_nanonis_parameters(self, filepath: Path) -&gt; dict:
        &#34;&#34;&#34;Read parameters from a Nanonis .dat file.

        Args:
            filepath: Path to the Nanonis .dat file.

        Returns:
            Dictionary of parameters.
        &#34;&#34;&#34;
        parameters = {}
        with open(filepath) as dat_file:
            for line in dat_file:
                if line == &#34;\n&#34;:
                    # Break when reaching empty line
                    break
                elif &#34;User&#34; in line or line.split(&#34;\t&#34;)[0] == &#34;&#34;:
                    # Cleanup excess parameters and skip empty lines
                    pass
                else:
                    label, value, _ = line.split(&#34;\t&#34;)
                    try:
                        # Convert strings to number where possible
                        value = float(value)
                    except ValueError:
                        pass
                    if &#34;Oscillation Control&gt;&#34; in label:
                        label = label.replace(&#34;Oscillation Control&gt;&#34;, &#34;&#34;)
                    parameters[label] = value
        return parameters

    @_add_base_read_path
    @_check_extension(&#34;.sxm&#34;)
    def read_nanonis_spm_data(self, filepath: Path) -&gt; pySPM.SXM:
        &#34;&#34;&#34;Read a Nanonis .sxm data file.

        Args:
            filepath: Path to the .sxm file.

        Returns:
            pySPM.SXM object containing the data.
        &#34;&#34;&#34;
        return pySPM.SXM(filepath)

    @_add_base_read_path
    @_check_extension(&#34;.001&#34;)
    def read_bruker_spm_data(self, filepath: Path) -&gt; pySPM.Bruker:
        &#34;&#34;&#34;Read a Bruker SPM data file.

        Args:
            filepath: Path to the .001 file.

        Returns:
            pySPM.Bruker object containing the data.
        &#34;&#34;&#34;
        return pySPM.Bruker(filepath)

    @_add_base_read_path
    @_check_extension(&#34;.txt&#34;)
    def read_pfeiffer_data(self, filepath: Path) -&gt; pd.DataFrame:
        &#34;&#34;&#34;Read data stored by Pfeiffer vacuum monitoring software.

        Args:
            filepath: Path to the text file.

        Returns:
            DataFrame containing the data.
        &#34;&#34;&#34;
        # Extract rows including the header
        df = pd.read_csv(filepath, sep=&#34;\t&#34;, skiprows=[0, 2, 3, 4])
        # Combine data and time columns together
        df[&#34;Date&#34;] = df[&#34;Date&#34;] + &#34; &#34; + df[&#34;Time&#34;]
        df = df.drop(&#34;Time&#34;, axis=1)
        # Infer datetime format and convert to datetime objects
        df[&#34;Date&#34;] = pd.to_datetime(df[&#34;Date&#34;], infer_datetime_format=True)
        # Set datetime as index
        df = df.set_index(&#34;Date&#34;, drop=True)
        return df

    @_add_base_read_path
    @_check_extension(&#34;.xls&#34;)
    def read_lakeshore_data(self, filepath: Path) -&gt; pd.DataFrame:
        &#34;&#34;&#34;Read data stored by Lakeshore temperature monitor software.

        Args:
            filepath: Path to the Excel file.

        Returns:
            DataFrame containing the data.
        &#34;&#34;&#34;
        # Extract only the origin timestamp
        origin = pd.read_excel(filepath, skiprows=1, nrows=1, usecols=[1], header=None)[1][0]
        # Remove any tzinfo to prevent future exceptions in pandas
        origin = origin.replace(&#34;CET&#34;, &#34;&#34;)
        # Parse datetime object from timestamp
        origin = pd.to_datetime(origin)
        # Create DataFrame and drop empty cols
        df = pd.read_excel(filepath, skiprows=3)
        df = df.dropna(axis=1, how=&#34;all&#34;)
        # Add datetimes to DataFrame
        df[&#34;Datetime&#34;] = pd.to_datetime(df[&#34;Time&#34;], unit=&#34;ms&#34;, origin=origin)
        df = df.drop(&#34;Time&#34;, axis=1)
        # Set datetime as index
        df = df.set_index(&#34;Datetime&#34;, drop=True)
        return df

    @_add_base_read_path
    @_check_extension(&#34;.txt&#34;)
    def read_oceanoptics_data(self, filepath: str) -&gt; pd.DataFrame:
        &#34;&#34;&#34;Read spectrometer data from OceanOptics spectrometer.

        Args:
            filepath: Path to the data file.

        Returns:
            DataFrame containing the wavelength and intensity data.
        &#34;&#34;&#34;
        df = pd.read_csv(filepath, sep=&#34;\t&#34;, skiprows=14, names=[&#34;wavelength&#34;, &#34;intensity&#34;])
        return df

    @staticmethod
    def __get_forward_backward_counts(count_rates, num_pixels):
        split_array = np.split(count_rates, 2 * num_pixels)
        # Extract forward scan array as every second element
        forward_counts = np.stack(split_array[::2])
        # Extract backward scan array as every shifted second element
        # Flip scan so that backward and forward scans represent the same data
        backward_counts = np.flip(np.stack(split_array[1::2]), axis=1)
        return forward_counts, backward_counts

    def read_pixelscanner_data(self, filepath: Path) -&gt; (pySPM.SPM_image, pySPM.SPM_image):
        &#34;&#34;&#34; Read data from a PixelScanner measurement.

        Args:
            filepath: Path to the data file.

        Returns:
            Forward and backward scan data.
        &#34;&#34;&#34;
        df = self.read_into_dataframe(filepath)
        num_pixels = int(np.sqrt(len(df) // 2))

        if num_pixels ** 2 != len(df) // 2:
            raise ValueError(&#34;Number of pixels does not match data length.&#34;)

        try:
            fwd, bwd = self.__get_forward_backward_counts(df[&#34;count_rates&#34;], num_pixels)
        except KeyError:
            try:
                fwd, bwd = self.__get_forward_backward_counts(df[&#34;Count Rates (cps)&#34;], num_pixels)
            except KeyError:
                # Support old data format
                fwd = df[&#34;forward (cps)&#34;].to_numpy().reshape(num_pixels, num_pixels)
                bwd = df[&#34;backward (cps)&#34;].to_numpy().reshape(num_pixels, num_pixels)

        fwd = pySPM.SPM_image(fwd, channel=&#34;Forward&#34;, _type=&#34;NV-PL&#34;)
        bwd = pySPM.SPM_image(bwd, channel=&#34;Backward&#34;, _type=&#34;NV-PL&#34;)
        return fwd, bwd

    @_add_base_write_path
    @_check_extension(&#34;.pkl&#34;)
    def save_pkl(self, filepath: Path, obj: object):
        &#34;&#34;&#34;Saves pickle files.

        Args:
            filepath: Path to the data file.
            obj: Object to be saved.
        &#34;&#34;&#34;
        with open(filepath, &#39;wb&#39;) as f:
            pickle.dump(obj, f)

    @_add_base_write_path
    @_check_extension(&#34;.pys&#34;)
    def save_pys(self, filepath: Path, dictionary: dict):
        &#34;&#34;&#34;Saves .pys files.

        Args:
            filepath: Path to the data file.
            dictionary: Dictionary to be saved.
        &#34;&#34;&#34;
        with open(filepath, &#39;wb&#39;) as f:
            pickle.dump(dictionary, f, 1)

    @_add_base_write_path
    @_check_extension(&#34;.pys&#34;)
    def save_df(self, filepath: Path, df: pd.DataFrame):
        &#34;&#34;&#34; Save Dataframe as csv. &#34;&#34;&#34;
        df.to_csv(filepath, sep=&#39;\t&#39;, encoding=&#39;utf-8&#39;)

    @_add_base_write_path
    def save_figures(self, filepath: Path, fig: plt.Figure, **kwargs):
        &#34;&#34;&#34;Saves figures from matplotlib plot data.

        By default, saves as jpg, png, pdf and svg.

        Args:
            fig: Matplotlib figure to save.
            filepath: Name of figure to save.
            only_jpg: If True, only save as jpg (default: False).
            only_pdf: If True, only save as pdf (default: False).
            **kwargs: Keyword arguments passed to fig.savefig().
        &#34;&#34;&#34;
        extensions = None
        if &#34;only_jpg&#34; in kwargs:
            if kwargs.get(&#34;only_jpg&#34;):
                extensions = [&#34;.jpg&#34;]
            kwargs.pop(&#34;only_jpg&#34;, None)
        elif &#34;only_pdf&#34; in kwargs:
            if kwargs.get(&#34;only_pdf&#34;):
                extensions = [&#34;.pdf&#34;]
            kwargs.pop(&#34;only_pdf&#34;, None)
        else:
            extensions = [&#34;.jpg&#34;, &#34;.pdf&#34;, &#34;.svg&#34;, &#34;.png&#34;]

        for ext in extensions:
            fig.savefig(filepath.with_suffix(ext), dpi=200, **kwargs)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="qudi_hira_analysis.io_handler.IOHandler"><code class="flex name class">
<span>class <span class="ident">IOHandler</span></span>
<span>(</span><span>base_read_path: pathlib.Path = None, base_write_path: pathlib.Path = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Handle all read and write operations.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class IOHandler:
    &#34;&#34;&#34; Handle all read and write operations. &#34;&#34;&#34;

    def __init__(self, base_read_path: Path = None, base_write_path: Path = None):
        super().__init__()
        self.base_read_path = base_read_path
        self.base_write_path = base_write_path

    @staticmethod
    def _add_base_read_path(func: Callable) -&gt; Callable:
        &#34;&#34;&#34;
        Decorator to add the `base_read_path` to the filepath if it is not None

        Args:
            func: Function to be decorated

        Returns:
            Decorated function
        &#34;&#34;&#34;

        @wraps(func)
        def wrapper(self, filepath: Path, **kwargs):
            if self.base_read_path:
                filepath = self.base_read_path / filepath
            return func(self, filepath, **kwargs)

        return wrapper

    @staticmethod
    def _add_base_write_path(func: Callable) -&gt; Callable:
        &#34;&#34;&#34;
        Decorator to add the `base_write_path` to the filepath if it is not None

        Args:
            func: Function to be decorated

        Returns:
            Decorated function
        &#34;&#34;&#34;

        @wraps(func)
        def wrapper(self, filepath: Path, **kwargs):
            if self.base_write_path:
                filepath = self.base_write_path / filepath
            filepath.parent.mkdir(exist_ok=True)
            return func(self, filepath, **kwargs)

        return wrapper

    @staticmethod
    def _check_extension(ext: str) -&gt; Callable:
        &#34;&#34;&#34;
        Decorator to check the extension of the filepath is correct

        Args:
            ext: Extension to check for

        Returns:
            Decorated function
        &#34;&#34;&#34;

        def decorator(func: Callable) -&gt; Callable:
            @wraps(func)
            def wrapper(self, filepath: Path, **kwargs) -&gt; Callable:
                if filepath.suffix == ext:
                    return func(self, filepath, **kwargs)
                elif filepath.suffix == &#34;&#34;:
                    return func(self, filepath.with_suffix(ext), **kwargs)
                else:
                    raise IOError(f&#34;Invalid extension &#39;{filepath.suffix}&#39; in &#39;{filepath}&#39;, extension should be &#39;{ext}&#39;&#34;)

            return wrapper

        return decorator

    @_add_base_read_path
    @_check_extension(&#34;.dat&#34;)
    def read_qudi_parameters(self, filepath: Path) -&gt; dict:
        &#34;&#34;&#34;Extract parameters from a qudi dat file.

        Args:
            filepath: Path to the qudi .dat file

        Returns:
            Dictionary of parameters
        &#34;&#34;&#34;
        params = {}
        with open(filepath) as file:
            for line in file:
                if line == &#39;#=====\n&#39;:
                    break
                else:
                    # noinspection PyBroadException
                    try:
                        # Remove # from beginning of lines
                        line = line[1:]
                        if line.count(&#34;:&#34;) == 1:
                            # Add params to dictionary
                            label, value = line.split(&#34;:&#34;)
                            if value != &#34;\n&#34;:
                                params[label] = ast.literal_eval(inspect.cleandoc(value))
                        elif line.count(&#34;:&#34;) == 3:
                            # Handle files with timestamps in them
                            label = line.split(&#34;:&#34;)[0]
                            timestamp_str = &#34;&#34;.join(line.split(&#34;:&#34;)[1:]).strip()
                            datetime_str = datetime.datetime.strptime(timestamp_str, &#34;%d.%m.%Y %Hh%Mmin%Ss&#34;).replace(
                                tzinfo=datetime.timezone.utc)
                            params[label] = datetime_str
                    except Exception as _:
                        pass
        return params

    @_add_base_read_path
    @_check_extension(&#34;.dat&#34;)
    def read_into_dataframe(self, filepath: Path) -&gt; pd.DataFrame:
        &#34;&#34;&#34;Read a qudi data file into a pandas DataFrame for analysis.

        Args:
            filepath: Path to the qudi data file

        Returns:
            DataFrame containing the data from the qudi data file
        &#34;&#34;&#34;
        with open(filepath) as handle:
            # Generate column names for DataFrame by parsing the file
            *_comments, names = itertools.takewhile(lambda line: line.startswith(&#39;#&#39;), handle)
            names = names[1:].strip().split(&#34;\t&#34;)
        return pd.read_csv(filepath, names=names, comment=&#34;#&#34;, sep=&#34;\t&#34;)

    @_add_base_read_path
    def read_csv(self, filepath: Path, **kwargs) -&gt; pd.DataFrame:
        &#34;&#34;&#34; Read a csv file into a pandas DataFrame. &#34;&#34;&#34;
        return pd.read_csv(filepath, **kwargs)

    @_add_base_read_path
    def read_excel(self, filepath: Path, **kwargs) -&gt; pd.DataFrame:
        &#34;&#34;&#34; Read a csv file into a pandas DataFrame. &#34;&#34;&#34;
        return pd.read_excel(filepath, **kwargs)

    @_add_base_read_path
    @_check_extension(&#34;.dat&#34;)
    def read_confocal_into_dataframe(self, filepath: Path) -&gt; pd.DataFrame:
        &#34;&#34;&#34; Read a qudi confocal data file into a pandas DataFrame for analysis. &#34;&#34;&#34;
        confocal_params = self.read_qudi_parameters(filepath)
        data = self.read_into_ndarray(filepath, delimiter=&#34;\t&#34;)
        # Use the confocal parameters to generate the index and columns for the DataFrame
        index = np.linspace(
            confocal_params[&#39;X image min (m)&#39;],
            confocal_params[&#39;X image max (m)&#39;],
            data.shape[0]
        )
        columns = np.linspace(
            confocal_params[&#39;Y image min&#39;],
            confocal_params[&#39;Y image max&#39;],
            data.shape[1]
        )
        df = pd.DataFrame(data, index=index, columns=columns)
        # Sort the index to get origin (0, 0) in the lower left corner of the DataFrame
        df.sort_index(axis=0, ascending=False, inplace=True)
        return df

    @_add_base_read_path
    def read_into_ndarray(self, filepath: Path, **kwargs) -&gt; np.ndarray:
        &#34;&#34;&#34; Read a file into a numpy ndarray. &#34;&#34;&#34;
        return np.genfromtxt(filepath, **kwargs)

    @_add_base_read_path
    def read_into_ndarray_transposed(self, filepath: Path, **kwargs) -&gt; np.ndarray:
        &#34;&#34;&#34; Read a file into a transposed numpy ndarray. &#34;&#34;&#34;
        return np.genfromtxt(filepath, **kwargs).T

    @_add_base_read_path
    @_check_extension(&#34;.pys&#34;)
    def read_pys(self, filepath: Path) -&gt; dict:
        &#34;&#34;&#34; Read raw .pys data files into a dictionary. &#34;&#34;&#34;
        byte_dict = np.load(str(filepath), encoding=&#34;bytes&#34;, allow_pickle=True)
        # Convert byte string keys to normal strings
        return {key.decode(&#39;utf8&#39;): byte_dict.get(key) for key in byte_dict.keys()}

    @_add_base_read_path
    @_check_extension(&#34;.pkl&#34;)
    def read_pkl(self, filepath: Path) -&gt; dict:
        &#34;&#34;&#34; Read pickle files into a dictionary. &#34;&#34;&#34;
        with open(filepath, &#39;rb&#39;) as f:
            file = pickle.load(f)
        return file

    @_add_base_read_path
    @_check_extension(&#34;.dat&#34;)
    def read_nanonis_data(self, filepath: Path) -&gt; pd.DataFrame:
        &#34;&#34;&#34;Read data from a Nanonis .dat file.

        Args:
            filepath: Path to the Nanonis .dat file.

        Returns:
            DataFrame of data.
        &#34;&#34;&#34;
        skip_rows = 0
        with open(filepath) as dat_file:
            for num, line in enumerate(dat_file, 1):
                if &#34;[DATA]&#34; in line:
                    # Find number of rows to skip when extracting data
                    skip_rows = num
                    break
                if &#34;#=====&#34; in line:
                    skip_rows = num
                    break
        df = pd.read_table(filepath, sep=&#34;\t&#34;, skiprows=skip_rows)
        return df

    @_add_base_read_path
    @_check_extension(&#34;.dat&#34;)
    def read_nanonis_parameters(self, filepath: Path) -&gt; dict:
        &#34;&#34;&#34;Read parameters from a Nanonis .dat file.

        Args:
            filepath: Path to the Nanonis .dat file.

        Returns:
            Dictionary of parameters.
        &#34;&#34;&#34;
        parameters = {}
        with open(filepath) as dat_file:
            for line in dat_file:
                if line == &#34;\n&#34;:
                    # Break when reaching empty line
                    break
                elif &#34;User&#34; in line or line.split(&#34;\t&#34;)[0] == &#34;&#34;:
                    # Cleanup excess parameters and skip empty lines
                    pass
                else:
                    label, value, _ = line.split(&#34;\t&#34;)
                    try:
                        # Convert strings to number where possible
                        value = float(value)
                    except ValueError:
                        pass
                    if &#34;Oscillation Control&gt;&#34; in label:
                        label = label.replace(&#34;Oscillation Control&gt;&#34;, &#34;&#34;)
                    parameters[label] = value
        return parameters

    @_add_base_read_path
    @_check_extension(&#34;.sxm&#34;)
    def read_nanonis_spm_data(self, filepath: Path) -&gt; pySPM.SXM:
        &#34;&#34;&#34;Read a Nanonis .sxm data file.

        Args:
            filepath: Path to the .sxm file.

        Returns:
            pySPM.SXM object containing the data.
        &#34;&#34;&#34;
        return pySPM.SXM(filepath)

    @_add_base_read_path
    @_check_extension(&#34;.001&#34;)
    def read_bruker_spm_data(self, filepath: Path) -&gt; pySPM.Bruker:
        &#34;&#34;&#34;Read a Bruker SPM data file.

        Args:
            filepath: Path to the .001 file.

        Returns:
            pySPM.Bruker object containing the data.
        &#34;&#34;&#34;
        return pySPM.Bruker(filepath)

    @_add_base_read_path
    @_check_extension(&#34;.txt&#34;)
    def read_pfeiffer_data(self, filepath: Path) -&gt; pd.DataFrame:
        &#34;&#34;&#34;Read data stored by Pfeiffer vacuum monitoring software.

        Args:
            filepath: Path to the text file.

        Returns:
            DataFrame containing the data.
        &#34;&#34;&#34;
        # Extract rows including the header
        df = pd.read_csv(filepath, sep=&#34;\t&#34;, skiprows=[0, 2, 3, 4])
        # Combine data and time columns together
        df[&#34;Date&#34;] = df[&#34;Date&#34;] + &#34; &#34; + df[&#34;Time&#34;]
        df = df.drop(&#34;Time&#34;, axis=1)
        # Infer datetime format and convert to datetime objects
        df[&#34;Date&#34;] = pd.to_datetime(df[&#34;Date&#34;], infer_datetime_format=True)
        # Set datetime as index
        df = df.set_index(&#34;Date&#34;, drop=True)
        return df

    @_add_base_read_path
    @_check_extension(&#34;.xls&#34;)
    def read_lakeshore_data(self, filepath: Path) -&gt; pd.DataFrame:
        &#34;&#34;&#34;Read data stored by Lakeshore temperature monitor software.

        Args:
            filepath: Path to the Excel file.

        Returns:
            DataFrame containing the data.
        &#34;&#34;&#34;
        # Extract only the origin timestamp
        origin = pd.read_excel(filepath, skiprows=1, nrows=1, usecols=[1], header=None)[1][0]
        # Remove any tzinfo to prevent future exceptions in pandas
        origin = origin.replace(&#34;CET&#34;, &#34;&#34;)
        # Parse datetime object from timestamp
        origin = pd.to_datetime(origin)
        # Create DataFrame and drop empty cols
        df = pd.read_excel(filepath, skiprows=3)
        df = df.dropna(axis=1, how=&#34;all&#34;)
        # Add datetimes to DataFrame
        df[&#34;Datetime&#34;] = pd.to_datetime(df[&#34;Time&#34;], unit=&#34;ms&#34;, origin=origin)
        df = df.drop(&#34;Time&#34;, axis=1)
        # Set datetime as index
        df = df.set_index(&#34;Datetime&#34;, drop=True)
        return df

    @_add_base_read_path
    @_check_extension(&#34;.txt&#34;)
    def read_oceanoptics_data(self, filepath: str) -&gt; pd.DataFrame:
        &#34;&#34;&#34;Read spectrometer data from OceanOptics spectrometer.

        Args:
            filepath: Path to the data file.

        Returns:
            DataFrame containing the wavelength and intensity data.
        &#34;&#34;&#34;
        df = pd.read_csv(filepath, sep=&#34;\t&#34;, skiprows=14, names=[&#34;wavelength&#34;, &#34;intensity&#34;])
        return df

    @staticmethod
    def __get_forward_backward_counts(count_rates, num_pixels):
        split_array = np.split(count_rates, 2 * num_pixels)
        # Extract forward scan array as every second element
        forward_counts = np.stack(split_array[::2])
        # Extract backward scan array as every shifted second element
        # Flip scan so that backward and forward scans represent the same data
        backward_counts = np.flip(np.stack(split_array[1::2]), axis=1)
        return forward_counts, backward_counts

    def read_pixelscanner_data(self, filepath: Path) -&gt; (pySPM.SPM_image, pySPM.SPM_image):
        &#34;&#34;&#34; Read data from a PixelScanner measurement.

        Args:
            filepath: Path to the data file.

        Returns:
            Forward and backward scan data.
        &#34;&#34;&#34;
        df = self.read_into_dataframe(filepath)
        num_pixels = int(np.sqrt(len(df) // 2))

        if num_pixels ** 2 != len(df) // 2:
            raise ValueError(&#34;Number of pixels does not match data length.&#34;)

        try:
            fwd, bwd = self.__get_forward_backward_counts(df[&#34;count_rates&#34;], num_pixels)
        except KeyError:
            try:
                fwd, bwd = self.__get_forward_backward_counts(df[&#34;Count Rates (cps)&#34;], num_pixels)
            except KeyError:
                # Support old data format
                fwd = df[&#34;forward (cps)&#34;].to_numpy().reshape(num_pixels, num_pixels)
                bwd = df[&#34;backward (cps)&#34;].to_numpy().reshape(num_pixels, num_pixels)

        fwd = pySPM.SPM_image(fwd, channel=&#34;Forward&#34;, _type=&#34;NV-PL&#34;)
        bwd = pySPM.SPM_image(bwd, channel=&#34;Backward&#34;, _type=&#34;NV-PL&#34;)
        return fwd, bwd

    @_add_base_write_path
    @_check_extension(&#34;.pkl&#34;)
    def save_pkl(self, filepath: Path, obj: object):
        &#34;&#34;&#34;Saves pickle files.

        Args:
            filepath: Path to the data file.
            obj: Object to be saved.
        &#34;&#34;&#34;
        with open(filepath, &#39;wb&#39;) as f:
            pickle.dump(obj, f)

    @_add_base_write_path
    @_check_extension(&#34;.pys&#34;)
    def save_pys(self, filepath: Path, dictionary: dict):
        &#34;&#34;&#34;Saves .pys files.

        Args:
            filepath: Path to the data file.
            dictionary: Dictionary to be saved.
        &#34;&#34;&#34;
        with open(filepath, &#39;wb&#39;) as f:
            pickle.dump(dictionary, f, 1)

    @_add_base_write_path
    @_check_extension(&#34;.pys&#34;)
    def save_df(self, filepath: Path, df: pd.DataFrame):
        &#34;&#34;&#34; Save Dataframe as csv. &#34;&#34;&#34;
        df.to_csv(filepath, sep=&#39;\t&#39;, encoding=&#39;utf-8&#39;)

    @_add_base_write_path
    def save_figures(self, filepath: Path, fig: plt.Figure, **kwargs):
        &#34;&#34;&#34;Saves figures from matplotlib plot data.

        By default, saves as jpg, png, pdf and svg.

        Args:
            fig: Matplotlib figure to save.
            filepath: Name of figure to save.
            only_jpg: If True, only save as jpg (default: False).
            only_pdf: If True, only save as pdf (default: False).
            **kwargs: Keyword arguments passed to fig.savefig().
        &#34;&#34;&#34;
        extensions = None
        if &#34;only_jpg&#34; in kwargs:
            if kwargs.get(&#34;only_jpg&#34;):
                extensions = [&#34;.jpg&#34;]
            kwargs.pop(&#34;only_jpg&#34;, None)
        elif &#34;only_pdf&#34; in kwargs:
            if kwargs.get(&#34;only_pdf&#34;):
                extensions = [&#34;.pdf&#34;]
            kwargs.pop(&#34;only_pdf&#34;, None)
        else:
            extensions = [&#34;.jpg&#34;, &#34;.pdf&#34;, &#34;.svg&#34;, &#34;.png&#34;]

        for ext in extensions:
            fig.savefig(filepath.with_suffix(ext), dpi=200, **kwargs)</code></pre>
</details>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="qudi_hira_analysis.data_handler.DataLoader" href="data_handler.html#qudi_hira_analysis.data_handler.DataLoader">DataLoader</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="qudi_hira_analysis.io_handler.IOHandler.read_bruker_spm_data"><code class="name flex">
<span>def <span class="ident">read_bruker_spm_data</span></span>(<span>self, filepath: pathlib.Path) ‑> pySPM.Bruker.Bruker</span>
</code></dt>
<dd>
<div class="desc"><p>Read a Bruker SPM data file.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>filepath</code></strong></dt>
<dd>Path to the .001 file.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>pySPM.Bruker object containing the data.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@_add_base_read_path
@_check_extension(&#34;.001&#34;)
def read_bruker_spm_data(self, filepath: Path) -&gt; pySPM.Bruker:
    &#34;&#34;&#34;Read a Bruker SPM data file.

    Args:
        filepath: Path to the .001 file.

    Returns:
        pySPM.Bruker object containing the data.
    &#34;&#34;&#34;
    return pySPM.Bruker(filepath)</code></pre>
</details>
</dd>
<dt id="qudi_hira_analysis.io_handler.IOHandler.read_confocal_into_dataframe"><code class="name flex">
<span>def <span class="ident">read_confocal_into_dataframe</span></span>(<span>self, filepath: pathlib.Path) ‑> pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Read a qudi confocal data file into a pandas DataFrame for analysis.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@_add_base_read_path
@_check_extension(&#34;.dat&#34;)
def read_confocal_into_dataframe(self, filepath: Path) -&gt; pd.DataFrame:
    &#34;&#34;&#34; Read a qudi confocal data file into a pandas DataFrame for analysis. &#34;&#34;&#34;
    confocal_params = self.read_qudi_parameters(filepath)
    data = self.read_into_ndarray(filepath, delimiter=&#34;\t&#34;)
    # Use the confocal parameters to generate the index and columns for the DataFrame
    index = np.linspace(
        confocal_params[&#39;X image min (m)&#39;],
        confocal_params[&#39;X image max (m)&#39;],
        data.shape[0]
    )
    columns = np.linspace(
        confocal_params[&#39;Y image min&#39;],
        confocal_params[&#39;Y image max&#39;],
        data.shape[1]
    )
    df = pd.DataFrame(data, index=index, columns=columns)
    # Sort the index to get origin (0, 0) in the lower left corner of the DataFrame
    df.sort_index(axis=0, ascending=False, inplace=True)
    return df</code></pre>
</details>
</dd>
<dt id="qudi_hira_analysis.io_handler.IOHandler.read_csv"><code class="name flex">
<span>def <span class="ident">read_csv</span></span>(<span>self, filepath: pathlib.Path, **kwargs) ‑> pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Read a csv file into a pandas DataFrame.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@_add_base_read_path
def read_csv(self, filepath: Path, **kwargs) -&gt; pd.DataFrame:
    &#34;&#34;&#34; Read a csv file into a pandas DataFrame. &#34;&#34;&#34;
    return pd.read_csv(filepath, **kwargs)</code></pre>
</details>
</dd>
<dt id="qudi_hira_analysis.io_handler.IOHandler.read_excel"><code class="name flex">
<span>def <span class="ident">read_excel</span></span>(<span>self, filepath: pathlib.Path, **kwargs) ‑> pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Read a csv file into a pandas DataFrame.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@_add_base_read_path
def read_excel(self, filepath: Path, **kwargs) -&gt; pd.DataFrame:
    &#34;&#34;&#34; Read a csv file into a pandas DataFrame. &#34;&#34;&#34;
    return pd.read_excel(filepath, **kwargs)</code></pre>
</details>
</dd>
<dt id="qudi_hira_analysis.io_handler.IOHandler.read_into_dataframe"><code class="name flex">
<span>def <span class="ident">read_into_dataframe</span></span>(<span>self, filepath: pathlib.Path) ‑> pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Read a qudi data file into a pandas DataFrame for analysis.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>filepath</code></strong></dt>
<dd>Path to the qudi data file</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>DataFrame containing the data from the qudi data file</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@_add_base_read_path
@_check_extension(&#34;.dat&#34;)
def read_into_dataframe(self, filepath: Path) -&gt; pd.DataFrame:
    &#34;&#34;&#34;Read a qudi data file into a pandas DataFrame for analysis.

    Args:
        filepath: Path to the qudi data file

    Returns:
        DataFrame containing the data from the qudi data file
    &#34;&#34;&#34;
    with open(filepath) as handle:
        # Generate column names for DataFrame by parsing the file
        *_comments, names = itertools.takewhile(lambda line: line.startswith(&#39;#&#39;), handle)
        names = names[1:].strip().split(&#34;\t&#34;)
    return pd.read_csv(filepath, names=names, comment=&#34;#&#34;, sep=&#34;\t&#34;)</code></pre>
</details>
</dd>
<dt id="qudi_hira_analysis.io_handler.IOHandler.read_into_ndarray"><code class="name flex">
<span>def <span class="ident">read_into_ndarray</span></span>(<span>self, filepath: pathlib.Path, **kwargs) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Read a file into a numpy ndarray.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@_add_base_read_path
def read_into_ndarray(self, filepath: Path, **kwargs) -&gt; np.ndarray:
    &#34;&#34;&#34; Read a file into a numpy ndarray. &#34;&#34;&#34;
    return np.genfromtxt(filepath, **kwargs)</code></pre>
</details>
</dd>
<dt id="qudi_hira_analysis.io_handler.IOHandler.read_into_ndarray_transposed"><code class="name flex">
<span>def <span class="ident">read_into_ndarray_transposed</span></span>(<span>self, filepath: pathlib.Path, **kwargs) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Read a file into a transposed numpy ndarray.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@_add_base_read_path
def read_into_ndarray_transposed(self, filepath: Path, **kwargs) -&gt; np.ndarray:
    &#34;&#34;&#34; Read a file into a transposed numpy ndarray. &#34;&#34;&#34;
    return np.genfromtxt(filepath, **kwargs).T</code></pre>
</details>
</dd>
<dt id="qudi_hira_analysis.io_handler.IOHandler.read_lakeshore_data"><code class="name flex">
<span>def <span class="ident">read_lakeshore_data</span></span>(<span>self, filepath: pathlib.Path) ‑> pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Read data stored by Lakeshore temperature monitor software.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>filepath</code></strong></dt>
<dd>Path to the Excel file.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>DataFrame containing the data.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@_add_base_read_path
@_check_extension(&#34;.xls&#34;)
def read_lakeshore_data(self, filepath: Path) -&gt; pd.DataFrame:
    &#34;&#34;&#34;Read data stored by Lakeshore temperature monitor software.

    Args:
        filepath: Path to the Excel file.

    Returns:
        DataFrame containing the data.
    &#34;&#34;&#34;
    # Extract only the origin timestamp
    origin = pd.read_excel(filepath, skiprows=1, nrows=1, usecols=[1], header=None)[1][0]
    # Remove any tzinfo to prevent future exceptions in pandas
    origin = origin.replace(&#34;CET&#34;, &#34;&#34;)
    # Parse datetime object from timestamp
    origin = pd.to_datetime(origin)
    # Create DataFrame and drop empty cols
    df = pd.read_excel(filepath, skiprows=3)
    df = df.dropna(axis=1, how=&#34;all&#34;)
    # Add datetimes to DataFrame
    df[&#34;Datetime&#34;] = pd.to_datetime(df[&#34;Time&#34;], unit=&#34;ms&#34;, origin=origin)
    df = df.drop(&#34;Time&#34;, axis=1)
    # Set datetime as index
    df = df.set_index(&#34;Datetime&#34;, drop=True)
    return df</code></pre>
</details>
</dd>
<dt id="qudi_hira_analysis.io_handler.IOHandler.read_nanonis_data"><code class="name flex">
<span>def <span class="ident">read_nanonis_data</span></span>(<span>self, filepath: pathlib.Path) ‑> pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Read data from a Nanonis .dat file.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>filepath</code></strong></dt>
<dd>Path to the Nanonis .dat file.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>DataFrame of data.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@_add_base_read_path
@_check_extension(&#34;.dat&#34;)
def read_nanonis_data(self, filepath: Path) -&gt; pd.DataFrame:
    &#34;&#34;&#34;Read data from a Nanonis .dat file.

    Args:
        filepath: Path to the Nanonis .dat file.

    Returns:
        DataFrame of data.
    &#34;&#34;&#34;
    skip_rows = 0
    with open(filepath) as dat_file:
        for num, line in enumerate(dat_file, 1):
            if &#34;[DATA]&#34; in line:
                # Find number of rows to skip when extracting data
                skip_rows = num
                break
            if &#34;#=====&#34; in line:
                skip_rows = num
                break
    df = pd.read_table(filepath, sep=&#34;\t&#34;, skiprows=skip_rows)
    return df</code></pre>
</details>
</dd>
<dt id="qudi_hira_analysis.io_handler.IOHandler.read_nanonis_parameters"><code class="name flex">
<span>def <span class="ident">read_nanonis_parameters</span></span>(<span>self, filepath: pathlib.Path) ‑> dict</span>
</code></dt>
<dd>
<div class="desc"><p>Read parameters from a Nanonis .dat file.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>filepath</code></strong></dt>
<dd>Path to the Nanonis .dat file.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Dictionary of parameters.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@_add_base_read_path
@_check_extension(&#34;.dat&#34;)
def read_nanonis_parameters(self, filepath: Path) -&gt; dict:
    &#34;&#34;&#34;Read parameters from a Nanonis .dat file.

    Args:
        filepath: Path to the Nanonis .dat file.

    Returns:
        Dictionary of parameters.
    &#34;&#34;&#34;
    parameters = {}
    with open(filepath) as dat_file:
        for line in dat_file:
            if line == &#34;\n&#34;:
                # Break when reaching empty line
                break
            elif &#34;User&#34; in line or line.split(&#34;\t&#34;)[0] == &#34;&#34;:
                # Cleanup excess parameters and skip empty lines
                pass
            else:
                label, value, _ = line.split(&#34;\t&#34;)
                try:
                    # Convert strings to number where possible
                    value = float(value)
                except ValueError:
                    pass
                if &#34;Oscillation Control&gt;&#34; in label:
                    label = label.replace(&#34;Oscillation Control&gt;&#34;, &#34;&#34;)
                parameters[label] = value
    return parameters</code></pre>
</details>
</dd>
<dt id="qudi_hira_analysis.io_handler.IOHandler.read_nanonis_spm_data"><code class="name flex">
<span>def <span class="ident">read_nanonis_spm_data</span></span>(<span>self, filepath: pathlib.Path) ‑> pySPM.SXM.SXM</span>
</code></dt>
<dd>
<div class="desc"><p>Read a Nanonis .sxm data file.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>filepath</code></strong></dt>
<dd>Path to the .sxm file.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>pySPM.SXM object containing the data.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@_add_base_read_path
@_check_extension(&#34;.sxm&#34;)
def read_nanonis_spm_data(self, filepath: Path) -&gt; pySPM.SXM:
    &#34;&#34;&#34;Read a Nanonis .sxm data file.

    Args:
        filepath: Path to the .sxm file.

    Returns:
        pySPM.SXM object containing the data.
    &#34;&#34;&#34;
    return pySPM.SXM(filepath)</code></pre>
</details>
</dd>
<dt id="qudi_hira_analysis.io_handler.IOHandler.read_oceanoptics_data"><code class="name flex">
<span>def <span class="ident">read_oceanoptics_data</span></span>(<span>self, filepath: str) ‑> pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Read spectrometer data from OceanOptics spectrometer.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>filepath</code></strong></dt>
<dd>Path to the data file.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>DataFrame containing the wavelength and intensity data.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@_add_base_read_path
@_check_extension(&#34;.txt&#34;)
def read_oceanoptics_data(self, filepath: str) -&gt; pd.DataFrame:
    &#34;&#34;&#34;Read spectrometer data from OceanOptics spectrometer.

    Args:
        filepath: Path to the data file.

    Returns:
        DataFrame containing the wavelength and intensity data.
    &#34;&#34;&#34;
    df = pd.read_csv(filepath, sep=&#34;\t&#34;, skiprows=14, names=[&#34;wavelength&#34;, &#34;intensity&#34;])
    return df</code></pre>
</details>
</dd>
<dt id="qudi_hira_analysis.io_handler.IOHandler.read_pfeiffer_data"><code class="name flex">
<span>def <span class="ident">read_pfeiffer_data</span></span>(<span>self, filepath: pathlib.Path) ‑> pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Read data stored by Pfeiffer vacuum monitoring software.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>filepath</code></strong></dt>
<dd>Path to the text file.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>DataFrame containing the data.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@_add_base_read_path
@_check_extension(&#34;.txt&#34;)
def read_pfeiffer_data(self, filepath: Path) -&gt; pd.DataFrame:
    &#34;&#34;&#34;Read data stored by Pfeiffer vacuum monitoring software.

    Args:
        filepath: Path to the text file.

    Returns:
        DataFrame containing the data.
    &#34;&#34;&#34;
    # Extract rows including the header
    df = pd.read_csv(filepath, sep=&#34;\t&#34;, skiprows=[0, 2, 3, 4])
    # Combine data and time columns together
    df[&#34;Date&#34;] = df[&#34;Date&#34;] + &#34; &#34; + df[&#34;Time&#34;]
    df = df.drop(&#34;Time&#34;, axis=1)
    # Infer datetime format and convert to datetime objects
    df[&#34;Date&#34;] = pd.to_datetime(df[&#34;Date&#34;], infer_datetime_format=True)
    # Set datetime as index
    df = df.set_index(&#34;Date&#34;, drop=True)
    return df</code></pre>
</details>
</dd>
<dt id="qudi_hira_analysis.io_handler.IOHandler.read_pixelscanner_data"><code class="name flex">
<span>def <span class="ident">read_pixelscanner_data</span></span>(<span>self, filepath: pathlib.Path) ‑> (<class 'pySPM.SPM.SPM_image'>, <class 'pySPM.SPM.SPM_image'>)</span>
</code></dt>
<dd>
<div class="desc"><p>Read data from a PixelScanner measurement.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>filepath</code></strong></dt>
<dd>Path to the data file.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Forward and backward scan data.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def read_pixelscanner_data(self, filepath: Path) -&gt; (pySPM.SPM_image, pySPM.SPM_image):
    &#34;&#34;&#34; Read data from a PixelScanner measurement.

    Args:
        filepath: Path to the data file.

    Returns:
        Forward and backward scan data.
    &#34;&#34;&#34;
    df = self.read_into_dataframe(filepath)
    num_pixels = int(np.sqrt(len(df) // 2))

    if num_pixels ** 2 != len(df) // 2:
        raise ValueError(&#34;Number of pixels does not match data length.&#34;)

    try:
        fwd, bwd = self.__get_forward_backward_counts(df[&#34;count_rates&#34;], num_pixels)
    except KeyError:
        try:
            fwd, bwd = self.__get_forward_backward_counts(df[&#34;Count Rates (cps)&#34;], num_pixels)
        except KeyError:
            # Support old data format
            fwd = df[&#34;forward (cps)&#34;].to_numpy().reshape(num_pixels, num_pixels)
            bwd = df[&#34;backward (cps)&#34;].to_numpy().reshape(num_pixels, num_pixels)

    fwd = pySPM.SPM_image(fwd, channel=&#34;Forward&#34;, _type=&#34;NV-PL&#34;)
    bwd = pySPM.SPM_image(bwd, channel=&#34;Backward&#34;, _type=&#34;NV-PL&#34;)
    return fwd, bwd</code></pre>
</details>
</dd>
<dt id="qudi_hira_analysis.io_handler.IOHandler.read_pkl"><code class="name flex">
<span>def <span class="ident">read_pkl</span></span>(<span>self, filepath: pathlib.Path) ‑> dict</span>
</code></dt>
<dd>
<div class="desc"><p>Read pickle files into a dictionary.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@_add_base_read_path
@_check_extension(&#34;.pkl&#34;)
def read_pkl(self, filepath: Path) -&gt; dict:
    &#34;&#34;&#34; Read pickle files into a dictionary. &#34;&#34;&#34;
    with open(filepath, &#39;rb&#39;) as f:
        file = pickle.load(f)
    return file</code></pre>
</details>
</dd>
<dt id="qudi_hira_analysis.io_handler.IOHandler.read_pys"><code class="name flex">
<span>def <span class="ident">read_pys</span></span>(<span>self, filepath: pathlib.Path) ‑> dict</span>
</code></dt>
<dd>
<div class="desc"><p>Read raw .pys data files into a dictionary.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@_add_base_read_path
@_check_extension(&#34;.pys&#34;)
def read_pys(self, filepath: Path) -&gt; dict:
    &#34;&#34;&#34; Read raw .pys data files into a dictionary. &#34;&#34;&#34;
    byte_dict = np.load(str(filepath), encoding=&#34;bytes&#34;, allow_pickle=True)
    # Convert byte string keys to normal strings
    return {key.decode(&#39;utf8&#39;): byte_dict.get(key) for key in byte_dict.keys()}</code></pre>
</details>
</dd>
<dt id="qudi_hira_analysis.io_handler.IOHandler.read_qudi_parameters"><code class="name flex">
<span>def <span class="ident">read_qudi_parameters</span></span>(<span>self, filepath: pathlib.Path) ‑> dict</span>
</code></dt>
<dd>
<div class="desc"><p>Extract parameters from a qudi dat file.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>filepath</code></strong></dt>
<dd>Path to the qudi .dat file</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Dictionary of parameters</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@_add_base_read_path
@_check_extension(&#34;.dat&#34;)
def read_qudi_parameters(self, filepath: Path) -&gt; dict:
    &#34;&#34;&#34;Extract parameters from a qudi dat file.

    Args:
        filepath: Path to the qudi .dat file

    Returns:
        Dictionary of parameters
    &#34;&#34;&#34;
    params = {}
    with open(filepath) as file:
        for line in file:
            if line == &#39;#=====\n&#39;:
                break
            else:
                # noinspection PyBroadException
                try:
                    # Remove # from beginning of lines
                    line = line[1:]
                    if line.count(&#34;:&#34;) == 1:
                        # Add params to dictionary
                        label, value = line.split(&#34;:&#34;)
                        if value != &#34;\n&#34;:
                            params[label] = ast.literal_eval(inspect.cleandoc(value))
                    elif line.count(&#34;:&#34;) == 3:
                        # Handle files with timestamps in them
                        label = line.split(&#34;:&#34;)[0]
                        timestamp_str = &#34;&#34;.join(line.split(&#34;:&#34;)[1:]).strip()
                        datetime_str = datetime.datetime.strptime(timestamp_str, &#34;%d.%m.%Y %Hh%Mmin%Ss&#34;).replace(
                            tzinfo=datetime.timezone.utc)
                        params[label] = datetime_str
                except Exception as _:
                    pass
    return params</code></pre>
</details>
</dd>
<dt id="qudi_hira_analysis.io_handler.IOHandler.save_df"><code class="name flex">
<span>def <span class="ident">save_df</span></span>(<span>self, filepath: pathlib.Path, df: pandas.core.frame.DataFrame)</span>
</code></dt>
<dd>
<div class="desc"><p>Save Dataframe as csv.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@_add_base_write_path
@_check_extension(&#34;.pys&#34;)
def save_df(self, filepath: Path, df: pd.DataFrame):
    &#34;&#34;&#34; Save Dataframe as csv. &#34;&#34;&#34;
    df.to_csv(filepath, sep=&#39;\t&#39;, encoding=&#39;utf-8&#39;)</code></pre>
</details>
</dd>
<dt id="qudi_hira_analysis.io_handler.IOHandler.save_figures"><code class="name flex">
<span>def <span class="ident">save_figures</span></span>(<span>self, filepath: pathlib.Path, fig: matplotlib.figure.Figure, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Saves figures from matplotlib plot data.</p>
<p>By default, saves as jpg, png, pdf and svg.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>fig</code></strong></dt>
<dd>Matplotlib figure to save.</dd>
<dt><strong><code>filepath</code></strong></dt>
<dd>Name of figure to save.</dd>
<dt><strong><code>only_jpg</code></strong></dt>
<dd>If True, only save as jpg (default: False).</dd>
<dt><strong><code>only_pdf</code></strong></dt>
<dd>If True, only save as pdf (default: False).</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Keyword arguments passed to fig.savefig().</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@_add_base_write_path
def save_figures(self, filepath: Path, fig: plt.Figure, **kwargs):
    &#34;&#34;&#34;Saves figures from matplotlib plot data.

    By default, saves as jpg, png, pdf and svg.

    Args:
        fig: Matplotlib figure to save.
        filepath: Name of figure to save.
        only_jpg: If True, only save as jpg (default: False).
        only_pdf: If True, only save as pdf (default: False).
        **kwargs: Keyword arguments passed to fig.savefig().
    &#34;&#34;&#34;
    extensions = None
    if &#34;only_jpg&#34; in kwargs:
        if kwargs.get(&#34;only_jpg&#34;):
            extensions = [&#34;.jpg&#34;]
        kwargs.pop(&#34;only_jpg&#34;, None)
    elif &#34;only_pdf&#34; in kwargs:
        if kwargs.get(&#34;only_pdf&#34;):
            extensions = [&#34;.pdf&#34;]
        kwargs.pop(&#34;only_pdf&#34;, None)
    else:
        extensions = [&#34;.jpg&#34;, &#34;.pdf&#34;, &#34;.svg&#34;, &#34;.png&#34;]

    for ext in extensions:
        fig.savefig(filepath.with_suffix(ext), dpi=200, **kwargs)</code></pre>
</details>
</dd>
<dt id="qudi_hira_analysis.io_handler.IOHandler.save_pkl"><code class="name flex">
<span>def <span class="ident">save_pkl</span></span>(<span>self, filepath: pathlib.Path, obj: object)</span>
</code></dt>
<dd>
<div class="desc"><p>Saves pickle files.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>filepath</code></strong></dt>
<dd>Path to the data file.</dd>
<dt><strong><code>obj</code></strong></dt>
<dd>Object to be saved.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@_add_base_write_path
@_check_extension(&#34;.pkl&#34;)
def save_pkl(self, filepath: Path, obj: object):
    &#34;&#34;&#34;Saves pickle files.

    Args:
        filepath: Path to the data file.
        obj: Object to be saved.
    &#34;&#34;&#34;
    with open(filepath, &#39;wb&#39;) as f:
        pickle.dump(obj, f)</code></pre>
</details>
</dd>
<dt id="qudi_hira_analysis.io_handler.IOHandler.save_pys"><code class="name flex">
<span>def <span class="ident">save_pys</span></span>(<span>self, filepath: pathlib.Path, dictionary: dict)</span>
</code></dt>
<dd>
<div class="desc"><p>Saves .pys files.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>filepath</code></strong></dt>
<dd>Path to the data file.</dd>
<dt><strong><code>dictionary</code></strong></dt>
<dd>Dictionary to be saved.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@_add_base_write_path
@_check_extension(&#34;.pys&#34;)
def save_pys(self, filepath: Path, dictionary: dict):
    &#34;&#34;&#34;Saves .pys files.

    Args:
        filepath: Path to the data file.
        dictionary: Dictionary to be saved.
    &#34;&#34;&#34;
    with open(filepath, &#39;wb&#39;) as f:
        pickle.dump(dictionary, f, 1)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="qudi_hira_analysis" href="index.html">qudi_hira_analysis</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="qudi_hira_analysis.io_handler.IOHandler" href="#qudi_hira_analysis.io_handler.IOHandler">IOHandler</a></code></h4>
<ul class="">
<li><code><a title="qudi_hira_analysis.io_handler.IOHandler.read_bruker_spm_data" href="#qudi_hira_analysis.io_handler.IOHandler.read_bruker_spm_data">read_bruker_spm_data</a></code></li>
<li><code><a title="qudi_hira_analysis.io_handler.IOHandler.read_confocal_into_dataframe" href="#qudi_hira_analysis.io_handler.IOHandler.read_confocal_into_dataframe">read_confocal_into_dataframe</a></code></li>
<li><code><a title="qudi_hira_analysis.io_handler.IOHandler.read_csv" href="#qudi_hira_analysis.io_handler.IOHandler.read_csv">read_csv</a></code></li>
<li><code><a title="qudi_hira_analysis.io_handler.IOHandler.read_excel" href="#qudi_hira_analysis.io_handler.IOHandler.read_excel">read_excel</a></code></li>
<li><code><a title="qudi_hira_analysis.io_handler.IOHandler.read_into_dataframe" href="#qudi_hira_analysis.io_handler.IOHandler.read_into_dataframe">read_into_dataframe</a></code></li>
<li><code><a title="qudi_hira_analysis.io_handler.IOHandler.read_into_ndarray" href="#qudi_hira_analysis.io_handler.IOHandler.read_into_ndarray">read_into_ndarray</a></code></li>
<li><code><a title="qudi_hira_analysis.io_handler.IOHandler.read_into_ndarray_transposed" href="#qudi_hira_analysis.io_handler.IOHandler.read_into_ndarray_transposed">read_into_ndarray_transposed</a></code></li>
<li><code><a title="qudi_hira_analysis.io_handler.IOHandler.read_lakeshore_data" href="#qudi_hira_analysis.io_handler.IOHandler.read_lakeshore_data">read_lakeshore_data</a></code></li>
<li><code><a title="qudi_hira_analysis.io_handler.IOHandler.read_nanonis_data" href="#qudi_hira_analysis.io_handler.IOHandler.read_nanonis_data">read_nanonis_data</a></code></li>
<li><code><a title="qudi_hira_analysis.io_handler.IOHandler.read_nanonis_parameters" href="#qudi_hira_analysis.io_handler.IOHandler.read_nanonis_parameters">read_nanonis_parameters</a></code></li>
<li><code><a title="qudi_hira_analysis.io_handler.IOHandler.read_nanonis_spm_data" href="#qudi_hira_analysis.io_handler.IOHandler.read_nanonis_spm_data">read_nanonis_spm_data</a></code></li>
<li><code><a title="qudi_hira_analysis.io_handler.IOHandler.read_oceanoptics_data" href="#qudi_hira_analysis.io_handler.IOHandler.read_oceanoptics_data">read_oceanoptics_data</a></code></li>
<li><code><a title="qudi_hira_analysis.io_handler.IOHandler.read_pfeiffer_data" href="#qudi_hira_analysis.io_handler.IOHandler.read_pfeiffer_data">read_pfeiffer_data</a></code></li>
<li><code><a title="qudi_hira_analysis.io_handler.IOHandler.read_pixelscanner_data" href="#qudi_hira_analysis.io_handler.IOHandler.read_pixelscanner_data">read_pixelscanner_data</a></code></li>
<li><code><a title="qudi_hira_analysis.io_handler.IOHandler.read_pkl" href="#qudi_hira_analysis.io_handler.IOHandler.read_pkl">read_pkl</a></code></li>
<li><code><a title="qudi_hira_analysis.io_handler.IOHandler.read_pys" href="#qudi_hira_analysis.io_handler.IOHandler.read_pys">read_pys</a></code></li>
<li><code><a title="qudi_hira_analysis.io_handler.IOHandler.read_qudi_parameters" href="#qudi_hira_analysis.io_handler.IOHandler.read_qudi_parameters">read_qudi_parameters</a></code></li>
<li><code><a title="qudi_hira_analysis.io_handler.IOHandler.save_df" href="#qudi_hira_analysis.io_handler.IOHandler.save_df">save_df</a></code></li>
<li><code><a title="qudi_hira_analysis.io_handler.IOHandler.save_figures" href="#qudi_hira_analysis.io_handler.IOHandler.save_figures">save_figures</a></code></li>
<li><code><a title="qudi_hira_analysis.io_handler.IOHandler.save_pkl" href="#qudi_hira_analysis.io_handler.IOHandler.save_pkl">save_pkl</a></code></li>
<li><code><a title="qudi_hira_analysis.io_handler.IOHandler.save_pys" href="#qudi_hira_analysis.io_handler.IOHandler.save_pys">save_pys</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>