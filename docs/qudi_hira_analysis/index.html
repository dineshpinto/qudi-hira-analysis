<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>qudi_hira_analysis API documentation</title>
<meta name="description" content="Analytics suite for qubit SPM using FPGA timetaggers …" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Package <code>qudi_hira_analysis</code></h1>
</header>
<section id="section-intro">
<p>Analytics suite for qubit SPM using FPGA timetaggers</p>
<h2 id="getting-started">Getting started</h2>
<pre><code class="language-python">from pathlib import Path
import matplotlib.pyplot as plt
import seaborn as sns

from qudi_hira_analysis import DataHandler

dh = DataHandler(
    data_folder=Path(&quot;C:/Data&quot;), # Path to the data folder
    figure_folder=Path(&quot;C:/QudiHiraAnalysis&quot;), # Path to the figure folder
    measurement_folder=Path(&quot;20230101_NV1&quot;) # Name of the measurement folder
 )

# Search and lazy-load files with &quot;odmr&quot; in the name
odmr_measurements = dh.load_measurements(&quot;odmr&quot;)
</code></pre>
<p>Start by creating an instance of the <code><a title="qudi_hira_analysis.DataHandler" href="#qudi_hira_analysis.DataHandler">DataHandler</a></code> class.
To load a specific set of measurements from the data folder, use the <code><a title="qudi_hira_analysis.DataHandler.load_measurements" href="#qudi_hira_analysis.DataHandler.load_measurements">DataHandler.load_measurements()</a></code> method.</p>
<h2 id="data-fitting">Data fitting</h2>
<p>To fit data, call the <code><a title="qudi_hira_analysis.DataHandler.fit" href="analysis_logic.html#qudi_hira_analysis.analysis_logic.AnalysisLogic.fit">AnalysisLogic.fit()</a></code> method.</p>
<pre><code class="language-python">odmr = odmr_measurements[&quot;20230101-0420-00&quot;]
xf, yf, res = dh.fit(x=&quot;Freq&quot;, y=&quot;Counts&quot;, fit_function=dh.fit_function.doublelorentzian, data=odmr.data)

# Plot the data and the fit
plot = sns.scatterplot(x=&quot;Freq&quot;, y=&quot;Counts&quot;, data=odmr.data, label=odmr.timestamp)
sns.lineplot(x=xf, y=yf, ax=plot, label=&quot;Fit&quot;)

# Generate fit report
print(res.fit_report())
</code></pre>
<p>To get the full list of available fit routines, explore the <code><a title="qudi_hira_analysis.DataHandler.fit_function" href="analysis_logic.html#qudi_hira_analysis.analysis_logic.AnalysisLogic.fit_function">DataHandler.fit_function</a></code> attribute or call <code><a title="qudi_hira_analysis.AnalysisLogic.get_all_fits" href="#qudi_hira_analysis.AnalysisLogic.get_all_fits">AnalysisLogic.get_all_fits()</a></code>.
The fit functions are:</p>
<table>
<thead>
<tr>
<th>Dimension</th>
<th>Fit</th>
</tr>
</thead>
<tbody>
<tr>
<td>1D</td>
<td>decayexponential</td>
</tr>
<tr>
<td></td>
<td>biexponential</td>
</tr>
<tr>
<td></td>
<td>decayexponentialstretched</td>
</tr>
<tr>
<td></td>
<td>gaussian</td>
</tr>
<tr>
<td></td>
<td>gaussiandouble</td>
</tr>
<tr>
<td></td>
<td>gaussianlinearoffset</td>
</tr>
<tr>
<td></td>
<td>hyperbolicsaturation</td>
</tr>
<tr>
<td></td>
<td>linear</td>
</tr>
<tr>
<td></td>
<td>lorentzian</td>
</tr>
<tr>
<td></td>
<td>lorentziandouble</td>
</tr>
<tr>
<td></td>
<td>lorentziantriple</td>
</tr>
<tr>
<td></td>
<td>sine</td>
</tr>
<tr>
<td></td>
<td>sinedouble</td>
</tr>
<tr>
<td></td>
<td>sinedoublewithexpdecay</td>
</tr>
<tr>
<td></td>
<td>sinedoublewithtwoexpdecay</td>
</tr>
<tr>
<td></td>
<td>sineexponentialdecay</td>
</tr>
<tr>
<td></td>
<td>sinestretchedexponentialdecay</td>
</tr>
<tr>
<td></td>
<td>sinetriple</td>
</tr>
<tr>
<td></td>
<td>sinetriplewithexpdecay</td>
</tr>
<tr>
<td></td>
<td>sinetriplewiththreeexpdecay</td>
</tr>
<tr>
<td>2D</td>
<td>twoDgaussian</td>
</tr>
</tbody>
</table>
<h2 id="data-saving">Data saving</h2>
<p>To save figures, call the <code><a title="qudi_hira_analysis.DataHandler.save_figures" href="io_handler.html#qudi_hira_analysis.io_handler.IOHandler.save_figures">IOHandler.save_figures()</a></code> method. By default,
the figures are saved as JPG, PDF, PNG and SVG.
This can be changed by setting the <code>only_jpg</code> or <code>only_pdf</code> arguments to <code>True</code>.</p>
<pre><code class="language-python"># Save the figure to the figure folder specified earlier
dh.save_figures(filepath=Path(&quot;odmr&quot;), fig=plot.get_figure(), only_pdf=True, bbox_inches=&quot;tight&quot;)
</code></pre>
<h2 id="examples">Examples</h2>
<h3 id="nv-odmr-map">NV-ODMR map</h3>
<pre><code class="language-python"># Extract ODMR measurements from the measurement folder
odmr_measurements = dh.load_measurements(measurement_str=&quot;2d_odmr_map&quot;)
odmr_measurements = dict(sorted(odmr_measurements.items()))

# Perform parallel (=num CPU cores) ODMR fitting
odmr_measurements = dh.fit_raster_odmr(odmr_measurements)

# Calculate 2D ODMR map from the fitted ODMR measurements
pixels = int(np.sqrt(len(odmr_measurements)))
image = np.zeros((pixels, pixels))

for idx, odmr in enumerate(odmr_measurements.values()):
  row, col = odmr.xy_position
  if len(odmr.fit_model.params) &gt; 6:
    # Calculate double Lorentzian splitting
    image[row, col] = np.abs(odmr.fit_model.best_values[&quot;l1_center&quot;]
                             - odmr.fit_model.best_values[&quot;l0_center&quot;])


map = sns.heatmap(image, cbar_kws={&quot;label&quot;: &quot;Delta E (MHz)&quot;})

# Save the figure to the figure folder specified earlier
dh.save_figures(filepath=&quot;2d_odmr_map&quot;, fig=map.get_figure(), only_jpg=True)
</code></pre>
<h3 id="nv-pl-map">NV-PL map</h3>
<pre><code class="language-python">pixel_scanner_measurements = dh.load_measurements(measurement_str=&quot;PixelScanner&quot;)

fwd, bwd = pixel_scanner_measurements[&quot;20230101-0420-00&quot;].data

# If size is known, it can be specified here
fwd.size[&quot;real&quot;] = {&quot;x&quot;: 1e-6, &quot;y&quot;: 1e-6, &quot;unit&quot;: &quot;m&quot;}

fig, ax = plt.subplots()

# Perform (optional) image corrections
fwd.filter_gaussian(sigma=0.5)

# Add scale bar, color bar and plot the data
img = fwd.show(cmap=&quot;inferno&quot;, ax=ax)
fwd.add_scale(length=1e-6, ax=ax, height=1)
cbar = fig.colorbar(img)
cbar.set_label(&quot;NV-PL (kcps)&quot;)

# Save the figure to the figure folder specified earlier
dh.save_figures(filepath=&quot;nv_pl_scan&quot;, fig=fig, only_jpg=True)
</code></pre>
<h3 id="nanonis-afm-measurements">Nanonis AFM measurements</h3>
<pre><code class="language-python">afm_measurements = dh.load_measurements(measurement_str=&quot;Scan&quot;, extension=&quot;.sxm&quot;, qudi=False)

afm = afm_measurements[&quot;20230101-0420-00&quot;].data

# Print the channels available in the data
afm.list_channels()
topo = afm.get_channel(&quot;Z&quot;)

fig, ax = plt.subplots()

# Perform (optional) image corrections
topo.correct_lines()
topo.correct_plane()
topo.filter_lowpass(fft_radius=20)
topo.zero_min()

# Add scale bar, color bar and plot the data
img = topo.show(cmap=&quot;inferno&quot;, ax=ax)
topo.add_scale(length=1e-6, ax=ax, height=1, fontsize=10)
cbar = fig.colorbar(img)
cbar.set_label(&quot;Height (nm)&quot;)

dh.save_figures(filepath=&quot;afm_topo&quot;, fig=fig, only_jpg=True)
</code></pre>
<h3 id="g2-measurements-anti-bunching-fit">g(2) measurements (anti-bunching fit)</h3>
<pre><code class="language-python">autocorrelation_measurements = dh.load_measurements(measurement_str=&quot;Autocorrelation&quot;)

fig, ax = plt.subplots()

for autocorrelation in autocorrelation_measurements.values():
    autocorrelation.data[&quot;Time (ns)&quot;] = autocorrelation.data[&quot;Time (ps)&quot;] * 1e-3
    # Plot the data
    sns.lineplot(data=autocorrelation.data, x=&quot;Time (ns)&quot;, y=&quot;g2(t) norm&quot;, ax=ax)
    # Fit the data using the antibunching function
    fit_x, fit_y, result = dh.fit(x=&quot;Time (ns)&quot;, y=&quot;g2(t) norm&quot;, data=autocorrelation.data,
                                  fit_function=dh.fit_function.antibunching)
    # Plot the fit
    sns.lineplot(x=fit_x, y=fit_y, ax=ax, color=&quot;C1&quot;)

# Save the figure to the figure folder specified earlier
dh.save_figures(filepath=&quot;autocorrelation_variation&quot;, fig=fig)
</code></pre>
<h3 id="odmr-measurements-double-lorentzian-fit">ODMR measurements (double Lorentzian fit)</h3>
<pre><code class="language-python">odmr_measurements = dh.load_measurements(measurement_str=&quot;ODMR&quot;, pulsed=True)

fig, ax = plt.subplots()

for odmr in odmr_measurements.values():
    sns.scatterplot(data=odmr.data, x=&quot;Controlled variable(Hz)&quot;, y=&quot;Signal&quot;, ax=ax)
    fit_x, fit_y, result = dh.fit(x=&quot;Controlled variable(Hz)&quot;, y=&quot;Signal&quot;, data=odmr.data,
                                  fit_function=dh.fit_function.lorentziandouble)
    sns.lineplot(x=fit_x, y=fit_y, ax=ax, color=&quot;C1&quot;)

dh.save_figures(filepath=&quot;odmr_variation&quot;, fig=fig)
</code></pre>
<h3 id="rabi-measurements-sine-exp-decay-fit">Rabi measurements (sine exp. decay fit)</h3>
<pre><code class="language-python">rabi_measurements = dh.load_measurements(measurement_str=&quot;Rabi&quot;, pulsed=True)

fig, ax = plt.subplots()

for rabi in rabi_measurements.values():
    sns.scatterplot(data=rabi.data, x=&quot;Controlled variable(s)&quot;, y=&quot;Signal&quot;, ax=ax)
    fit_x, fit_y, result = dh.fit(x=&quot;Controlled variable(s)&quot;, y=&quot;Signal&quot;, data=rabi.data,
                                  fit_function=dh.fit_function.sineexponentialdecay)
    sns.lineplot(x=fit_x, y=fit_y, ax=ax, color=&quot;C1&quot;)

dh.save_figures(filepath=&quot;rabi_variation&quot;, fig=fig)
</code></pre>
<h3 id="temperature-measurements">Temperature measurements</h3>
<pre><code class="language-python">temperature_measurements = dh.load_measurements(measurement_str=&quot;Temperature&quot;, qudi=False)

temperature = pd.concat([t.data for t in temperature_measurements.values()])

fig, ax = plt.subplots()
sns.lineplot(data=temperature, x=&quot;Time&quot;, y=&quot;Temperature&quot;, ax=ax)
dh.save_figures(filepath=&quot;temperature_monitoring&quot;, fig=fig)
</code></pre>
<h3 id="bruker-mfm-measurements">Bruker MFM measurements</h3>
<pre><code class="language-python">bruker_measurements = dh.load_measurements(measurement_str=&quot;&quot;, extension=&quot;.001&quot;, qudi=False)

bruker_data = bruker_measurements[&quot;20230101-0420-00&quot;].data

# Print the channels available in the data
bruker_data.list_channels()
mfm = bruker_data.get_channel(&quot;Phase&quot;, mfm=True)

fig, ax = plt.subplots()

# Perform (optional) image corrections
mfm.correct_plane()
mfm.zero_min()

# Add scale bar, color bar and plot the data
img = mfm.show(cmap=&quot;inferno&quot;, ax=ax)
mfm.add_scale(length=1, ax=ax, height=1, fontsize=10)
cbar = fig.colorbar(img)
cbar.set_label(&quot;MFM contrast (deg)&quot;)

dh.save_figures(filepath=&quot;MFM&quot;, fig=fig, only_jpg=True)
</code></pre>
<h3 id="pys-data-pi3diamond-compatibility">PYS data (pi3diamond compatibility)</h3>
<pre><code class="language-python">pys_measurements = dh.load_measurements(measurement_str=&quot;ndmin&quot;, extension=&quot;.pys&quot;, qudi=False)
pys = pys_measurements[list(pys_measurements)[0]].data

fig, ax = plt.subplots()
sns.lineplot(x=pys[&quot;time_bins&quot;], y=pys[&quot;counts&quot;], ax=ax)
dh.save_figures(filepath=&quot;pys_measurement&quot;, fig=fig)
</code></pre>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;Analytics suite for qubit SPM using FPGA timetaggers

## Getting started

```python
from pathlib import Path
import matplotlib.pyplot as plt
import seaborn as sns

from qudi_hira_analysis import DataHandler

dh = DataHandler(
    data_folder=Path(&#34;C:/Data&#34;), # Path to the data folder
    figure_folder=Path(&#34;C:/QudiHiraAnalysis&#34;), # Path to the figure folder
    measurement_folder=Path(&#34;20230101_NV1&#34;) # Name of the measurement folder
 )

# Search and lazy-load files with &#34;odmr&#34; in the name
odmr_measurements = dh.load_measurements(&#34;odmr&#34;)
```
Start by creating an instance of the `DataHandler` class.
To load a specific set of measurements from the data folder, use the `DataHandler.load_measurements()` method.

## Data fitting

To fit data, call the `DataHandler.fit()` method.

```python
odmr = odmr_measurements[&#34;20230101-0420-00&#34;]
xf, yf, res = dh.fit(x=&#34;Freq&#34;, y=&#34;Counts&#34;, fit_function=dh.fit_function.doublelorentzian, data=odmr.data)

# Plot the data and the fit
plot = sns.scatterplot(x=&#34;Freq&#34;, y=&#34;Counts&#34;, data=odmr.data, label=odmr.timestamp)
sns.lineplot(x=xf, y=yf, ax=plot, label=&#34;Fit&#34;)

# Generate fit report
print(res.fit_report())
```

To get the full list of available fit routines, explore the `DataHandler.fit_function` attribute or call `AnalysisLogic.get_all_fits()`.
The fit functions are:

| Dimension | Fit                           |
|-----------|-------------------------------|
| 1D        | decayexponential              |
|           | biexponential                 |
|           | decayexponentialstretched     |
|           | gaussian                      |
|           | gaussiandouble                |
|           | gaussianlinearoffset          |
|           | hyperbolicsaturation          |
|           | linear                        |
|           | lorentzian                    |
|           | lorentziandouble              |
|           | lorentziantriple              |
|           | sine                          |
|           | sinedouble                    |
|           | sinedoublewithexpdecay        |
|           | sinedoublewithtwoexpdecay     |
|           | sineexponentialdecay          |
|           | sinestretchedexponentialdecay |
|           | sinetriple                    |
|           | sinetriplewithexpdecay        |
|           | sinetriplewiththreeexpdecay   |
| 2D        | twoDgaussian                  |


## Data saving

To save figures, call the `DataHandler.save_figures()` method. By default,
the figures are saved as JPG, PDF, PNG and SVG.
This can be changed by setting the `only_jpg` or `only_pdf` arguments to `True`.

```python
# Save the figure to the figure folder specified earlier
dh.save_figures(filepath=Path(&#34;odmr&#34;), fig=plot.get_figure(), only_pdf=True, bbox_inches=&#34;tight&#34;)
```


## Examples

### NV-ODMR map
```python
# Extract ODMR measurements from the measurement folder
odmr_measurements = dh.load_measurements(measurement_str=&#34;2d_odmr_map&#34;)
odmr_measurements = dict(sorted(odmr_measurements.items()))

# Perform parallel (=num CPU cores) ODMR fitting
odmr_measurements = dh.fit_raster_odmr(odmr_measurements)

# Calculate 2D ODMR map from the fitted ODMR measurements
pixels = int(np.sqrt(len(odmr_measurements)))
image = np.zeros((pixels, pixels))

for idx, odmr in enumerate(odmr_measurements.values()):
  row, col = odmr.xy_position
  if len(odmr.fit_model.params) &gt; 6:
    # Calculate double Lorentzian splitting
    image[row, col] = np.abs(odmr.fit_model.best_values[&#34;l1_center&#34;]
                             - odmr.fit_model.best_values[&#34;l0_center&#34;])


map = sns.heatmap(image, cbar_kws={&#34;label&#34;: &#34;Delta E (MHz)&#34;})

# Save the figure to the figure folder specified earlier
dh.save_figures(filepath=&#34;2d_odmr_map&#34;, fig=map.get_figure(), only_jpg=True)
```


### NV-PL map

```python
pixel_scanner_measurements = dh.load_measurements(measurement_str=&#34;PixelScanner&#34;)

fwd, bwd = pixel_scanner_measurements[&#34;20230101-0420-00&#34;].data

# If size is known, it can be specified here
fwd.size[&#34;real&#34;] = {&#34;x&#34;: 1e-6, &#34;y&#34;: 1e-6, &#34;unit&#34;: &#34;m&#34;}

fig, ax = plt.subplots()

# Perform (optional) image corrections
fwd.filter_gaussian(sigma=0.5)

# Add scale bar, color bar and plot the data
img = fwd.show(cmap=&#34;inferno&#34;, ax=ax)
fwd.add_scale(length=1e-6, ax=ax, height=1)
cbar = fig.colorbar(img)
cbar.set_label(&#34;NV-PL (kcps)&#34;)

# Save the figure to the figure folder specified earlier
dh.save_figures(filepath=&#34;nv_pl_scan&#34;, fig=fig, only_jpg=True)
```

### Nanonis AFM measurements

```python
afm_measurements = dh.load_measurements(measurement_str=&#34;Scan&#34;, extension=&#34;.sxm&#34;, qudi=False)

afm = afm_measurements[&#34;20230101-0420-00&#34;].data

# Print the channels available in the data
afm.list_channels()
topo = afm.get_channel(&#34;Z&#34;)

fig, ax = plt.subplots()

# Perform (optional) image corrections
topo.correct_lines()
topo.correct_plane()
topo.filter_lowpass(fft_radius=20)
topo.zero_min()

# Add scale bar, color bar and plot the data
img = topo.show(cmap=&#34;inferno&#34;, ax=ax)
topo.add_scale(length=1e-6, ax=ax, height=1, fontsize=10)
cbar = fig.colorbar(img)
cbar.set_label(&#34;Height (nm)&#34;)

dh.save_figures(filepath=&#34;afm_topo&#34;, fig=fig, only_jpg=True)
```

### g(2) measurements (anti-bunching fit)

```python
autocorrelation_measurements = dh.load_measurements(measurement_str=&#34;Autocorrelation&#34;)

fig, ax = plt.subplots()

for autocorrelation in autocorrelation_measurements.values():
    autocorrelation.data[&#34;Time (ns)&#34;] = autocorrelation.data[&#34;Time (ps)&#34;] * 1e-3
    # Plot the data
    sns.lineplot(data=autocorrelation.data, x=&#34;Time (ns)&#34;, y=&#34;g2(t) norm&#34;, ax=ax)
    # Fit the data using the antibunching function
    fit_x, fit_y, result = dh.fit(x=&#34;Time (ns)&#34;, y=&#34;g2(t) norm&#34;, data=autocorrelation.data,
                                  fit_function=dh.fit_function.antibunching)
    # Plot the fit
    sns.lineplot(x=fit_x, y=fit_y, ax=ax, color=&#34;C1&#34;)

# Save the figure to the figure folder specified earlier
dh.save_figures(filepath=&#34;autocorrelation_variation&#34;, fig=fig)
```

### ODMR measurements (double Lorentzian fit)

```python
odmr_measurements = dh.load_measurements(measurement_str=&#34;ODMR&#34;, pulsed=True)

fig, ax = plt.subplots()

for odmr in odmr_measurements.values():
    sns.scatterplot(data=odmr.data, x=&#34;Controlled variable(Hz)&#34;, y=&#34;Signal&#34;, ax=ax)
    fit_x, fit_y, result = dh.fit(x=&#34;Controlled variable(Hz)&#34;, y=&#34;Signal&#34;, data=odmr.data,
                                  fit_function=dh.fit_function.lorentziandouble)
    sns.lineplot(x=fit_x, y=fit_y, ax=ax, color=&#34;C1&#34;)

dh.save_figures(filepath=&#34;odmr_variation&#34;, fig=fig)
```

### Rabi measurements (sine exp. decay fit)

```python
rabi_measurements = dh.load_measurements(measurement_str=&#34;Rabi&#34;, pulsed=True)

fig, ax = plt.subplots()

for rabi in rabi_measurements.values():
    sns.scatterplot(data=rabi.data, x=&#34;Controlled variable(s)&#34;, y=&#34;Signal&#34;, ax=ax)
    fit_x, fit_y, result = dh.fit(x=&#34;Controlled variable(s)&#34;, y=&#34;Signal&#34;, data=rabi.data,
                                  fit_function=dh.fit_function.sineexponentialdecay)
    sns.lineplot(x=fit_x, y=fit_y, ax=ax, color=&#34;C1&#34;)

dh.save_figures(filepath=&#34;rabi_variation&#34;, fig=fig)
```

### Temperature measurements

```python
temperature_measurements = dh.load_measurements(measurement_str=&#34;Temperature&#34;, qudi=False)

temperature = pd.concat([t.data for t in temperature_measurements.values()])

fig, ax = plt.subplots()
sns.lineplot(data=temperature, x=&#34;Time&#34;, y=&#34;Temperature&#34;, ax=ax)
dh.save_figures(filepath=&#34;temperature_monitoring&#34;, fig=fig)
```

### Bruker MFM measurements

```python
bruker_measurements = dh.load_measurements(measurement_str=&#34;&#34;, extension=&#34;.001&#34;, qudi=False)

bruker_data = bruker_measurements[&#34;20230101-0420-00&#34;].data

# Print the channels available in the data
bruker_data.list_channels()
mfm = bruker_data.get_channel(&#34;Phase&#34;, mfm=True)

fig, ax = plt.subplots()

# Perform (optional) image corrections
mfm.correct_plane()
mfm.zero_min()

# Add scale bar, color bar and plot the data
img = mfm.show(cmap=&#34;inferno&#34;, ax=ax)
mfm.add_scale(length=1, ax=ax, height=1, fontsize=10)
cbar = fig.colorbar(img)
cbar.set_label(&#34;MFM contrast (deg)&#34;)

dh.save_figures(filepath=&#34;MFM&#34;, fig=fig, only_jpg=True)
```

### PYS data (pi3diamond compatibility)

```python
pys_measurements = dh.load_measurements(measurement_str=&#34;ndmin&#34;, extension=&#34;.pys&#34;, qudi=False)
pys = pys_measurements[list(pys_measurements)[0]].data

fig, ax = plt.subplots()
sns.lineplot(x=pys[&#34;time_bins&#34;], y=pys[&#34;counts&#34;], ax=ax)
dh.save_figures(filepath=&#34;pys_measurement&#34;, fig=fig)
```

&#34;&#34;&#34;

from .analysis_logic import AnalysisLogic, FitMethodsAndEstimators
from .data_handler import DataHandler
from .io_handler import IOHandler

__all__ = [&#34;DataHandler&#34;, &#34;IOHandler&#34;, &#34;AnalysisLogic&#34;, &#34;FitMethodsAndEstimators&#34;]</code></pre>
</details>
</section>
<section>
<h2 class="section-title" id="header-submodules">Sub-modules</h2>
<dl>
<dt><code class="name"><a title="qudi_hira_analysis.analysis_logic" href="analysis_logic.html">qudi_hira_analysis.analysis_logic</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="qudi_hira_analysis.data_handler" href="data_handler.html">qudi_hira_analysis.data_handler</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="qudi_hira_analysis.helper_functions" href="helper_functions.html">qudi_hira_analysis.helper_functions</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="qudi_hira_analysis.io_handler" href="io_handler.html">qudi_hira_analysis.io_handler</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="qudi_hira_analysis.measurement_dataclass" href="measurement_dataclass.html">qudi_hira_analysis.measurement_dataclass</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="qudi_hira_analysis.AnalysisLogic"><code class="flex name class">
<span>class <span class="ident">AnalysisLogic</span></span>
</code></dt>
<dd>
<div class="desc"><p>Class for performing analysis on measurement data</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class AnalysisLogic(FitLogic):
    &#34;&#34;&#34; Class for performing analysis on measurement data &#34;&#34;&#34;
    fit_function = FitMethodsAndEstimators

    def __init__(self):
        super().__init__()
        self.log = logging.getLogger(__name__)

    def _perform_fit(
            self,
            x: np.ndarray,
            y: np.ndarray,
            fit_function: str,
            estimator: str,
            parameters: list[Parameter] = None,
            dims: str = &#34;1d&#34;) -&gt; Tuple[np.ndarray, np.ndarray, ModelResult]:
        fit = {dims: {&#39;default&#39;: {&#39;fit_function&#39;: fit_function, &#39;estimator&#39;: estimator}}}
        user_fit = self.validate_load_fits(fit)

        if parameters:
            user_fit[dims][&#34;default&#34;][&#34;parameters&#34;].add_many(*parameters)

        use_settings = {}
        for key in user_fit[dims][&#34;default&#34;][&#34;parameters&#34;].keys():
            if parameters:
                if key in [p.name for p in parameters]:
                    use_settings[key] = True
                else:
                    use_settings[key] = False
            else:
                use_settings[key] = False
        user_fit[dims][&#34;default&#34;][&#34;use_settings&#34;] = use_settings

        fc = self.make_fit_container(&#34;test&#34;, dims)
        fc.set_fit_functions(user_fit[dims])
        fc.set_current_fit(&#34;default&#34;)
        fit_x, fit_y, result = fc.do_fit(x, y)
        return fit_x, fit_y, result

    def fit(
            self,
            x: str | np.ndarray | pd.Series,
            y: str | np.ndarray | pd.Series,
            fit_function: FitMethodsAndEstimators,
            data: pd.DataFrame = None,
            parameters: list[Parameter] = None
    ) -&gt; Tuple[np.ndarray, np.ndarray, ModelResult]:
        &#34;&#34;&#34;
        Args:
            x: x data, can be string, numpy array or pandas Series
            y: y data, can be string, numpy array or pandas Series
            fit_function: fit function to use
            data: pandas DataFrame containing x and y data, if None x and y must be numpy arrays or pandas Series
            parameters: list of parameters to use in fit (optional)

        Returns:
            Fit x data, fit y data and lmfit ModelResult
        &#34;&#34;&#34;
        if &#34;twoD&#34; in fit_function[0]:
            dims: str = &#34;2d&#34;
        else:
            dims: str = &#34;1d&#34;

        if data is None:
            if isinstance(x, pd.Series) or isinstance(x, pd.Index):
                x: np.ndarray = x.to_numpy()
            if isinstance(y, pd.Series):
                y: np.ndarray = y.to_numpy()
        elif isinstance(data, pd.DataFrame):
            x: np.ndarray = data[x].to_numpy()
            y: np.ndarray = data[y].to_numpy()
        else:
            raise TypeError(&#34;Data must be a pandas DataFrame or None&#34;)

        return self._perform_fit(
            x=x,
            y=y,
            fit_function=fit_function[0],
            estimator=fit_function[1],
            parameters=parameters,
            dims=dims
        )

    def get_all_fits(self) -&gt; Tuple[list, list]:
        &#34;&#34;&#34;Get all available fits

        Returns:
            Tuple with list of 1d and 2d fits
        &#34;&#34;&#34;
        one_d_fits: list = list(self.fit_list[&#39;1d&#39;].keys())
        two_d_fits: list = list(self.fit_list[&#39;2d&#39;].keys())
        self.log.info(f&#34;1d fits: {one_d_fits}\n2d fits: {two_d_fits}&#34;)
        return one_d_fits, two_d_fits

    @staticmethod
    def analyze_mean(
            laser_data: np.ndarray,
            signal_start: float = 100e-9,
            signal_end: float = 300e-9,
            bin_width: float = 1e-9
    ) -&gt; Tuple[np.ndarray, np.ndarray]:
        &#34;&#34;&#34;
        Calculate the mean of the signal window.

        Args:
            laser_data: 2D array of laser data
            signal_start: start of the signal window in seconds
            signal_end: end of the signal window in seconds
            bin_width: width of a bin in seconds

        Returns:
            Mean of the signal window and measurement error
        &#34;&#34;&#34;
        # Get number of lasers
        num_of_lasers = laser_data.shape[0]

        if not isinstance(bin_width, float):
            return np.zeros(num_of_lasers), np.zeros(num_of_lasers)

        # Convert the times in seconds to bins (i.e. array indices)
        signal_start_bin = round(signal_start / bin_width)
        signal_end_bin = round(signal_end / bin_width)

        # initialize data arrays for signal and measurement error
        signal_data = np.empty(num_of_lasers, dtype=float)
        error_data = np.empty(num_of_lasers, dtype=float)

        # loop over all laser pulses and analyze them
        for ii, laser_arr in enumerate(laser_data):
            # calculate the mean of the data in the signal window
            signal = laser_arr[signal_start_bin:signal_end_bin].mean()
            signal_sum = laser_arr[signal_start_bin:signal_end_bin].sum()
            signal_error = np.sqrt(signal_sum) / (signal_end_bin - signal_start_bin)

            # Avoid numpy C type variables overflow and NaN values
            if signal &lt; 0 or signal != signal:
                signal_data[ii] = 0.0
                error_data[ii] = 0.0
            else:
                signal_data[ii] = signal
                error_data[ii] = signal_error

        return signal_data, error_data

    @staticmethod
    def analyze_mean_reference(
            laser_data: np.ndarray,
            signal_start: float = 100e-9,
            signal_end: float = 300e-9,
            norm_start: float = 1000e-9,
            norm_end: float = 2000e-9,
            bin_width: float = 1e-9) -&gt; Tuple[np.ndarray, np.ndarray]:
        &#34;&#34;&#34;
        Subtracts the mean of the signal window from the mean of the reference window.

        Args:
            laser_data: 2D array of laser data
            signal_start: start of the signal window in seconds
            signal_end: end of the signal window in seconds
            norm_start: start of the reference window in seconds
            norm_end: end of the reference window in seconds
            bin_width: width of a bin in seconds

        Returns:
            Referenced mean of the signal window and measurement error
        &#34;&#34;&#34;
        # Get number of lasers
        num_of_lasers = laser_data.shape[0]

        if not isinstance(bin_width, float):
            return np.zeros(num_of_lasers), np.zeros(num_of_lasers)

        # Convert the times in seconds to bins (i.e. array indices)
        signal_start_bin = round(signal_start / bin_width)
        signal_end_bin = round(signal_end / bin_width)
        norm_start_bin = round(norm_start / bin_width)
        norm_end_bin = round(norm_end / bin_width)

        # initialize data arrays for signal and measurement error
        signal_data = np.empty(num_of_lasers, dtype=float)
        error_data = np.empty(num_of_lasers, dtype=float)

        # loop over all laser pulses and analyze them
        for ii, laser_arr in enumerate(laser_data):
            # calculate the sum and mean of the data in the normalization window
            tmp_data = laser_arr[norm_start_bin:norm_end_bin]
            reference_sum = np.sum(tmp_data)
            reference_mean = (reference_sum / len(tmp_data)) if len(tmp_data) != 0 else 0.0

            # calculate the sum and mean of the data in the signal window
            tmp_data = laser_arr[signal_start_bin:signal_end_bin]
            signal_sum = np.sum(tmp_data)
            signal_mean = (signal_sum / len(tmp_data)) if len(tmp_data) != 0 else 0.0

            signal_data[ii] = signal_mean - reference_mean

            # calculate with respect to gaussian error &#39;evolution&#39;
            error_data[ii] = signal_data[ii] * np.sqrt(1 / abs(signal_sum) + 1 / abs(reference_sum))

        return signal_data, error_data

    @staticmethod
    def analyze_mean_norm(
            laser_data: np.ndarray,
            signal_start: float = 100e-9,
            signal_end: float = 300e-9,
            norm_start: float = 1000e-9,
            norm_end=2000e-9,
            bin_width: float = 1e-9
    ) -&gt; Tuple[np.ndarray, np.ndarray]:
        &#34;&#34;&#34;
        Divides the mean of the signal window from the mean of the reference window.

        Args:
            laser_data: 2D array of laser data
            signal_start: start of the signal window in seconds
            signal_end: end of the signal window in seconds
            norm_start: start of the reference window in seconds
            norm_end: end of the reference window in seconds
            bin_width: width of a bin in seconds

        Returns:
            Normalized mean of the signal window and measurement error
        &#34;&#34;&#34;
        # Get number of lasers
        num_of_lasers = laser_data.shape[0]

        if not isinstance(bin_width, float):
            return np.zeros(num_of_lasers), np.zeros(num_of_lasers)

        # Convert the times in seconds to bins (i.e. array indices)
        signal_start_bin = round(signal_start / bin_width)
        signal_end_bin = round(signal_end / bin_width)
        norm_start_bin = round(norm_start / bin_width)
        norm_end_bin = round(norm_end / bin_width)

        # initialize data arrays for signal and measurement error
        signal_data = np.empty(num_of_lasers, dtype=float)
        error_data = np.empty(num_of_lasers, dtype=float)

        # loop over all laser pulses and analyze them
        for ii, laser_arr in enumerate(laser_data):
            # calculate the sum and mean of the data in the normalization window
            tmp_data = laser_arr[norm_start_bin:norm_end_bin]
            reference_sum = np.sum(tmp_data)
            reference_mean = (reference_sum / len(tmp_data)) if len(tmp_data) != 0 else 0.0

            # calculate the sum and mean of the data in the signal window
            tmp_data = laser_arr[signal_start_bin:signal_end_bin]
            signal_sum = np.sum(tmp_data)
            signal_mean = (signal_sum / len(tmp_data)) if len(tmp_data) != 0 else 0.0

            # Calculate normalized signal while avoiding division by zero
            if reference_mean &gt; 0 and signal_mean &gt;= 0:
                signal_data[ii] = signal_mean / reference_mean
            else:
                signal_data[ii] = 0.0

            # Calculate measurement error while avoiding division by zero
            if reference_sum &gt; 0 and signal_sum &gt; 0:
                # calculate with respect to gaussian error &#39;evolution&#39;
                error_data[ii] = signal_data[ii] * np.sqrt(1 / signal_sum + 1 / reference_sum)
            else:
                error_data[ii] = 0.0

        return signal_data, error_data

    def optimize_raster_odmr_params(
            self,
            measurements: dict[str, MeasurementDataclass],
            num_samples: int = 10,
            num_params: int = 3,
    ) -&gt; Tuple[float, Tuple[float, float, float]]:
        &#34;&#34;&#34;
        This method optimizes the hyperparameters of the ODMR analysis.
        It does so by randomly sampling a subset of the measurements and
        then optimizing the hyperparameters for them.

        Args:
            measurements: A dictionary of measurements to optimize the hyperparameters for.
            num_params: The number of parameters to optimize.
            num_samples: The number of measurements to sample.

        Returns:
            The highest minimum R2 value and the optimized hyperparameters.
        &#34;&#34;&#34;
        r2_threshs: np.ndarray = np.around(np.linspace(start=0.9, stop=0.99, num=num_params), decimals=2)
        thresh_fracs: np.ndarray = np.around(np.linspace(start=0.5, stop=0.9, num=num_params), decimals=1)
        sigma_thresh_fracs: np.ndarray = np.around(np.linspace(start=0.1, stop=0.2, num=num_params), decimals=1)

        odmr_sample: dict = {}
        for k, v in random.sample(sorted(measurements.items()), k=num_samples):
            odmr_sample[k] = v

        highest_min_r2: float = 0
        optimal_params: Tuple[float, float, float] = (0, 0, 0)

        for idx, (r2_thresh, thresh_frac, sigma_thresh_frac) in enumerate(
                product(r2_threshs, thresh_fracs, sigma_thresh_fracs)):
            odmr_sample = self.fit_raster_odmr(
                odmr_sample,
                r2_thresh=r2_thresh,
                thresh_frac=thresh_frac,
                sigma_thresh_frac=sigma_thresh_frac,
                min_thresh=0.01,
                progress_bar=False
            )

            r2s: np.ndarray = np.zeros(len(odmr_sample))
            for _idx, odmr in enumerate(odmr_sample.values()):
                r2s[_idx] = odmr.fit_model.rsquared
            min_r2: float = np.min(r2s)

            if highest_min_r2 &lt; min_r2:
                highest_min_r2 = min_r2
                optimal_params = (r2_thresh, thresh_frac, sigma_thresh_frac)

        return highest_min_r2, optimal_params

    @staticmethod
    def fit_raster_odmr(
            odmr_measurements: dict[str, MeasurementDataclass],
            r2_thresh: float = 0.95,
            thresh_frac: float = 0.5,
            sigma_thresh_frac: float = 0.15,
            min_thresh: float = 0.01,
            extract_pixel_from_filename: bool = True,
            progress_bar: bool = True
    ) -&gt; dict[str, MeasurementDataclass]:
        &#34;&#34;&#34;
        Fit a list of ODMR data to single and double Lorentzian functions

        Args:
            odmr_measurements: Dict of ODMR data in MeasurementDataclasses
            r2_thresh: R^2 Threshold below which a double lorentzian is fitted instead of a single lorentzian
            thresh_frac: Threshold fraction for the peak finding
            min_thresh: Minimum threshold for the peak finding
            sigma_thresh_frac: Change in threshold fraction for the peak finding
            extract_pixel_from_filename: Extract `(row, col)` (in this format) from filename
            progress_bar: Show progress bar

        Returns:
            Dict of ODMR MeasurementDataclass with fit, fit model and pixels attributes set
        &#34;&#34;&#34;

        model1, base_params1 = rof.make_lorentzian_model()
        model2, base_params2 = rof.make_lorentziandouble_model()

        # Generate arguments for the parallel fitting
        args = []
        for odmr in tqdm(odmr_measurements.values(), disable=not progress_bar):
            x = odmr.data[&#34;Freq(MHz)&#34;].to_numpy()
            y = odmr.data[&#34;Counts&#34;].to_numpy()
            _, params1 = rof.estimate_lorentzian_dip(x, y, base_params1)
            _, params2 = rof.estimate_lorentziandouble_dip(x, y, base_params2, thresh_frac, min_thresh,
                                                           sigma_thresh_frac)
            args.append((x, y, model1, model2, params1, params2, r2_thresh))

        # Parallel fitting
        model_results = Parallel(n_jobs=cpu_count())(
            delayed(rof.lorentzian_fitting)(
                x, y, model1, model2, params1, params2, r2_thresh) for x, y, model1, model2, params1, params2, r2_thresh
            in
            tqdm(args, disable=not progress_bar)
        )

        x = list(odmr_measurements.values())[0].data[&#34;Freq(MHz)&#34;].to_numpy()
        x_fit = np.linspace(start=x[0], stop=x[-1], num=int(len(x) * 2))

        for odmr, res in zip(odmr_measurements.values(), model_results):

            if len(res.params) == 6:
                # Fit to a single Lorentzian
                y_fit = model1.eval(x=x_fit, params=res.params)
            else:
                # Fit to a double Lorentzian
                y_fit = model2.eval(x=x_fit, params=res.params)

            # Plug results into the DataClass
            odmr.fit_model = res
            odmr.fit_data = pd.DataFrame(np.vstack((x_fit, y_fit)).T, columns=[&#34;x_fit&#34;, &#34;y_fit&#34;])

            if extract_pixel_from_filename:
                row, col = map(int, re.findall(r&#39;(?&lt;=\().*?(?=\))&#39;, odmr.filename)[0].split(&#34;,&#34;))
                odmr.xy_position = (row, col)

        return odmr_measurements

    @staticmethod
    def average_raster_odmr_pixels(orig_image: np.ndarray) -&gt; np.ndarray:
        &#34;&#34;&#34; Average a NaN pixel to its surrounding pixels.

        Args:
            orig_image: Image with NaN pixels

        Returns:
            Image with NaN pixels replaced by the average of its surrounding pixels
        &#34;&#34;&#34;
        image: np.ndarray = orig_image.copy()
        for row, col in np.argwhere(np.isnan(image)):
            if row == 0:
                pixel_avg = np.nanmean(image[row + 1:row + 2, col - 1:col + 2])
            elif row == image.shape[0] - 1:
                pixel_avg = np.nanmean(image[row - 1:row, col - 1:col + 2])
            elif col == 0:
                pixel_avg = np.nanmean(image[row - 1:row + 2, col + 1:col + 2])
            elif col == image.shape[1] - 1:
                pixel_avg = np.nanmean(image[row - 1:row + 2, col - 1:col])
            else:
                pixel_avg = np.nanmean(image[row - 1:row + 2, col - 1:col + 2])

            image[row, col] = pixel_avg
        return image

    # Aliases for backwards compatibility
    analyse_mean = analyze_mean
    analyse_mean_norm = analyze_mean_norm
    analyse_mean_reference = analyze_mean_reference</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>qudi_hira_analysis._qudi_fit_logic.FitLogic</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="qudi_hira_analysis.data_handler.DataHandler" href="data_handler.html#qudi_hira_analysis.data_handler.DataHandler">DataHandler</a></li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="qudi_hira_analysis.AnalysisLogic.fit_function"><code class="name">var <span class="ident">fit_function</span></code></dt>
<dd>
<div class="desc"><p>Class for storing fit methods and estimators.
Fit methods are stored as tuples of (method, estimator)
where method is the name of the fit method and estimator is the name of the estimator.</p></div>
</dd>
</dl>
<h3>Static methods</h3>
<dl>
<dt id="qudi_hira_analysis.AnalysisLogic.analyse_mean"><code class="name flex">
<span>def <span class="ident">analyse_mean</span></span>(<span>laser_data: np.ndarray, signal_start: float = 1e-07, signal_end: float = 3e-07, bin_width: float = 1e-09) ‑> Tuple[numpy.ndarray, numpy.ndarray]</span>
</code></dt>
<dd>
<div class="desc"><p>Calculate the mean of the signal window.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>laser_data</code></strong></dt>
<dd>2D array of laser data</dd>
<dt><strong><code>signal_start</code></strong></dt>
<dd>start of the signal window in seconds</dd>
<dt><strong><code>signal_end</code></strong></dt>
<dd>end of the signal window in seconds</dd>
<dt><strong><code>bin_width</code></strong></dt>
<dd>width of a bin in seconds</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Mean of the signal window and measurement error</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def analyze_mean(
        laser_data: np.ndarray,
        signal_start: float = 100e-9,
        signal_end: float = 300e-9,
        bin_width: float = 1e-9
) -&gt; Tuple[np.ndarray, np.ndarray]:
    &#34;&#34;&#34;
    Calculate the mean of the signal window.

    Args:
        laser_data: 2D array of laser data
        signal_start: start of the signal window in seconds
        signal_end: end of the signal window in seconds
        bin_width: width of a bin in seconds

    Returns:
        Mean of the signal window and measurement error
    &#34;&#34;&#34;
    # Get number of lasers
    num_of_lasers = laser_data.shape[0]

    if not isinstance(bin_width, float):
        return np.zeros(num_of_lasers), np.zeros(num_of_lasers)

    # Convert the times in seconds to bins (i.e. array indices)
    signal_start_bin = round(signal_start / bin_width)
    signal_end_bin = round(signal_end / bin_width)

    # initialize data arrays for signal and measurement error
    signal_data = np.empty(num_of_lasers, dtype=float)
    error_data = np.empty(num_of_lasers, dtype=float)

    # loop over all laser pulses and analyze them
    for ii, laser_arr in enumerate(laser_data):
        # calculate the mean of the data in the signal window
        signal = laser_arr[signal_start_bin:signal_end_bin].mean()
        signal_sum = laser_arr[signal_start_bin:signal_end_bin].sum()
        signal_error = np.sqrt(signal_sum) / (signal_end_bin - signal_start_bin)

        # Avoid numpy C type variables overflow and NaN values
        if signal &lt; 0 or signal != signal:
            signal_data[ii] = 0.0
            error_data[ii] = 0.0
        else:
            signal_data[ii] = signal
            error_data[ii] = signal_error

    return signal_data, error_data</code></pre>
</details>
</dd>
<dt id="qudi_hira_analysis.AnalysisLogic.analyse_mean_norm"><code class="name flex">
<span>def <span class="ident">analyse_mean_norm</span></span>(<span>laser_data: np.ndarray, signal_start: float = 1e-07, signal_end: float = 3e-07, norm_start: float = 1e-06, norm_end=2e-06, bin_width: float = 1e-09) ‑> Tuple[numpy.ndarray, numpy.ndarray]</span>
</code></dt>
<dd>
<div class="desc"><p>Divides the mean of the signal window from the mean of the reference window.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>laser_data</code></strong></dt>
<dd>2D array of laser data</dd>
<dt><strong><code>signal_start</code></strong></dt>
<dd>start of the signal window in seconds</dd>
<dt><strong><code>signal_end</code></strong></dt>
<dd>end of the signal window in seconds</dd>
<dt><strong><code>norm_start</code></strong></dt>
<dd>start of the reference window in seconds</dd>
<dt><strong><code>norm_end</code></strong></dt>
<dd>end of the reference window in seconds</dd>
<dt><strong><code>bin_width</code></strong></dt>
<dd>width of a bin in seconds</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Normalized mean of the signal window and measurement error</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def analyze_mean_norm(
        laser_data: np.ndarray,
        signal_start: float = 100e-9,
        signal_end: float = 300e-9,
        norm_start: float = 1000e-9,
        norm_end=2000e-9,
        bin_width: float = 1e-9
) -&gt; Tuple[np.ndarray, np.ndarray]:
    &#34;&#34;&#34;
    Divides the mean of the signal window from the mean of the reference window.

    Args:
        laser_data: 2D array of laser data
        signal_start: start of the signal window in seconds
        signal_end: end of the signal window in seconds
        norm_start: start of the reference window in seconds
        norm_end: end of the reference window in seconds
        bin_width: width of a bin in seconds

    Returns:
        Normalized mean of the signal window and measurement error
    &#34;&#34;&#34;
    # Get number of lasers
    num_of_lasers = laser_data.shape[0]

    if not isinstance(bin_width, float):
        return np.zeros(num_of_lasers), np.zeros(num_of_lasers)

    # Convert the times in seconds to bins (i.e. array indices)
    signal_start_bin = round(signal_start / bin_width)
    signal_end_bin = round(signal_end / bin_width)
    norm_start_bin = round(norm_start / bin_width)
    norm_end_bin = round(norm_end / bin_width)

    # initialize data arrays for signal and measurement error
    signal_data = np.empty(num_of_lasers, dtype=float)
    error_data = np.empty(num_of_lasers, dtype=float)

    # loop over all laser pulses and analyze them
    for ii, laser_arr in enumerate(laser_data):
        # calculate the sum and mean of the data in the normalization window
        tmp_data = laser_arr[norm_start_bin:norm_end_bin]
        reference_sum = np.sum(tmp_data)
        reference_mean = (reference_sum / len(tmp_data)) if len(tmp_data) != 0 else 0.0

        # calculate the sum and mean of the data in the signal window
        tmp_data = laser_arr[signal_start_bin:signal_end_bin]
        signal_sum = np.sum(tmp_data)
        signal_mean = (signal_sum / len(tmp_data)) if len(tmp_data) != 0 else 0.0

        # Calculate normalized signal while avoiding division by zero
        if reference_mean &gt; 0 and signal_mean &gt;= 0:
            signal_data[ii] = signal_mean / reference_mean
        else:
            signal_data[ii] = 0.0

        # Calculate measurement error while avoiding division by zero
        if reference_sum &gt; 0 and signal_sum &gt; 0:
            # calculate with respect to gaussian error &#39;evolution&#39;
            error_data[ii] = signal_data[ii] * np.sqrt(1 / signal_sum + 1 / reference_sum)
        else:
            error_data[ii] = 0.0

    return signal_data, error_data</code></pre>
</details>
</dd>
<dt id="qudi_hira_analysis.AnalysisLogic.analyse_mean_reference"><code class="name flex">
<span>def <span class="ident">analyse_mean_reference</span></span>(<span>laser_data: np.ndarray, signal_start: float = 1e-07, signal_end: float = 3e-07, norm_start: float = 1e-06, norm_end: float = 2e-06, bin_width: float = 1e-09) ‑> Tuple[numpy.ndarray, numpy.ndarray]</span>
</code></dt>
<dd>
<div class="desc"><p>Subtracts the mean of the signal window from the mean of the reference window.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>laser_data</code></strong></dt>
<dd>2D array of laser data</dd>
<dt><strong><code>signal_start</code></strong></dt>
<dd>start of the signal window in seconds</dd>
<dt><strong><code>signal_end</code></strong></dt>
<dd>end of the signal window in seconds</dd>
<dt><strong><code>norm_start</code></strong></dt>
<dd>start of the reference window in seconds</dd>
<dt><strong><code>norm_end</code></strong></dt>
<dd>end of the reference window in seconds</dd>
<dt><strong><code>bin_width</code></strong></dt>
<dd>width of a bin in seconds</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Referenced mean of the signal window and measurement error</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def analyze_mean_reference(
        laser_data: np.ndarray,
        signal_start: float = 100e-9,
        signal_end: float = 300e-9,
        norm_start: float = 1000e-9,
        norm_end: float = 2000e-9,
        bin_width: float = 1e-9) -&gt; Tuple[np.ndarray, np.ndarray]:
    &#34;&#34;&#34;
    Subtracts the mean of the signal window from the mean of the reference window.

    Args:
        laser_data: 2D array of laser data
        signal_start: start of the signal window in seconds
        signal_end: end of the signal window in seconds
        norm_start: start of the reference window in seconds
        norm_end: end of the reference window in seconds
        bin_width: width of a bin in seconds

    Returns:
        Referenced mean of the signal window and measurement error
    &#34;&#34;&#34;
    # Get number of lasers
    num_of_lasers = laser_data.shape[0]

    if not isinstance(bin_width, float):
        return np.zeros(num_of_lasers), np.zeros(num_of_lasers)

    # Convert the times in seconds to bins (i.e. array indices)
    signal_start_bin = round(signal_start / bin_width)
    signal_end_bin = round(signal_end / bin_width)
    norm_start_bin = round(norm_start / bin_width)
    norm_end_bin = round(norm_end / bin_width)

    # initialize data arrays for signal and measurement error
    signal_data = np.empty(num_of_lasers, dtype=float)
    error_data = np.empty(num_of_lasers, dtype=float)

    # loop over all laser pulses and analyze them
    for ii, laser_arr in enumerate(laser_data):
        # calculate the sum and mean of the data in the normalization window
        tmp_data = laser_arr[norm_start_bin:norm_end_bin]
        reference_sum = np.sum(tmp_data)
        reference_mean = (reference_sum / len(tmp_data)) if len(tmp_data) != 0 else 0.0

        # calculate the sum and mean of the data in the signal window
        tmp_data = laser_arr[signal_start_bin:signal_end_bin]
        signal_sum = np.sum(tmp_data)
        signal_mean = (signal_sum / len(tmp_data)) if len(tmp_data) != 0 else 0.0

        signal_data[ii] = signal_mean - reference_mean

        # calculate with respect to gaussian error &#39;evolution&#39;
        error_data[ii] = signal_data[ii] * np.sqrt(1 / abs(signal_sum) + 1 / abs(reference_sum))

    return signal_data, error_data</code></pre>
</details>
</dd>
<dt id="qudi_hira_analysis.AnalysisLogic.analyze_mean"><code class="name flex">
<span>def <span class="ident">analyze_mean</span></span>(<span>laser_data: np.ndarray, signal_start: float = 1e-07, signal_end: float = 3e-07, bin_width: float = 1e-09) ‑> Tuple[numpy.ndarray, numpy.ndarray]</span>
</code></dt>
<dd>
<div class="desc"><p>Calculate the mean of the signal window.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>laser_data</code></strong></dt>
<dd>2D array of laser data</dd>
<dt><strong><code>signal_start</code></strong></dt>
<dd>start of the signal window in seconds</dd>
<dt><strong><code>signal_end</code></strong></dt>
<dd>end of the signal window in seconds</dd>
<dt><strong><code>bin_width</code></strong></dt>
<dd>width of a bin in seconds</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Mean of the signal window and measurement error</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def analyze_mean(
        laser_data: np.ndarray,
        signal_start: float = 100e-9,
        signal_end: float = 300e-9,
        bin_width: float = 1e-9
) -&gt; Tuple[np.ndarray, np.ndarray]:
    &#34;&#34;&#34;
    Calculate the mean of the signal window.

    Args:
        laser_data: 2D array of laser data
        signal_start: start of the signal window in seconds
        signal_end: end of the signal window in seconds
        bin_width: width of a bin in seconds

    Returns:
        Mean of the signal window and measurement error
    &#34;&#34;&#34;
    # Get number of lasers
    num_of_lasers = laser_data.shape[0]

    if not isinstance(bin_width, float):
        return np.zeros(num_of_lasers), np.zeros(num_of_lasers)

    # Convert the times in seconds to bins (i.e. array indices)
    signal_start_bin = round(signal_start / bin_width)
    signal_end_bin = round(signal_end / bin_width)

    # initialize data arrays for signal and measurement error
    signal_data = np.empty(num_of_lasers, dtype=float)
    error_data = np.empty(num_of_lasers, dtype=float)

    # loop over all laser pulses and analyze them
    for ii, laser_arr in enumerate(laser_data):
        # calculate the mean of the data in the signal window
        signal = laser_arr[signal_start_bin:signal_end_bin].mean()
        signal_sum = laser_arr[signal_start_bin:signal_end_bin].sum()
        signal_error = np.sqrt(signal_sum) / (signal_end_bin - signal_start_bin)

        # Avoid numpy C type variables overflow and NaN values
        if signal &lt; 0 or signal != signal:
            signal_data[ii] = 0.0
            error_data[ii] = 0.0
        else:
            signal_data[ii] = signal
            error_data[ii] = signal_error

    return signal_data, error_data</code></pre>
</details>
</dd>
<dt id="qudi_hira_analysis.AnalysisLogic.analyze_mean_norm"><code class="name flex">
<span>def <span class="ident">analyze_mean_norm</span></span>(<span>laser_data: np.ndarray, signal_start: float = 1e-07, signal_end: float = 3e-07, norm_start: float = 1e-06, norm_end=2e-06, bin_width: float = 1e-09) ‑> Tuple[numpy.ndarray, numpy.ndarray]</span>
</code></dt>
<dd>
<div class="desc"><p>Divides the mean of the signal window from the mean of the reference window.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>laser_data</code></strong></dt>
<dd>2D array of laser data</dd>
<dt><strong><code>signal_start</code></strong></dt>
<dd>start of the signal window in seconds</dd>
<dt><strong><code>signal_end</code></strong></dt>
<dd>end of the signal window in seconds</dd>
<dt><strong><code>norm_start</code></strong></dt>
<dd>start of the reference window in seconds</dd>
<dt><strong><code>norm_end</code></strong></dt>
<dd>end of the reference window in seconds</dd>
<dt><strong><code>bin_width</code></strong></dt>
<dd>width of a bin in seconds</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Normalized mean of the signal window and measurement error</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def analyze_mean_norm(
        laser_data: np.ndarray,
        signal_start: float = 100e-9,
        signal_end: float = 300e-9,
        norm_start: float = 1000e-9,
        norm_end=2000e-9,
        bin_width: float = 1e-9
) -&gt; Tuple[np.ndarray, np.ndarray]:
    &#34;&#34;&#34;
    Divides the mean of the signal window from the mean of the reference window.

    Args:
        laser_data: 2D array of laser data
        signal_start: start of the signal window in seconds
        signal_end: end of the signal window in seconds
        norm_start: start of the reference window in seconds
        norm_end: end of the reference window in seconds
        bin_width: width of a bin in seconds

    Returns:
        Normalized mean of the signal window and measurement error
    &#34;&#34;&#34;
    # Get number of lasers
    num_of_lasers = laser_data.shape[0]

    if not isinstance(bin_width, float):
        return np.zeros(num_of_lasers), np.zeros(num_of_lasers)

    # Convert the times in seconds to bins (i.e. array indices)
    signal_start_bin = round(signal_start / bin_width)
    signal_end_bin = round(signal_end / bin_width)
    norm_start_bin = round(norm_start / bin_width)
    norm_end_bin = round(norm_end / bin_width)

    # initialize data arrays for signal and measurement error
    signal_data = np.empty(num_of_lasers, dtype=float)
    error_data = np.empty(num_of_lasers, dtype=float)

    # loop over all laser pulses and analyze them
    for ii, laser_arr in enumerate(laser_data):
        # calculate the sum and mean of the data in the normalization window
        tmp_data = laser_arr[norm_start_bin:norm_end_bin]
        reference_sum = np.sum(tmp_data)
        reference_mean = (reference_sum / len(tmp_data)) if len(tmp_data) != 0 else 0.0

        # calculate the sum and mean of the data in the signal window
        tmp_data = laser_arr[signal_start_bin:signal_end_bin]
        signal_sum = np.sum(tmp_data)
        signal_mean = (signal_sum / len(tmp_data)) if len(tmp_data) != 0 else 0.0

        # Calculate normalized signal while avoiding division by zero
        if reference_mean &gt; 0 and signal_mean &gt;= 0:
            signal_data[ii] = signal_mean / reference_mean
        else:
            signal_data[ii] = 0.0

        # Calculate measurement error while avoiding division by zero
        if reference_sum &gt; 0 and signal_sum &gt; 0:
            # calculate with respect to gaussian error &#39;evolution&#39;
            error_data[ii] = signal_data[ii] * np.sqrt(1 / signal_sum + 1 / reference_sum)
        else:
            error_data[ii] = 0.0

    return signal_data, error_data</code></pre>
</details>
</dd>
<dt id="qudi_hira_analysis.AnalysisLogic.analyze_mean_reference"><code class="name flex">
<span>def <span class="ident">analyze_mean_reference</span></span>(<span>laser_data: np.ndarray, signal_start: float = 1e-07, signal_end: float = 3e-07, norm_start: float = 1e-06, norm_end: float = 2e-06, bin_width: float = 1e-09) ‑> Tuple[numpy.ndarray, numpy.ndarray]</span>
</code></dt>
<dd>
<div class="desc"><p>Subtracts the mean of the signal window from the mean of the reference window.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>laser_data</code></strong></dt>
<dd>2D array of laser data</dd>
<dt><strong><code>signal_start</code></strong></dt>
<dd>start of the signal window in seconds</dd>
<dt><strong><code>signal_end</code></strong></dt>
<dd>end of the signal window in seconds</dd>
<dt><strong><code>norm_start</code></strong></dt>
<dd>start of the reference window in seconds</dd>
<dt><strong><code>norm_end</code></strong></dt>
<dd>end of the reference window in seconds</dd>
<dt><strong><code>bin_width</code></strong></dt>
<dd>width of a bin in seconds</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Referenced mean of the signal window and measurement error</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def analyze_mean_reference(
        laser_data: np.ndarray,
        signal_start: float = 100e-9,
        signal_end: float = 300e-9,
        norm_start: float = 1000e-9,
        norm_end: float = 2000e-9,
        bin_width: float = 1e-9) -&gt; Tuple[np.ndarray, np.ndarray]:
    &#34;&#34;&#34;
    Subtracts the mean of the signal window from the mean of the reference window.

    Args:
        laser_data: 2D array of laser data
        signal_start: start of the signal window in seconds
        signal_end: end of the signal window in seconds
        norm_start: start of the reference window in seconds
        norm_end: end of the reference window in seconds
        bin_width: width of a bin in seconds

    Returns:
        Referenced mean of the signal window and measurement error
    &#34;&#34;&#34;
    # Get number of lasers
    num_of_lasers = laser_data.shape[0]

    if not isinstance(bin_width, float):
        return np.zeros(num_of_lasers), np.zeros(num_of_lasers)

    # Convert the times in seconds to bins (i.e. array indices)
    signal_start_bin = round(signal_start / bin_width)
    signal_end_bin = round(signal_end / bin_width)
    norm_start_bin = round(norm_start / bin_width)
    norm_end_bin = round(norm_end / bin_width)

    # initialize data arrays for signal and measurement error
    signal_data = np.empty(num_of_lasers, dtype=float)
    error_data = np.empty(num_of_lasers, dtype=float)

    # loop over all laser pulses and analyze them
    for ii, laser_arr in enumerate(laser_data):
        # calculate the sum and mean of the data in the normalization window
        tmp_data = laser_arr[norm_start_bin:norm_end_bin]
        reference_sum = np.sum(tmp_data)
        reference_mean = (reference_sum / len(tmp_data)) if len(tmp_data) != 0 else 0.0

        # calculate the sum and mean of the data in the signal window
        tmp_data = laser_arr[signal_start_bin:signal_end_bin]
        signal_sum = np.sum(tmp_data)
        signal_mean = (signal_sum / len(tmp_data)) if len(tmp_data) != 0 else 0.0

        signal_data[ii] = signal_mean - reference_mean

        # calculate with respect to gaussian error &#39;evolution&#39;
        error_data[ii] = signal_data[ii] * np.sqrt(1 / abs(signal_sum) + 1 / abs(reference_sum))

    return signal_data, error_data</code></pre>
</details>
</dd>
<dt id="qudi_hira_analysis.AnalysisLogic.average_raster_odmr_pixels"><code class="name flex">
<span>def <span class="ident">average_raster_odmr_pixels</span></span>(<span>orig_image: np.ndarray) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Average a NaN pixel to its surrounding pixels.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>orig_image</code></strong></dt>
<dd>Image with NaN pixels</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Image with NaN pixels replaced by the average of its surrounding pixels</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def average_raster_odmr_pixels(orig_image: np.ndarray) -&gt; np.ndarray:
    &#34;&#34;&#34; Average a NaN pixel to its surrounding pixels.

    Args:
        orig_image: Image with NaN pixels

    Returns:
        Image with NaN pixels replaced by the average of its surrounding pixels
    &#34;&#34;&#34;
    image: np.ndarray = orig_image.copy()
    for row, col in np.argwhere(np.isnan(image)):
        if row == 0:
            pixel_avg = np.nanmean(image[row + 1:row + 2, col - 1:col + 2])
        elif row == image.shape[0] - 1:
            pixel_avg = np.nanmean(image[row - 1:row, col - 1:col + 2])
        elif col == 0:
            pixel_avg = np.nanmean(image[row - 1:row + 2, col + 1:col + 2])
        elif col == image.shape[1] - 1:
            pixel_avg = np.nanmean(image[row - 1:row + 2, col - 1:col])
        else:
            pixel_avg = np.nanmean(image[row - 1:row + 2, col - 1:col + 2])

        image[row, col] = pixel_avg
    return image</code></pre>
</details>
</dd>
<dt id="qudi_hira_analysis.AnalysisLogic.fit_raster_odmr"><code class="name flex">
<span>def <span class="ident">fit_raster_odmr</span></span>(<span>odmr_measurements: dict[str, MeasurementDataclass], r2_thresh: float = 0.95, thresh_frac: float = 0.5, sigma_thresh_frac: float = 0.15, min_thresh: float = 0.01, extract_pixel_from_filename: bool = True, progress_bar: bool = True) ‑> dict[str, MeasurementDataclass]</span>
</code></dt>
<dd>
<div class="desc"><p>Fit a list of ODMR data to single and double Lorentzian functions</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>odmr_measurements</code></strong></dt>
<dd>Dict of ODMR data in MeasurementDataclasses</dd>
<dt><strong><code>r2_thresh</code></strong></dt>
<dd>R^2 Threshold below which a double lorentzian is fitted instead of a single lorentzian</dd>
<dt><strong><code>thresh_frac</code></strong></dt>
<dd>Threshold fraction for the peak finding</dd>
<dt><strong><code>min_thresh</code></strong></dt>
<dd>Minimum threshold for the peak finding</dd>
<dt><strong><code>sigma_thresh_frac</code></strong></dt>
<dd>Change in threshold fraction for the peak finding</dd>
<dt><strong><code>extract_pixel_from_filename</code></strong></dt>
<dd>Extract <code>(row, col)</code> (in this format) from filename</dd>
<dt><strong><code>progress_bar</code></strong></dt>
<dd>Show progress bar</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Dict of ODMR MeasurementDataclass with fit, fit model and pixels attributes set</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def fit_raster_odmr(
        odmr_measurements: dict[str, MeasurementDataclass],
        r2_thresh: float = 0.95,
        thresh_frac: float = 0.5,
        sigma_thresh_frac: float = 0.15,
        min_thresh: float = 0.01,
        extract_pixel_from_filename: bool = True,
        progress_bar: bool = True
) -&gt; dict[str, MeasurementDataclass]:
    &#34;&#34;&#34;
    Fit a list of ODMR data to single and double Lorentzian functions

    Args:
        odmr_measurements: Dict of ODMR data in MeasurementDataclasses
        r2_thresh: R^2 Threshold below which a double lorentzian is fitted instead of a single lorentzian
        thresh_frac: Threshold fraction for the peak finding
        min_thresh: Minimum threshold for the peak finding
        sigma_thresh_frac: Change in threshold fraction for the peak finding
        extract_pixel_from_filename: Extract `(row, col)` (in this format) from filename
        progress_bar: Show progress bar

    Returns:
        Dict of ODMR MeasurementDataclass with fit, fit model and pixels attributes set
    &#34;&#34;&#34;

    model1, base_params1 = rof.make_lorentzian_model()
    model2, base_params2 = rof.make_lorentziandouble_model()

    # Generate arguments for the parallel fitting
    args = []
    for odmr in tqdm(odmr_measurements.values(), disable=not progress_bar):
        x = odmr.data[&#34;Freq(MHz)&#34;].to_numpy()
        y = odmr.data[&#34;Counts&#34;].to_numpy()
        _, params1 = rof.estimate_lorentzian_dip(x, y, base_params1)
        _, params2 = rof.estimate_lorentziandouble_dip(x, y, base_params2, thresh_frac, min_thresh,
                                                       sigma_thresh_frac)
        args.append((x, y, model1, model2, params1, params2, r2_thresh))

    # Parallel fitting
    model_results = Parallel(n_jobs=cpu_count())(
        delayed(rof.lorentzian_fitting)(
            x, y, model1, model2, params1, params2, r2_thresh) for x, y, model1, model2, params1, params2, r2_thresh
        in
        tqdm(args, disable=not progress_bar)
    )

    x = list(odmr_measurements.values())[0].data[&#34;Freq(MHz)&#34;].to_numpy()
    x_fit = np.linspace(start=x[0], stop=x[-1], num=int(len(x) * 2))

    for odmr, res in zip(odmr_measurements.values(), model_results):

        if len(res.params) == 6:
            # Fit to a single Lorentzian
            y_fit = model1.eval(x=x_fit, params=res.params)
        else:
            # Fit to a double Lorentzian
            y_fit = model2.eval(x=x_fit, params=res.params)

        # Plug results into the DataClass
        odmr.fit_model = res
        odmr.fit_data = pd.DataFrame(np.vstack((x_fit, y_fit)).T, columns=[&#34;x_fit&#34;, &#34;y_fit&#34;])

        if extract_pixel_from_filename:
            row, col = map(int, re.findall(r&#39;(?&lt;=\().*?(?=\))&#39;, odmr.filename)[0].split(&#34;,&#34;))
            odmr.xy_position = (row, col)

    return odmr_measurements</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="qudi_hira_analysis.AnalysisLogic.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self, x: str | np.ndarray | pd.Series, y: str | np.ndarray | pd.Series, fit_function: <a title="qudi_hira_analysis.FitMethodsAndEstimators" href="#qudi_hira_analysis.FitMethodsAndEstimators">FitMethodsAndEstimators</a>, data: pd.DataFrame = None, parameters: list[Parameter] = None) ‑> Tuple[np.ndarray, np.ndarray, ModelResult]</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd>x data, can be string, numpy array or pandas Series</dd>
<dt><strong><code>y</code></strong></dt>
<dd>y data, can be string, numpy array or pandas Series</dd>
<dt><strong><code>fit_function</code></strong></dt>
<dd>fit function to use</dd>
<dt><strong><code>data</code></strong></dt>
<dd>pandas DataFrame containing x and y data, if None x and y must be numpy arrays or pandas Series</dd>
<dt><strong><code>parameters</code></strong></dt>
<dd>list of parameters to use in fit (optional)</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Fit x data, fit y data and lmfit ModelResult</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fit(
        self,
        x: str | np.ndarray | pd.Series,
        y: str | np.ndarray | pd.Series,
        fit_function: FitMethodsAndEstimators,
        data: pd.DataFrame = None,
        parameters: list[Parameter] = None
) -&gt; Tuple[np.ndarray, np.ndarray, ModelResult]:
    &#34;&#34;&#34;
    Args:
        x: x data, can be string, numpy array or pandas Series
        y: y data, can be string, numpy array or pandas Series
        fit_function: fit function to use
        data: pandas DataFrame containing x and y data, if None x and y must be numpy arrays or pandas Series
        parameters: list of parameters to use in fit (optional)

    Returns:
        Fit x data, fit y data and lmfit ModelResult
    &#34;&#34;&#34;
    if &#34;twoD&#34; in fit_function[0]:
        dims: str = &#34;2d&#34;
    else:
        dims: str = &#34;1d&#34;

    if data is None:
        if isinstance(x, pd.Series) or isinstance(x, pd.Index):
            x: np.ndarray = x.to_numpy()
        if isinstance(y, pd.Series):
            y: np.ndarray = y.to_numpy()
    elif isinstance(data, pd.DataFrame):
        x: np.ndarray = data[x].to_numpy()
        y: np.ndarray = data[y].to_numpy()
    else:
        raise TypeError(&#34;Data must be a pandas DataFrame or None&#34;)

    return self._perform_fit(
        x=x,
        y=y,
        fit_function=fit_function[0],
        estimator=fit_function[1],
        parameters=parameters,
        dims=dims
    )</code></pre>
</details>
</dd>
<dt id="qudi_hira_analysis.AnalysisLogic.get_all_fits"><code class="name flex">
<span>def <span class="ident">get_all_fits</span></span>(<span>self) ‑> Tuple[list, list]</span>
</code></dt>
<dd>
<div class="desc"><p>Get all available fits</p>
<h2 id="returns">Returns</h2>
<p>Tuple with list of 1d and 2d fits</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_all_fits(self) -&gt; Tuple[list, list]:
    &#34;&#34;&#34;Get all available fits

    Returns:
        Tuple with list of 1d and 2d fits
    &#34;&#34;&#34;
    one_d_fits: list = list(self.fit_list[&#39;1d&#39;].keys())
    two_d_fits: list = list(self.fit_list[&#39;2d&#39;].keys())
    self.log.info(f&#34;1d fits: {one_d_fits}\n2d fits: {two_d_fits}&#34;)
    return one_d_fits, two_d_fits</code></pre>
</details>
</dd>
<dt id="qudi_hira_analysis.AnalysisLogic.optimize_raster_odmr_params"><code class="name flex">
<span>def <span class="ident">optimize_raster_odmr_params</span></span>(<span>self, measurements: dict[str, MeasurementDataclass], num_samples: int = 10, num_params: int = 3) ‑> Tuple[float, Tuple[float, float, float]]</span>
</code></dt>
<dd>
<div class="desc"><p>This method optimizes the hyperparameters of the ODMR analysis.
It does so by randomly sampling a subset of the measurements and
then optimizing the hyperparameters for them.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>measurements</code></strong></dt>
<dd>A dictionary of measurements to optimize the hyperparameters for.</dd>
<dt><strong><code>num_params</code></strong></dt>
<dd>The number of parameters to optimize.</dd>
<dt><strong><code>num_samples</code></strong></dt>
<dd>The number of measurements to sample.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>The highest minimum R2 value and the optimized hyperparameters.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def optimize_raster_odmr_params(
        self,
        measurements: dict[str, MeasurementDataclass],
        num_samples: int = 10,
        num_params: int = 3,
) -&gt; Tuple[float, Tuple[float, float, float]]:
    &#34;&#34;&#34;
    This method optimizes the hyperparameters of the ODMR analysis.
    It does so by randomly sampling a subset of the measurements and
    then optimizing the hyperparameters for them.

    Args:
        measurements: A dictionary of measurements to optimize the hyperparameters for.
        num_params: The number of parameters to optimize.
        num_samples: The number of measurements to sample.

    Returns:
        The highest minimum R2 value and the optimized hyperparameters.
    &#34;&#34;&#34;
    r2_threshs: np.ndarray = np.around(np.linspace(start=0.9, stop=0.99, num=num_params), decimals=2)
    thresh_fracs: np.ndarray = np.around(np.linspace(start=0.5, stop=0.9, num=num_params), decimals=1)
    sigma_thresh_fracs: np.ndarray = np.around(np.linspace(start=0.1, stop=0.2, num=num_params), decimals=1)

    odmr_sample: dict = {}
    for k, v in random.sample(sorted(measurements.items()), k=num_samples):
        odmr_sample[k] = v

    highest_min_r2: float = 0
    optimal_params: Tuple[float, float, float] = (0, 0, 0)

    for idx, (r2_thresh, thresh_frac, sigma_thresh_frac) in enumerate(
            product(r2_threshs, thresh_fracs, sigma_thresh_fracs)):
        odmr_sample = self.fit_raster_odmr(
            odmr_sample,
            r2_thresh=r2_thresh,
            thresh_frac=thresh_frac,
            sigma_thresh_frac=sigma_thresh_frac,
            min_thresh=0.01,
            progress_bar=False
        )

        r2s: np.ndarray = np.zeros(len(odmr_sample))
        for _idx, odmr in enumerate(odmr_sample.values()):
            r2s[_idx] = odmr.fit_model.rsquared
        min_r2: float = np.min(r2s)

        if highest_min_r2 &lt; min_r2:
            highest_min_r2 = min_r2
            optimal_params = (r2_thresh, thresh_frac, sigma_thresh_frac)

    return highest_min_r2, optimal_params</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="qudi_hira_analysis.DataHandler"><code class="flex name class">
<span>class <span class="ident">DataHandler</span></span>
<span>(</span><span>data_folder: Path, figure_folder: Path, measurement_folder: Path = PosixPath('.'), copy_measurement_folder_structure: bool = True)</span>
</code></dt>
<dd>
<div class="desc"><p>Handles automated data searching and extraction into dataclasses.</p>
<h2 id="parameters">Parameters</h2>
<p>data_folder: Path to the data folder.
figure_folder: Path to the figure folder.
measurement_folder: Path to the measurement folder.</p>
<h2 id="examples">Examples</h2>
<p>Create an instance of the DataHandler class:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; dh = DataHandler(
&gt;&gt;&gt;     data_folder=Path('C:\'', 'Data'),
&gt;&gt;&gt;     figure_folder=Path('C:\'', 'QudiHiraAnalysis'),
&gt;&gt;&gt;     measurement_folder=Path('20230101_Bakeout'),
&gt;&gt;&gt; )
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DataHandler(DataLoader, AnalysisLogic):
    &#34;&#34;&#34;
    Handles automated data searching and extraction into dataclasses.

    Parameters:
        data_folder: Path to the data folder.
        figure_folder: Path to the figure folder.
        measurement_folder: Path to the measurement folder.

    Examples:
        Create an instance of the DataHandler class:

        &gt;&gt;&gt; dh = DataHandler(
        &gt;&gt;&gt;     data_folder=Path(&#39;C:\\&#39;&#39;, &#39;Data&#39;),
        &gt;&gt;&gt;     figure_folder=Path(&#39;C:\\&#39;&#39;, &#39;QudiHiraAnalysis&#39;),
        &gt;&gt;&gt;     measurement_folder=Path(&#39;20230101_Bakeout&#39;),
        &gt;&gt;&gt; )
    &#34;&#34;&#34;

    def __init__(
            self,
            data_folder: Path,
            figure_folder: Path,
            measurement_folder: Path = Path(),
            copy_measurement_folder_structure: bool = True,
    ):
        self.log = logging.getLogger(__name__)

        self.data_folder_path = self.__get_data_folder_path(data_folder, measurement_folder)

        if copy_measurement_folder_structure:
            self.figure_folder_path = self.__get_figure_folder_path(figure_folder, measurement_folder)
        else:
            self.figure_folder_path = self.__get_figure_folder_path(figure_folder, Path())

        super().__init__(base_read_path=self.data_folder_path, base_write_path=self.figure_folder_path)

        self.timestamp_format_str = &#34;%Y%m%d-%H%M-%S&#34;

    def __get_data_folder_path(self, data_folder: Path, folder_name: Path) -&gt; Path:
        &#34;&#34;&#34; Check if folder exists, if not, create it and return absolute folder paths. &#34;&#34;&#34;
        path = data_folder / folder_name

        if not path.exists():
            raise IOError(&#34;Data folder path does not exist.&#34;)

        self.log.info(f&#34;Data folder path is {path}&#34;)
        return path

    def __get_figure_folder_path(self, figure_folder: Path, folder_name: Path) -&gt; Path:
        &#34;&#34;&#34; Check if folder exists, if not, create it and return absolute folder paths. &#34;&#34;&#34;
        path = figure_folder / folder_name

        if not path.exists():
            path.mkdir()
            self.log.info(f&#34;Creating new output folder path {path}&#34;)
        else:
            self.log.info(f&#34;Figure folder path is {path}&#34;)
        return path

    def __tree(self, dir_path: Path, prefix: str = &#39;&#39;):
        &#34;&#34;&#34;
        A recursive generator, given a directory Path object
        will yield a visual tree structure line by line
        with each line prefixed by the same characters
        &#34;&#34;&#34;
        # prefix components:
        space = &#39;    &#39;
        branch = &#39;│   &#39;
        # pointers:
        tee = &#39;├── &#39;
        last = &#39;└── &#39;

        contents = list(dir_path.iterdir())
        # contents each get pointers that are ├── with a final └── :
        pointers = [tee] * (len(contents) - 1) + [last]
        for pointer, path in zip(pointers, contents):
            yield prefix + pointer + path.name
            if path.is_dir():  # extend the prefix and recurse:
                extension = branch if pointer == tee else space
                # i.e. space because last, └── , above so no more |
                yield from self.__tree(path, prefix=prefix + extension)

    def data_folder_tree(self):
        &#34;&#34;&#34; Print a tree of the data folder. &#34;&#34;&#34;
        for line in self.__tree(self.data_folder_path):
            print(line)

    def figure_folder_tree(self):
        &#34;&#34;&#34; Print a tree of the figure folder. &#34;&#34;&#34;
        for line in self.__tree(self.figure_folder_path):
            print(line)

    def _get_measurement_filepaths(
            self,
            measurement: str,
            extension: str,
            exclude_str: str | None = None
    ) -&gt; list[Path]:
        &#34;&#34;&#34;
        List all measurement files for a single measurement type, regardless of date
        within a similar set (i.e. top level folder).
        &#34;&#34;&#34;
        filepaths: List[Path] = []

        for path in self.data_folder_path.rglob(&#34;*&#34;):
            if path.is_file() and measurement.lower() in str(path).lower():
                if exclude_str is None or exclude_str not in str(path):
                    if extension:
                        if path.suffix == extension:
                            filepaths.append(path)
                    else:
                        filepaths.append(path)
        return filepaths

    def __load_qudi_measurements_into_dataclass(
            self,
            measurement_str: str,
            pulsed: bool,
            extension: str
    ) -&gt; dict[str: MeasurementDataclass]:

        if pulsed:
            filtered_filepaths = []
            timestamps = set()

            # Get set of unique timestamps containing pulsed_measurement_str
            for filepath in self._get_measurement_filepaths(measurement=measurement_str, extension=extension,
                                                            exclude_str=&#34;image_1.dat&#34;):
                filename = filepath.name
                if measurement_str in filename:
                    timestamps.add(filename[:16])
                    filtered_filepaths.append(filepath)

            pulsed_measurement_data: dict[str: MeasurementDataclass] = {}

            for ts in timestamps:
                pm, lp, rt = None, None, None

                for filepath in filtered_filepaths:
                    filename = filepath.name
                    if filename.startswith(ts):
                        if str(filename).endswith(&#34;laser_pulses.dat&#34;):
                            lp = LaserPulses(filepath=filepath, loaders=self.trace_qudi_loader)
                        elif str(filename).endswith(&#34;pulsed_measurement.dat&#34;):
                            pm = PulsedMeasurement(filepath=filepath, loaders=self.default_qudi_loader)
                        elif str(filename).endswith(&#34;raw_timetrace.dat&#34;):
                            rt = RawTimetrace(filepath=filepath, loaders=self.trace_qudi_loader)

                    if lp and pm and rt:
                        break

                pulsed_measurement_data[ts] = (
                    MeasurementDataclass(
                        timestamp=datetime.datetime.strptime(ts, self.timestamp_format_str),
                        pulsed=PulsedMeasurementDataclass(
                            measurement=pm,
                            laser_pulses=lp,
                            timetrace=rt
                        )
                    )
                )
            return pulsed_measurement_data
        else:
            if measurement_str.lower() == &#34;confocal&#34;:
                loaders = self.confocal_qudi_loader
                exclude_str = &#34;xy_data.dat&#34;
            elif measurement_str.lower() == &#34;pixelscanner&#34;:
                loaders = self.pixelscanner_qudi_loader
                exclude_str = None
            else:
                loaders = self.default_qudi_loader
                exclude_str = None

            measurement_data: dict[str: MeasurementDataclass] = {}

            for filepath in self._get_measurement_filepaths(measurement_str, extension, exclude_str):
                ts = filepath.name[:16]
                measurement_data[ts] = (
                    MeasurementDataclass(
                        filepath=filepath,
                        timestamp=datetime.datetime.strptime(ts, self.timestamp_format_str),
                        _loaders=loaders
                    )
                )
            return measurement_data

    def __load_standard_measurements_into_dataclass(
            self,
            measurement_str: str,
            extension: str
    ) -&gt; dict[str: MeasurementDataclass]:
        measurement_list: dict[str: MeasurementDataclass] = {}

        # Try and infer measurement type
        if measurement_str.lower() == &#34;temperature-monitoring&#34;:
            loaders = self.temperature_loader
            extension = &#34;.xls&#34;
            exclude_str = None
        elif measurement_str.lower() == &#34;pressure-monitoring&#34;:
            loaders = self.pressure_loader
            extension = &#34;.txt&#34;
            exclude_str = None
        elif measurement_str == &#34;frq-sweep&#34;:
            loaders = self.nanonis_loader
            exclude_str = None
        elif extension == &#34;.sxm&#34;:
            loaders = self.nanonis_spm_loader
            exclude_str = None
        elif extension == &#34;.pys&#34;:
            loaders = self.pys_loader
            exclude_str = None
        elif extension == &#34;.001&#34;:
            loaders = self.bruker_spm_loader
            exclude_str = None
        else:
            loaders = self.default_qudi_loader
            exclude_str = None

        for filepath in self._get_measurement_filepaths(measurement_str, extension, exclude_str):
            timestamp = datetime.datetime.fromtimestamp(filepath.stat().st_mtime)
            self.log.warning(&#34;Extracting timestamp from file modified time, may not be accurate.&#34;)
            ts = datetime.datetime.strftime(timestamp, self.timestamp_format_str)
            measurement_list[ts] = (
                MeasurementDataclass(
                    filepath=filepath,
                    timestamp=timestamp,
                    _loaders=loaders
                )
            )
        return measurement_list

    def load_measurements(
            self,
            measurement_str: str,
            qudi: bool = True,
            pulsed: bool = False,
            extension: str = &#34;.dat&#34;
    ) -&gt; dict[str: MeasurementDataclass]:
        &#34;&#34;&#34;
        Lazy load all measurements of a given type into a dictionary of dataclasses.

        Args:
            measurement_str: The name of the measurement type to load e.g. t1, t2, confocal etc. Recursively searches through the path defined by data_folder and measurement_folder
            qudi: Whether the measurement is a qudi measurement (default: False).
            pulsed: Whether the measurement is a pulsed measurement (default: False).
            extension: The file extension of the measurement files (default: .dat).

        Returns:
            dict: A dictionary of dataclasses containing the measurement data.

        Examples:
            `dh` is an instance of the `DataHandler` class.

            Load all T1 measurements:

            &gt;&gt;&gt; dh.load_measurements(measurement_str=&#34;ODMR&#34;, pulsed=True)

            Load all confocal data:

            &gt;&gt;&gt; dh.load_measurements(measurement_str=&#34;Confocal&#34;)

            Load all temperature monitoring data:

            &gt;&gt;&gt; dh.load_measurements(measurement_str=&#34;Temperature&#34;)

            Load all pressure monitoring data:

            &gt;&gt;&gt; dh.load_measurements(measurement_str=&#34;Pressure&#34;)
        &#34;&#34;&#34;

        measurement_str = measurement_str.lower()
        if qudi:
            return self.__load_qudi_measurements_into_dataclass(measurement_str, pulsed=pulsed, extension=&#34;.dat&#34;)
        else:
            return self.__load_standard_measurements_into_dataclass(measurement_str, extension=extension)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="qudi_hira_analysis.data_handler.DataLoader" href="data_handler.html#qudi_hira_analysis.data_handler.DataLoader">DataLoader</a></li>
<li><a title="qudi_hira_analysis.io_handler.IOHandler" href="io_handler.html#qudi_hira_analysis.io_handler.IOHandler">IOHandler</a></li>
<li><a title="qudi_hira_analysis.analysis_logic.AnalysisLogic" href="analysis_logic.html#qudi_hira_analysis.analysis_logic.AnalysisLogic">AnalysisLogic</a></li>
<li>qudi_hira_analysis._qudi_fit_logic.FitLogic</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="qudi_hira_analysis.DataHandler.data_folder_tree"><code class="name flex">
<span>def <span class="ident">data_folder_tree</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Print a tree of the data folder.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def data_folder_tree(self):
    &#34;&#34;&#34; Print a tree of the data folder. &#34;&#34;&#34;
    for line in self.__tree(self.data_folder_path):
        print(line)</code></pre>
</details>
</dd>
<dt id="qudi_hira_analysis.DataHandler.figure_folder_tree"><code class="name flex">
<span>def <span class="ident">figure_folder_tree</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Print a tree of the figure folder.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def figure_folder_tree(self):
    &#34;&#34;&#34; Print a tree of the figure folder. &#34;&#34;&#34;
    for line in self.__tree(self.figure_folder_path):
        print(line)</code></pre>
</details>
</dd>
<dt id="qudi_hira_analysis.DataHandler.load_measurements"><code class="name flex">
<span>def <span class="ident">load_measurements</span></span>(<span>self, measurement_str: str, qudi: bool = True, pulsed: bool = False, extension: str = '.dat') ‑> dict[slice(<class 'str'>, <class '<a title="qudi_hira_analysis.measurement_dataclass.MeasurementDataclass" href="measurement_dataclass.html#qudi_hira_analysis.measurement_dataclass.MeasurementDataclass">MeasurementDataclass</a>'>, None)]</span>
</code></dt>
<dd>
<div class="desc"><p>Lazy load all measurements of a given type into a dictionary of dataclasses.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>measurement_str</code></strong></dt>
<dd>The name of the measurement type to load e.g. t1, t2, confocal etc. Recursively searches through the path defined by data_folder and measurement_folder</dd>
<dt><strong><code>qudi</code></strong></dt>
<dd>Whether the measurement is a qudi measurement (default: False).</dd>
<dt><strong><code>pulsed</code></strong></dt>
<dd>Whether the measurement is a pulsed measurement (default: False).</dd>
<dt><strong><code>extension</code></strong></dt>
<dd>The file extension of the measurement files (default: .dat).</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>dict</code></dt>
<dd>A dictionary of dataclasses containing the measurement data.</dd>
</dl>
<h2 id="examples">Examples</h2>
<p><code>dh</code> is an instance of the <code><a title="qudi_hira_analysis.DataHandler" href="#qudi_hira_analysis.DataHandler">DataHandler</a></code> class.</p>
<p>Load all T1 measurements:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; dh.load_measurements(measurement_str=&quot;ODMR&quot;, pulsed=True)
</code></pre>
<p>Load all confocal data:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; dh.load_measurements(measurement_str=&quot;Confocal&quot;)
</code></pre>
<p>Load all temperature monitoring data:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; dh.load_measurements(measurement_str=&quot;Temperature&quot;)
</code></pre>
<p>Load all pressure monitoring data:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; dh.load_measurements(measurement_str=&quot;Pressure&quot;)
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_measurements(
        self,
        measurement_str: str,
        qudi: bool = True,
        pulsed: bool = False,
        extension: str = &#34;.dat&#34;
) -&gt; dict[str: MeasurementDataclass]:
    &#34;&#34;&#34;
    Lazy load all measurements of a given type into a dictionary of dataclasses.

    Args:
        measurement_str: The name of the measurement type to load e.g. t1, t2, confocal etc. Recursively searches through the path defined by data_folder and measurement_folder
        qudi: Whether the measurement is a qudi measurement (default: False).
        pulsed: Whether the measurement is a pulsed measurement (default: False).
        extension: The file extension of the measurement files (default: .dat).

    Returns:
        dict: A dictionary of dataclasses containing the measurement data.

    Examples:
        `dh` is an instance of the `DataHandler` class.

        Load all T1 measurements:

        &gt;&gt;&gt; dh.load_measurements(measurement_str=&#34;ODMR&#34;, pulsed=True)

        Load all confocal data:

        &gt;&gt;&gt; dh.load_measurements(measurement_str=&#34;Confocal&#34;)

        Load all temperature monitoring data:

        &gt;&gt;&gt; dh.load_measurements(measurement_str=&#34;Temperature&#34;)

        Load all pressure monitoring data:

        &gt;&gt;&gt; dh.load_measurements(measurement_str=&#34;Pressure&#34;)
    &#34;&#34;&#34;

    measurement_str = measurement_str.lower()
    if qudi:
        return self.__load_qudi_measurements_into_dataclass(measurement_str, pulsed=pulsed, extension=&#34;.dat&#34;)
    else:
        return self.__load_standard_measurements_into_dataclass(measurement_str, extension=extension)</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="qudi_hira_analysis.data_handler.DataLoader" href="data_handler.html#qudi_hira_analysis.data_handler.DataLoader">DataLoader</a></b></code>:
<ul class="hlist">
<li><code><a title="qudi_hira_analysis.data_handler.DataLoader.read_bruker_spm_data" href="io_handler.html#qudi_hira_analysis.io_handler.IOHandler.read_bruker_spm_data">read_bruker_spm_data</a></code></li>
<li><code><a title="qudi_hira_analysis.data_handler.DataLoader.read_confocal_into_dataframe" href="io_handler.html#qudi_hira_analysis.io_handler.IOHandler.read_confocal_into_dataframe">read_confocal_into_dataframe</a></code></li>
<li><code><a title="qudi_hira_analysis.data_handler.DataLoader.read_csv" href="io_handler.html#qudi_hira_analysis.io_handler.IOHandler.read_csv">read_csv</a></code></li>
<li><code><a title="qudi_hira_analysis.data_handler.DataLoader.read_excel" href="io_handler.html#qudi_hira_analysis.io_handler.IOHandler.read_excel">read_excel</a></code></li>
<li><code><a title="qudi_hira_analysis.data_handler.DataLoader.read_into_dataframe" href="io_handler.html#qudi_hira_analysis.io_handler.IOHandler.read_into_dataframe">read_into_dataframe</a></code></li>
<li><code><a title="qudi_hira_analysis.data_handler.DataLoader.read_into_ndarray" href="io_handler.html#qudi_hira_analysis.io_handler.IOHandler.read_into_ndarray">read_into_ndarray</a></code></li>
<li><code><a title="qudi_hira_analysis.data_handler.DataLoader.read_into_ndarray_transposed" href="io_handler.html#qudi_hira_analysis.io_handler.IOHandler.read_into_ndarray_transposed">read_into_ndarray_transposed</a></code></li>
<li><code><a title="qudi_hira_analysis.data_handler.DataLoader.read_lakeshore_data" href="io_handler.html#qudi_hira_analysis.io_handler.IOHandler.read_lakeshore_data">read_lakeshore_data</a></code></li>
<li><code><a title="qudi_hira_analysis.data_handler.DataLoader.read_nanonis_data" href="io_handler.html#qudi_hira_analysis.io_handler.IOHandler.read_nanonis_data">read_nanonis_data</a></code></li>
<li><code><a title="qudi_hira_analysis.data_handler.DataLoader.read_nanonis_parameters" href="io_handler.html#qudi_hira_analysis.io_handler.IOHandler.read_nanonis_parameters">read_nanonis_parameters</a></code></li>
<li><code><a title="qudi_hira_analysis.data_handler.DataLoader.read_nanonis_spm_data" href="io_handler.html#qudi_hira_analysis.io_handler.IOHandler.read_nanonis_spm_data">read_nanonis_spm_data</a></code></li>
<li><code><a title="qudi_hira_analysis.data_handler.DataLoader.read_oceanoptics_data" href="io_handler.html#qudi_hira_analysis.io_handler.IOHandler.read_oceanoptics_data">read_oceanoptics_data</a></code></li>
<li><code><a title="qudi_hira_analysis.data_handler.DataLoader.read_pfeiffer_data" href="io_handler.html#qudi_hira_analysis.io_handler.IOHandler.read_pfeiffer_data">read_pfeiffer_data</a></code></li>
<li><code><a title="qudi_hira_analysis.data_handler.DataLoader.read_pixelscanner_data" href="io_handler.html#qudi_hira_analysis.io_handler.IOHandler.read_pixelscanner_data">read_pixelscanner_data</a></code></li>
<li><code><a title="qudi_hira_analysis.data_handler.DataLoader.read_pkl" href="io_handler.html#qudi_hira_analysis.io_handler.IOHandler.read_pkl">read_pkl</a></code></li>
<li><code><a title="qudi_hira_analysis.data_handler.DataLoader.read_pys" href="io_handler.html#qudi_hira_analysis.io_handler.IOHandler.read_pys">read_pys</a></code></li>
<li><code><a title="qudi_hira_analysis.data_handler.DataLoader.read_qudi_parameters" href="io_handler.html#qudi_hira_analysis.io_handler.IOHandler.read_qudi_parameters">read_qudi_parameters</a></code></li>
<li><code><a title="qudi_hira_analysis.data_handler.DataLoader.save_df" href="io_handler.html#qudi_hira_analysis.io_handler.IOHandler.save_df">save_df</a></code></li>
<li><code><a title="qudi_hira_analysis.data_handler.DataLoader.save_figures" href="io_handler.html#qudi_hira_analysis.io_handler.IOHandler.save_figures">save_figures</a></code></li>
<li><code><a title="qudi_hira_analysis.data_handler.DataLoader.save_pkl" href="io_handler.html#qudi_hira_analysis.io_handler.IOHandler.save_pkl">save_pkl</a></code></li>
<li><code><a title="qudi_hira_analysis.data_handler.DataLoader.save_pys" href="io_handler.html#qudi_hira_analysis.io_handler.IOHandler.save_pys">save_pys</a></code></li>
</ul>
</li>
<li><code><b><a title="qudi_hira_analysis.analysis_logic.AnalysisLogic" href="analysis_logic.html#qudi_hira_analysis.analysis_logic.AnalysisLogic">AnalysisLogic</a></b></code>:
<ul class="hlist">
<li><code><a title="qudi_hira_analysis.analysis_logic.AnalysisLogic.analyse_mean" href="analysis_logic.html#qudi_hira_analysis.analysis_logic.AnalysisLogic.analyse_mean">analyse_mean</a></code></li>
<li><code><a title="qudi_hira_analysis.analysis_logic.AnalysisLogic.analyse_mean_norm" href="analysis_logic.html#qudi_hira_analysis.analysis_logic.AnalysisLogic.analyse_mean_norm">analyse_mean_norm</a></code></li>
<li><code><a title="qudi_hira_analysis.analysis_logic.AnalysisLogic.analyse_mean_reference" href="analysis_logic.html#qudi_hira_analysis.analysis_logic.AnalysisLogic.analyse_mean_reference">analyse_mean_reference</a></code></li>
<li><code><a title="qudi_hira_analysis.analysis_logic.AnalysisLogic.analyze_mean" href="analysis_logic.html#qudi_hira_analysis.analysis_logic.AnalysisLogic.analyze_mean">analyze_mean</a></code></li>
<li><code><a title="qudi_hira_analysis.analysis_logic.AnalysisLogic.analyze_mean_norm" href="analysis_logic.html#qudi_hira_analysis.analysis_logic.AnalysisLogic.analyze_mean_norm">analyze_mean_norm</a></code></li>
<li><code><a title="qudi_hira_analysis.analysis_logic.AnalysisLogic.analyze_mean_reference" href="analysis_logic.html#qudi_hira_analysis.analysis_logic.AnalysisLogic.analyze_mean_reference">analyze_mean_reference</a></code></li>
<li><code><a title="qudi_hira_analysis.analysis_logic.AnalysisLogic.average_raster_odmr_pixels" href="analysis_logic.html#qudi_hira_analysis.analysis_logic.AnalysisLogic.average_raster_odmr_pixels">average_raster_odmr_pixels</a></code></li>
<li><code><a title="qudi_hira_analysis.analysis_logic.AnalysisLogic.fit" href="analysis_logic.html#qudi_hira_analysis.analysis_logic.AnalysisLogic.fit">fit</a></code></li>
<li><code><a title="qudi_hira_analysis.analysis_logic.AnalysisLogic.fit_function" href="analysis_logic.html#qudi_hira_analysis.analysis_logic.AnalysisLogic.fit_function">fit_function</a></code></li>
<li><code><a title="qudi_hira_analysis.analysis_logic.AnalysisLogic.fit_raster_odmr" href="analysis_logic.html#qudi_hira_analysis.analysis_logic.AnalysisLogic.fit_raster_odmr">fit_raster_odmr</a></code></li>
<li><code><a title="qudi_hira_analysis.analysis_logic.AnalysisLogic.get_all_fits" href="analysis_logic.html#qudi_hira_analysis.analysis_logic.AnalysisLogic.get_all_fits">get_all_fits</a></code></li>
<li><code><a title="qudi_hira_analysis.analysis_logic.AnalysisLogic.optimize_raster_odmr_params" href="analysis_logic.html#qudi_hira_analysis.analysis_logic.AnalysisLogic.optimize_raster_odmr_params">optimize_raster_odmr_params</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="qudi_hira_analysis.FitMethodsAndEstimators"><code class="flex name class">
<span>class <span class="ident">FitMethodsAndEstimators</span></span>
</code></dt>
<dd>
<div class="desc"><p>Class for storing fit methods and estimators.
Fit methods are stored as tuples of (method, estimator)
where method is the name of the fit method and estimator is the name of the estimator.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class FitMethodsAndEstimators:
    &#34;&#34;&#34;
        Class for storing fit methods and estimators.
        Fit methods are stored as tuples of (method, estimator)
        where method is the name of the fit method and estimator is the name of the estimator.
    &#34;&#34;&#34;
    # Fit methods with corresponding estimators
    antibunching: tuple = (&#34;antibunching&#34;, &#34;dip&#34;)
    hyperbolicsaturation: tuple = (&#34;hyperbolicsaturation&#34;, &#34;generic&#34;)
    lorentzian: tuple = (&#34;lorentzian&#34;, &#34;dip&#34;)
    lorentziandouble: tuple = (&#34;lorentziandouble&#34;, &#34;dip&#34;)
    sineexponentialdecay: tuple = (&#34;sineexponentialdecay&#34;, &#34;generic&#34;)
    decayexponential: tuple = (&#34;decayexponential&#34;, &#34;generic&#34;)
    gaussian: tuple = (&#34;gaussian&#34;, &#34;dip&#34;)
    gaussiandouble: tuple = (&#34;gaussiandouble&#34;, &#34;dip&#34;)
    gaussianlinearoffset: tuple = (&#34;gaussianlinearoffset&#34;, &#34;dip&#34;)
    lorentziantriple: tuple = (&#34;lorentziantriple&#34;, &#34;dip&#34;)
    biexponential: tuple = (&#34;biexponential&#34;, &#34;generic&#34;)
    decayexponentialstretched: tuple = (&#34;decayexponentialstretched&#34;, &#34;generic&#34;)
    linear: tuple = (&#34;linear&#34;, &#34;generic&#34;)
    sine: tuple = (&#34;sine&#34;, &#34;generic&#34;)
    sinedouble: tuple = (&#34;sinedouble&#34;, &#34;generic&#34;)
    sinedoublewithexpdecay: tuple = (&#34;sinedoublewithexpdecay&#34;, &#34;generic&#34;)
    sinedoublewithtwoexpdecay: tuple = (&#34;sinedoublewithtwoexpdecay&#34;, &#34;generic&#34;)
    sinestretchedexponentialdecay: tuple = (&#34;sinestretchedexponentialdecay&#34;, &#34;generic&#34;)
    sinetriple: tuple = (&#34;sinetriple&#34;, &#34;generic&#34;)
    sinetriplewithexpdecay: tuple = (&#34;sinetriplewithexpdecay&#34;, &#34;generic&#34;)
    sinetriplewiththreeexpdecay: tuple = (&#34;sinetriplewiththreeexpdecay&#34;, &#34;generic&#34;)
    twoDgaussian: tuple = (&#34;twoDgaussian&#34;, &#34;generic&#34;)</code></pre>
</details>
<h3>Class variables</h3>
<dl>
<dt id="qudi_hira_analysis.FitMethodsAndEstimators.antibunching"><code class="name">var <span class="ident">antibunching</span> : tuple</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="qudi_hira_analysis.FitMethodsAndEstimators.biexponential"><code class="name">var <span class="ident">biexponential</span> : tuple</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="qudi_hira_analysis.FitMethodsAndEstimators.decayexponential"><code class="name">var <span class="ident">decayexponential</span> : tuple</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="qudi_hira_analysis.FitMethodsAndEstimators.decayexponentialstretched"><code class="name">var <span class="ident">decayexponentialstretched</span> : tuple</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="qudi_hira_analysis.FitMethodsAndEstimators.gaussian"><code class="name">var <span class="ident">gaussian</span> : tuple</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="qudi_hira_analysis.FitMethodsAndEstimators.gaussiandouble"><code class="name">var <span class="ident">gaussiandouble</span> : tuple</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="qudi_hira_analysis.FitMethodsAndEstimators.gaussianlinearoffset"><code class="name">var <span class="ident">gaussianlinearoffset</span> : tuple</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="qudi_hira_analysis.FitMethodsAndEstimators.hyperbolicsaturation"><code class="name">var <span class="ident">hyperbolicsaturation</span> : tuple</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="qudi_hira_analysis.FitMethodsAndEstimators.linear"><code class="name">var <span class="ident">linear</span> : tuple</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="qudi_hira_analysis.FitMethodsAndEstimators.lorentzian"><code class="name">var <span class="ident">lorentzian</span> : tuple</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="qudi_hira_analysis.FitMethodsAndEstimators.lorentziandouble"><code class="name">var <span class="ident">lorentziandouble</span> : tuple</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="qudi_hira_analysis.FitMethodsAndEstimators.lorentziantriple"><code class="name">var <span class="ident">lorentziantriple</span> : tuple</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="qudi_hira_analysis.FitMethodsAndEstimators.sine"><code class="name">var <span class="ident">sine</span> : tuple</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="qudi_hira_analysis.FitMethodsAndEstimators.sinedouble"><code class="name">var <span class="ident">sinedouble</span> : tuple</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="qudi_hira_analysis.FitMethodsAndEstimators.sinedoublewithexpdecay"><code class="name">var <span class="ident">sinedoublewithexpdecay</span> : tuple</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="qudi_hira_analysis.FitMethodsAndEstimators.sinedoublewithtwoexpdecay"><code class="name">var <span class="ident">sinedoublewithtwoexpdecay</span> : tuple</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="qudi_hira_analysis.FitMethodsAndEstimators.sineexponentialdecay"><code class="name">var <span class="ident">sineexponentialdecay</span> : tuple</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="qudi_hira_analysis.FitMethodsAndEstimators.sinestretchedexponentialdecay"><code class="name">var <span class="ident">sinestretchedexponentialdecay</span> : tuple</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="qudi_hira_analysis.FitMethodsAndEstimators.sinetriple"><code class="name">var <span class="ident">sinetriple</span> : tuple</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="qudi_hira_analysis.FitMethodsAndEstimators.sinetriplewithexpdecay"><code class="name">var <span class="ident">sinetriplewithexpdecay</span> : tuple</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="qudi_hira_analysis.FitMethodsAndEstimators.sinetriplewiththreeexpdecay"><code class="name">var <span class="ident">sinetriplewiththreeexpdecay</span> : tuple</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="qudi_hira_analysis.FitMethodsAndEstimators.twoDgaussian"><code class="name">var <span class="ident">twoDgaussian</span> : tuple</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="qudi_hira_analysis.IOHandler"><code class="flex name class">
<span>class <span class="ident">IOHandler</span></span>
<span>(</span><span>base_read_path: pathlib.Path = None, base_write_path: pathlib.Path = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Handle all read and write operations.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class IOHandler:
    &#34;&#34;&#34; Handle all read and write operations. &#34;&#34;&#34;

    def __init__(self, base_read_path: Path = None, base_write_path: Path = None):
        super().__init__()
        self.base_read_path = base_read_path
        self.base_write_path = base_write_path

    @staticmethod
    def _add_base_read_path(func: Callable) -&gt; Callable:
        &#34;&#34;&#34;
        Decorator to add the `base_read_path` to the filepath if it is not None

        Args:
            func: Function to be decorated

        Returns:
            Decorated function
        &#34;&#34;&#34;

        @wraps(func)
        def wrapper(self, filepath: Path, **kwargs):
            if self.base_read_path:
                filepath = self.base_read_path / filepath
            return func(self, filepath, **kwargs)

        return wrapper

    @staticmethod
    def _add_base_write_path(func: Callable) -&gt; Callable:
        &#34;&#34;&#34;
        Decorator to add the `base_write_path` to the filepath if it is not None

        Args:
            func: Function to be decorated

        Returns:
            Decorated function
        &#34;&#34;&#34;

        @wraps(func)
        def wrapper(self, filepath: Path, **kwargs):
            if self.base_write_path:
                filepath = self.base_write_path / filepath
            filepath.parent.mkdir(exist_ok=True)
            return func(self, filepath, **kwargs)

        return wrapper

    @staticmethod
    def _check_extension(ext: str) -&gt; Callable:
        &#34;&#34;&#34;
        Decorator to check the extension of the filepath is correct

        Args:
            ext: Extension to check for

        Returns:
            Decorated function
        &#34;&#34;&#34;

        def decorator(func: Callable) -&gt; Callable:
            @wraps(func)
            def wrapper(self, filepath: Path, **kwargs) -&gt; Callable:
                if filepath.suffix == ext:
                    return func(self, filepath, **kwargs)
                elif filepath.suffix == &#34;&#34;:
                    return func(self, filepath.with_suffix(ext), **kwargs)
                else:
                    raise IOError(f&#34;Invalid extension &#39;{filepath.suffix}&#39; in &#39;{filepath}&#39;, extension should be &#39;{ext}&#39;&#34;)

            return wrapper

        return decorator

    @_add_base_read_path
    @_check_extension(&#34;.dat&#34;)
    def read_qudi_parameters(self, filepath: Path) -&gt; dict:
        &#34;&#34;&#34;Extract parameters from a qudi dat file.

        Args:
            filepath: Path to the qudi .dat file

        Returns:
            Dictionary of parameters
        &#34;&#34;&#34;
        params = {}
        with open(filepath) as file:
            for line in file:
                if line == &#39;#=====\n&#39;:
                    break
                else:
                    # noinspection PyBroadException
                    try:
                        # Remove # from beginning of lines
                        line = line[1:]
                        if line.count(&#34;:&#34;) == 1:
                            # Add params to dictionary
                            label, value = line.split(&#34;:&#34;)
                            if value != &#34;\n&#34;:
                                params[label] = ast.literal_eval(inspect.cleandoc(value))
                        elif line.count(&#34;:&#34;) == 3:
                            # Handle files with timestamps in them
                            label = line.split(&#34;:&#34;)[0]
                            timestamp_str = &#34;&#34;.join(line.split(&#34;:&#34;)[1:]).strip()
                            datetime_str = datetime.datetime.strptime(timestamp_str, &#34;%d.%m.%Y %Hh%Mmin%Ss&#34;).replace(
                                tzinfo=datetime.timezone.utc)
                            params[label] = datetime_str
                    except Exception as _:
                        pass
        return params

    @_add_base_read_path
    @_check_extension(&#34;.dat&#34;)
    def read_into_dataframe(self, filepath: Path) -&gt; pd.DataFrame:
        &#34;&#34;&#34;Read a qudi data file into a pandas DataFrame for analysis.

        Args:
            filepath: Path to the qudi data file

        Returns:
            DataFrame containing the data from the qudi data file
        &#34;&#34;&#34;
        with open(filepath) as handle:
            # Generate column names for DataFrame by parsing the file
            *_comments, names = itertools.takewhile(lambda line: line.startswith(&#39;#&#39;), handle)
            names = names[1:].strip().split(&#34;\t&#34;)
        return pd.read_csv(filepath, names=names, comment=&#34;#&#34;, sep=&#34;\t&#34;)

    @_add_base_read_path
    def read_csv(self, filepath: Path, **kwargs) -&gt; pd.DataFrame:
        &#34;&#34;&#34; Read a csv file into a pandas DataFrame. &#34;&#34;&#34;
        return pd.read_csv(filepath, **kwargs)

    @_add_base_read_path
    def read_excel(self, filepath: Path, **kwargs) -&gt; pd.DataFrame:
        &#34;&#34;&#34; Read a csv file into a pandas DataFrame. &#34;&#34;&#34;
        return pd.read_excel(filepath, **kwargs)

    @_add_base_read_path
    @_check_extension(&#34;.dat&#34;)
    def read_confocal_into_dataframe(self, filepath: Path) -&gt; pd.DataFrame:
        &#34;&#34;&#34; Read a qudi confocal data file into a pandas DataFrame for analysis. &#34;&#34;&#34;
        confocal_params = self.read_qudi_parameters(filepath)
        data = self.read_into_ndarray(filepath, delimiter=&#34;\t&#34;)
        # Use the confocal parameters to generate the index and columns for the DataFrame
        index = np.linspace(
            confocal_params[&#39;X image min (m)&#39;],
            confocal_params[&#39;X image max (m)&#39;],
            data.shape[0]
        )
        columns = np.linspace(
            confocal_params[&#39;Y image min&#39;],
            confocal_params[&#39;Y image max&#39;],
            data.shape[1]
        )
        df = pd.DataFrame(data, index=index, columns=columns)
        # Sort the index to get origin (0, 0) in the lower left corner of the DataFrame
        df.sort_index(axis=0, ascending=False, inplace=True)
        return df

    @_add_base_read_path
    def read_into_ndarray(self, filepath: Path, **kwargs) -&gt; np.ndarray:
        &#34;&#34;&#34; Read a file into a numpy ndarray. &#34;&#34;&#34;
        return np.genfromtxt(filepath, **kwargs)

    @_add_base_read_path
    def read_into_ndarray_transposed(self, filepath: Path, **kwargs) -&gt; np.ndarray:
        &#34;&#34;&#34; Read a file into a transposed numpy ndarray. &#34;&#34;&#34;
        return np.genfromtxt(filepath, **kwargs).T

    @_add_base_read_path
    @_check_extension(&#34;.pys&#34;)
    def read_pys(self, filepath: Path) -&gt; dict:
        &#34;&#34;&#34; Read raw .pys data files into a dictionary. &#34;&#34;&#34;
        byte_dict = np.load(str(filepath), encoding=&#34;bytes&#34;, allow_pickle=True)
        # Convert byte string keys to normal strings
        return {key.decode(&#39;utf8&#39;): byte_dict.get(key) for key in byte_dict.keys()}

    @_add_base_read_path
    @_check_extension(&#34;.pkl&#34;)
    def read_pkl(self, filepath: Path) -&gt; dict:
        &#34;&#34;&#34; Read pickle files into a dictionary. &#34;&#34;&#34;
        with open(filepath, &#39;rb&#39;) as f:
            file = pickle.load(f)
        return file

    @_add_base_read_path
    @_check_extension(&#34;.dat&#34;)
    def read_nanonis_data(self, filepath: Path) -&gt; pd.DataFrame:
        &#34;&#34;&#34;Read data from a Nanonis .dat file.

        Args:
            filepath: Path to the Nanonis .dat file.

        Returns:
            DataFrame of data.
        &#34;&#34;&#34;
        skip_rows = 0
        with open(filepath) as dat_file:
            for num, line in enumerate(dat_file, 1):
                if &#34;[DATA]&#34; in line:
                    # Find number of rows to skip when extracting data
                    skip_rows = num
                    break
                if &#34;#=====&#34; in line:
                    skip_rows = num
                    break
        df = pd.read_table(filepath, sep=&#34;\t&#34;, skiprows=skip_rows)
        return df

    @_add_base_read_path
    @_check_extension(&#34;.dat&#34;)
    def read_nanonis_parameters(self, filepath: Path) -&gt; dict:
        &#34;&#34;&#34;Read parameters from a Nanonis .dat file.

        Args:
            filepath: Path to the Nanonis .dat file.

        Returns:
            Dictionary of parameters.
        &#34;&#34;&#34;
        parameters = {}
        with open(filepath) as dat_file:
            for line in dat_file:
                if line == &#34;\n&#34;:
                    # Break when reaching empty line
                    break
                elif &#34;User&#34; in line or line.split(&#34;\t&#34;)[0] == &#34;&#34;:
                    # Cleanup excess parameters and skip empty lines
                    pass
                else:
                    label, value, _ = line.split(&#34;\t&#34;)
                    try:
                        # Convert strings to number where possible
                        value = float(value)
                    except ValueError:
                        pass
                    if &#34;Oscillation Control&gt;&#34; in label:
                        label = label.replace(&#34;Oscillation Control&gt;&#34;, &#34;&#34;)
                    parameters[label] = value
        return parameters

    @_add_base_read_path
    @_check_extension(&#34;.sxm&#34;)
    def read_nanonis_spm_data(self, filepath: Path) -&gt; pySPM.SXM:
        &#34;&#34;&#34;Read a Nanonis .sxm data file.

        Args:
            filepath: Path to the .sxm file.

        Returns:
            pySPM.SXM object containing the data.
        &#34;&#34;&#34;
        return pySPM.SXM(filepath)

    @_add_base_read_path
    @_check_extension(&#34;.001&#34;)
    def read_bruker_spm_data(self, filepath: Path) -&gt; pySPM.Bruker:
        &#34;&#34;&#34;Read a Bruker SPM data file.

        Args:
            filepath: Path to the .001 file.

        Returns:
            pySPM.Bruker object containing the data.
        &#34;&#34;&#34;
        return pySPM.Bruker(filepath)

    @_add_base_read_path
    @_check_extension(&#34;.txt&#34;)
    def read_pfeiffer_data(self, filepath: Path) -&gt; pd.DataFrame:
        &#34;&#34;&#34;Read data stored by Pfeiffer vacuum monitoring software.

        Args:
            filepath: Path to the text file.

        Returns:
            DataFrame containing the data.
        &#34;&#34;&#34;
        # Extract rows including the header
        df = pd.read_csv(filepath, sep=&#34;\t&#34;, skiprows=[0, 2, 3, 4])
        # Combine data and time columns together
        df[&#34;Date&#34;] = df[&#34;Date&#34;] + &#34; &#34; + df[&#34;Time&#34;]
        df = df.drop(&#34;Time&#34;, axis=1)
        # Infer datetime format and convert to datetime objects
        df[&#34;Date&#34;] = pd.to_datetime(df[&#34;Date&#34;], infer_datetime_format=True)
        # Set datetime as index
        df = df.set_index(&#34;Date&#34;, drop=True)
        return df

    @_add_base_read_path
    @_check_extension(&#34;.xls&#34;)
    def read_lakeshore_data(self, filepath: Path) -&gt; pd.DataFrame:
        &#34;&#34;&#34;Read data stored by Lakeshore temperature monitor software.

        Args:
            filepath: Path to the Excel file.

        Returns:
            DataFrame containing the data.
        &#34;&#34;&#34;
        # Extract only the origin timestamp
        origin = pd.read_excel(filepath, skiprows=1, nrows=1, usecols=[1], header=None)[1][0]
        # Remove any tzinfo to prevent future exceptions in pandas
        origin = origin.replace(&#34;CET&#34;, &#34;&#34;)
        # Parse datetime object from timestamp
        origin = pd.to_datetime(origin)
        # Create DataFrame and drop empty cols
        df = pd.read_excel(filepath, skiprows=3)
        df = df.dropna(axis=1, how=&#34;all&#34;)
        # Add datetimes to DataFrame
        df[&#34;Datetime&#34;] = pd.to_datetime(df[&#34;Time&#34;], unit=&#34;ms&#34;, origin=origin)
        df = df.drop(&#34;Time&#34;, axis=1)
        # Set datetime as index
        df = df.set_index(&#34;Datetime&#34;, drop=True)
        return df

    @_add_base_read_path
    @_check_extension(&#34;.txt&#34;)
    def read_oceanoptics_data(self, filepath: str) -&gt; pd.DataFrame:
        &#34;&#34;&#34;Read spectrometer data from OceanOptics spectrometer.

        Args:
            filepath: Path to the data file.

        Returns:
            DataFrame containing the wavelength and intensity data.
        &#34;&#34;&#34;
        df = pd.read_csv(filepath, sep=&#34;\t&#34;, skiprows=14, names=[&#34;wavelength&#34;, &#34;intensity&#34;])
        return df

    @staticmethod
    def __get_forward_backward_counts(count_rates, num_pixels):
        split_array = np.split(count_rates, 2 * num_pixels)
        # Extract forward scan array as every second element
        forward_counts = np.stack(split_array[::2])
        # Extract backward scan array as every shifted second element
        # Flip scan so that backward and forward scans represent the same data
        backward_counts = np.flip(np.stack(split_array[1::2]), axis=1)
        return forward_counts, backward_counts

    def read_pixelscanner_data(self, filepath: Path) -&gt; (pySPM.SPM_image, pySPM.SPM_image):
        &#34;&#34;&#34; Read data from a PixelScanner measurement.

        Args:
            filepath: Path to the data file.

        Returns:
            Forward and backward scan data.
        &#34;&#34;&#34;
        df = self.read_into_dataframe(filepath)
        num_pixels = int(np.sqrt(len(df) // 2))

        if num_pixels ** 2 != len(df) // 2:
            raise ValueError(&#34;Number of pixels does not match data length.&#34;)

        try:
            fwd, bwd = self.__get_forward_backward_counts(df[&#34;count_rates&#34;], num_pixels)
        except KeyError:
            try:
                fwd, bwd = self.__get_forward_backward_counts(df[&#34;Count Rates (cps)&#34;], num_pixels)
            except KeyError:
                # Support old data format
                fwd = df[&#34;forward (cps)&#34;].to_numpy().reshape(num_pixels, num_pixels)
                bwd = df[&#34;backward (cps)&#34;].to_numpy().reshape(num_pixels, num_pixels)

        fwd = pySPM.SPM_image(fwd, channel=&#34;Forward&#34;, _type=&#34;NV-PL&#34;)
        bwd = pySPM.SPM_image(bwd, channel=&#34;Backward&#34;, _type=&#34;NV-PL&#34;)
        return fwd, bwd

    @_add_base_write_path
    @_check_extension(&#34;.pkl&#34;)
    def save_pkl(self, filepath: Path, obj: object):
        &#34;&#34;&#34;Saves pickle files.

        Args:
            filepath: Path to the data file.
            obj: Object to be saved.
        &#34;&#34;&#34;
        with open(filepath, &#39;wb&#39;) as f:
            pickle.dump(obj, f)

    @_add_base_write_path
    @_check_extension(&#34;.pys&#34;)
    def save_pys(self, filepath: Path, dictionary: dict):
        &#34;&#34;&#34;Saves .pys files.

        Args:
            filepath: Path to the data file.
            dictionary: Dictionary to be saved.
        &#34;&#34;&#34;
        with open(filepath, &#39;wb&#39;) as f:
            pickle.dump(dictionary, f, 1)

    @_add_base_write_path
    @_check_extension(&#34;.pys&#34;)
    def save_df(self, filepath: Path, df: pd.DataFrame):
        &#34;&#34;&#34; Save Dataframe as csv. &#34;&#34;&#34;
        df.to_csv(filepath, sep=&#39;\t&#39;, encoding=&#39;utf-8&#39;)

    @_add_base_write_path
    def save_figures(self, filepath: Path, fig: plt.Figure, **kwargs):
        &#34;&#34;&#34;Saves figures from matplotlib plot data.

        By default, saves as jpg, png, pdf and svg.

        Args:
            fig: Matplotlib figure to save.
            filepath: Name of figure to save.
            only_jpg: If True, only save as jpg (default: False).
            only_pdf: If True, only save as pdf (default: False).
            **kwargs: Keyword arguments passed to matplotlib.pyplot.savefig().
        &#34;&#34;&#34;
        extensions = None
        if &#34;only_jpg&#34; in kwargs:
            if kwargs.get(&#34;only_jpg&#34;):
                extensions = [&#34;.jpg&#34;]
            kwargs.pop(&#34;only_jpg&#34;, None)
        elif &#34;only_pdf&#34; in kwargs:
            if kwargs.get(&#34;only_pdf&#34;):
                extensions = [&#34;.pdf&#34;]
            kwargs.pop(&#34;only_pdf&#34;, None)
        else:
            extensions = [&#34;.jpg&#34;, &#34;.pdf&#34;, &#34;.svg&#34;, &#34;.png&#34;]

        for ext in extensions:
            fig.savefig(filepath.with_suffix(ext), dpi=200, **kwargs)</code></pre>
</details>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="qudi_hira_analysis.data_handler.DataLoader" href="data_handler.html#qudi_hira_analysis.data_handler.DataLoader">DataLoader</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="qudi_hira_analysis.IOHandler.read_bruker_spm_data"><code class="name flex">
<span>def <span class="ident">read_bruker_spm_data</span></span>(<span>self, filepath: pathlib.Path) ‑> pySPM.Bruker.Bruker</span>
</code></dt>
<dd>
<div class="desc"><p>Read a Bruker SPM data file.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>filepath</code></strong></dt>
<dd>Path to the .001 file.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>pySPM.Bruker object containing the data.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@_add_base_read_path
@_check_extension(&#34;.001&#34;)
def read_bruker_spm_data(self, filepath: Path) -&gt; pySPM.Bruker:
    &#34;&#34;&#34;Read a Bruker SPM data file.

    Args:
        filepath: Path to the .001 file.

    Returns:
        pySPM.Bruker object containing the data.
    &#34;&#34;&#34;
    return pySPM.Bruker(filepath)</code></pre>
</details>
</dd>
<dt id="qudi_hira_analysis.IOHandler.read_confocal_into_dataframe"><code class="name flex">
<span>def <span class="ident">read_confocal_into_dataframe</span></span>(<span>self, filepath: pathlib.Path) ‑> pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Read a qudi confocal data file into a pandas DataFrame for analysis.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@_add_base_read_path
@_check_extension(&#34;.dat&#34;)
def read_confocal_into_dataframe(self, filepath: Path) -&gt; pd.DataFrame:
    &#34;&#34;&#34; Read a qudi confocal data file into a pandas DataFrame for analysis. &#34;&#34;&#34;
    confocal_params = self.read_qudi_parameters(filepath)
    data = self.read_into_ndarray(filepath, delimiter=&#34;\t&#34;)
    # Use the confocal parameters to generate the index and columns for the DataFrame
    index = np.linspace(
        confocal_params[&#39;X image min (m)&#39;],
        confocal_params[&#39;X image max (m)&#39;],
        data.shape[0]
    )
    columns = np.linspace(
        confocal_params[&#39;Y image min&#39;],
        confocal_params[&#39;Y image max&#39;],
        data.shape[1]
    )
    df = pd.DataFrame(data, index=index, columns=columns)
    # Sort the index to get origin (0, 0) in the lower left corner of the DataFrame
    df.sort_index(axis=0, ascending=False, inplace=True)
    return df</code></pre>
</details>
</dd>
<dt id="qudi_hira_analysis.IOHandler.read_csv"><code class="name flex">
<span>def <span class="ident">read_csv</span></span>(<span>self, filepath: pathlib.Path, **kwargs) ‑> pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Read a csv file into a pandas DataFrame.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@_add_base_read_path
def read_csv(self, filepath: Path, **kwargs) -&gt; pd.DataFrame:
    &#34;&#34;&#34; Read a csv file into a pandas DataFrame. &#34;&#34;&#34;
    return pd.read_csv(filepath, **kwargs)</code></pre>
</details>
</dd>
<dt id="qudi_hira_analysis.IOHandler.read_excel"><code class="name flex">
<span>def <span class="ident">read_excel</span></span>(<span>self, filepath: pathlib.Path, **kwargs) ‑> pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Read a csv file into a pandas DataFrame.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@_add_base_read_path
def read_excel(self, filepath: Path, **kwargs) -&gt; pd.DataFrame:
    &#34;&#34;&#34; Read a csv file into a pandas DataFrame. &#34;&#34;&#34;
    return pd.read_excel(filepath, **kwargs)</code></pre>
</details>
</dd>
<dt id="qudi_hira_analysis.IOHandler.read_into_dataframe"><code class="name flex">
<span>def <span class="ident">read_into_dataframe</span></span>(<span>self, filepath: pathlib.Path) ‑> pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Read a qudi data file into a pandas DataFrame for analysis.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>filepath</code></strong></dt>
<dd>Path to the qudi data file</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>DataFrame containing the data from the qudi data file</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@_add_base_read_path
@_check_extension(&#34;.dat&#34;)
def read_into_dataframe(self, filepath: Path) -&gt; pd.DataFrame:
    &#34;&#34;&#34;Read a qudi data file into a pandas DataFrame for analysis.

    Args:
        filepath: Path to the qudi data file

    Returns:
        DataFrame containing the data from the qudi data file
    &#34;&#34;&#34;
    with open(filepath) as handle:
        # Generate column names for DataFrame by parsing the file
        *_comments, names = itertools.takewhile(lambda line: line.startswith(&#39;#&#39;), handle)
        names = names[1:].strip().split(&#34;\t&#34;)
    return pd.read_csv(filepath, names=names, comment=&#34;#&#34;, sep=&#34;\t&#34;)</code></pre>
</details>
</dd>
<dt id="qudi_hira_analysis.IOHandler.read_into_ndarray"><code class="name flex">
<span>def <span class="ident">read_into_ndarray</span></span>(<span>self, filepath: pathlib.Path, **kwargs) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Read a file into a numpy ndarray.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@_add_base_read_path
def read_into_ndarray(self, filepath: Path, **kwargs) -&gt; np.ndarray:
    &#34;&#34;&#34; Read a file into a numpy ndarray. &#34;&#34;&#34;
    return np.genfromtxt(filepath, **kwargs)</code></pre>
</details>
</dd>
<dt id="qudi_hira_analysis.IOHandler.read_into_ndarray_transposed"><code class="name flex">
<span>def <span class="ident">read_into_ndarray_transposed</span></span>(<span>self, filepath: pathlib.Path, **kwargs) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Read a file into a transposed numpy ndarray.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@_add_base_read_path
def read_into_ndarray_transposed(self, filepath: Path, **kwargs) -&gt; np.ndarray:
    &#34;&#34;&#34; Read a file into a transposed numpy ndarray. &#34;&#34;&#34;
    return np.genfromtxt(filepath, **kwargs).T</code></pre>
</details>
</dd>
<dt id="qudi_hira_analysis.IOHandler.read_lakeshore_data"><code class="name flex">
<span>def <span class="ident">read_lakeshore_data</span></span>(<span>self, filepath: pathlib.Path) ‑> pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Read data stored by Lakeshore temperature monitor software.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>filepath</code></strong></dt>
<dd>Path to the Excel file.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>DataFrame containing the data.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@_add_base_read_path
@_check_extension(&#34;.xls&#34;)
def read_lakeshore_data(self, filepath: Path) -&gt; pd.DataFrame:
    &#34;&#34;&#34;Read data stored by Lakeshore temperature monitor software.

    Args:
        filepath: Path to the Excel file.

    Returns:
        DataFrame containing the data.
    &#34;&#34;&#34;
    # Extract only the origin timestamp
    origin = pd.read_excel(filepath, skiprows=1, nrows=1, usecols=[1], header=None)[1][0]
    # Remove any tzinfo to prevent future exceptions in pandas
    origin = origin.replace(&#34;CET&#34;, &#34;&#34;)
    # Parse datetime object from timestamp
    origin = pd.to_datetime(origin)
    # Create DataFrame and drop empty cols
    df = pd.read_excel(filepath, skiprows=3)
    df = df.dropna(axis=1, how=&#34;all&#34;)
    # Add datetimes to DataFrame
    df[&#34;Datetime&#34;] = pd.to_datetime(df[&#34;Time&#34;], unit=&#34;ms&#34;, origin=origin)
    df = df.drop(&#34;Time&#34;, axis=1)
    # Set datetime as index
    df = df.set_index(&#34;Datetime&#34;, drop=True)
    return df</code></pre>
</details>
</dd>
<dt id="qudi_hira_analysis.IOHandler.read_nanonis_data"><code class="name flex">
<span>def <span class="ident">read_nanonis_data</span></span>(<span>self, filepath: pathlib.Path) ‑> pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Read data from a Nanonis .dat file.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>filepath</code></strong></dt>
<dd>Path to the Nanonis .dat file.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>DataFrame of data.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@_add_base_read_path
@_check_extension(&#34;.dat&#34;)
def read_nanonis_data(self, filepath: Path) -&gt; pd.DataFrame:
    &#34;&#34;&#34;Read data from a Nanonis .dat file.

    Args:
        filepath: Path to the Nanonis .dat file.

    Returns:
        DataFrame of data.
    &#34;&#34;&#34;
    skip_rows = 0
    with open(filepath) as dat_file:
        for num, line in enumerate(dat_file, 1):
            if &#34;[DATA]&#34; in line:
                # Find number of rows to skip when extracting data
                skip_rows = num
                break
            if &#34;#=====&#34; in line:
                skip_rows = num
                break
    df = pd.read_table(filepath, sep=&#34;\t&#34;, skiprows=skip_rows)
    return df</code></pre>
</details>
</dd>
<dt id="qudi_hira_analysis.IOHandler.read_nanonis_parameters"><code class="name flex">
<span>def <span class="ident">read_nanonis_parameters</span></span>(<span>self, filepath: pathlib.Path) ‑> dict</span>
</code></dt>
<dd>
<div class="desc"><p>Read parameters from a Nanonis .dat file.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>filepath</code></strong></dt>
<dd>Path to the Nanonis .dat file.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Dictionary of parameters.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@_add_base_read_path
@_check_extension(&#34;.dat&#34;)
def read_nanonis_parameters(self, filepath: Path) -&gt; dict:
    &#34;&#34;&#34;Read parameters from a Nanonis .dat file.

    Args:
        filepath: Path to the Nanonis .dat file.

    Returns:
        Dictionary of parameters.
    &#34;&#34;&#34;
    parameters = {}
    with open(filepath) as dat_file:
        for line in dat_file:
            if line == &#34;\n&#34;:
                # Break when reaching empty line
                break
            elif &#34;User&#34; in line or line.split(&#34;\t&#34;)[0] == &#34;&#34;:
                # Cleanup excess parameters and skip empty lines
                pass
            else:
                label, value, _ = line.split(&#34;\t&#34;)
                try:
                    # Convert strings to number where possible
                    value = float(value)
                except ValueError:
                    pass
                if &#34;Oscillation Control&gt;&#34; in label:
                    label = label.replace(&#34;Oscillation Control&gt;&#34;, &#34;&#34;)
                parameters[label] = value
    return parameters</code></pre>
</details>
</dd>
<dt id="qudi_hira_analysis.IOHandler.read_nanonis_spm_data"><code class="name flex">
<span>def <span class="ident">read_nanonis_spm_data</span></span>(<span>self, filepath: pathlib.Path) ‑> pySPM.SXM.SXM</span>
</code></dt>
<dd>
<div class="desc"><p>Read a Nanonis .sxm data file.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>filepath</code></strong></dt>
<dd>Path to the .sxm file.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>pySPM.SXM object containing the data.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@_add_base_read_path
@_check_extension(&#34;.sxm&#34;)
def read_nanonis_spm_data(self, filepath: Path) -&gt; pySPM.SXM:
    &#34;&#34;&#34;Read a Nanonis .sxm data file.

    Args:
        filepath: Path to the .sxm file.

    Returns:
        pySPM.SXM object containing the data.
    &#34;&#34;&#34;
    return pySPM.SXM(filepath)</code></pre>
</details>
</dd>
<dt id="qudi_hira_analysis.IOHandler.read_oceanoptics_data"><code class="name flex">
<span>def <span class="ident">read_oceanoptics_data</span></span>(<span>self, filepath: str) ‑> pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Read spectrometer data from OceanOptics spectrometer.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>filepath</code></strong></dt>
<dd>Path to the data file.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>DataFrame containing the wavelength and intensity data.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@_add_base_read_path
@_check_extension(&#34;.txt&#34;)
def read_oceanoptics_data(self, filepath: str) -&gt; pd.DataFrame:
    &#34;&#34;&#34;Read spectrometer data from OceanOptics spectrometer.

    Args:
        filepath: Path to the data file.

    Returns:
        DataFrame containing the wavelength and intensity data.
    &#34;&#34;&#34;
    df = pd.read_csv(filepath, sep=&#34;\t&#34;, skiprows=14, names=[&#34;wavelength&#34;, &#34;intensity&#34;])
    return df</code></pre>
</details>
</dd>
<dt id="qudi_hira_analysis.IOHandler.read_pfeiffer_data"><code class="name flex">
<span>def <span class="ident">read_pfeiffer_data</span></span>(<span>self, filepath: pathlib.Path) ‑> pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Read data stored by Pfeiffer vacuum monitoring software.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>filepath</code></strong></dt>
<dd>Path to the text file.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>DataFrame containing the data.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@_add_base_read_path
@_check_extension(&#34;.txt&#34;)
def read_pfeiffer_data(self, filepath: Path) -&gt; pd.DataFrame:
    &#34;&#34;&#34;Read data stored by Pfeiffer vacuum monitoring software.

    Args:
        filepath: Path to the text file.

    Returns:
        DataFrame containing the data.
    &#34;&#34;&#34;
    # Extract rows including the header
    df = pd.read_csv(filepath, sep=&#34;\t&#34;, skiprows=[0, 2, 3, 4])
    # Combine data and time columns together
    df[&#34;Date&#34;] = df[&#34;Date&#34;] + &#34; &#34; + df[&#34;Time&#34;]
    df = df.drop(&#34;Time&#34;, axis=1)
    # Infer datetime format and convert to datetime objects
    df[&#34;Date&#34;] = pd.to_datetime(df[&#34;Date&#34;], infer_datetime_format=True)
    # Set datetime as index
    df = df.set_index(&#34;Date&#34;, drop=True)
    return df</code></pre>
</details>
</dd>
<dt id="qudi_hira_analysis.IOHandler.read_pixelscanner_data"><code class="name flex">
<span>def <span class="ident">read_pixelscanner_data</span></span>(<span>self, filepath: pathlib.Path) ‑> (<class 'pySPM.SPM.SPM_image'>, <class 'pySPM.SPM.SPM_image'>)</span>
</code></dt>
<dd>
<div class="desc"><p>Read data from a PixelScanner measurement.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>filepath</code></strong></dt>
<dd>Path to the data file.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Forward and backward scan data.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def read_pixelscanner_data(self, filepath: Path) -&gt; (pySPM.SPM_image, pySPM.SPM_image):
    &#34;&#34;&#34; Read data from a PixelScanner measurement.

    Args:
        filepath: Path to the data file.

    Returns:
        Forward and backward scan data.
    &#34;&#34;&#34;
    df = self.read_into_dataframe(filepath)
    num_pixels = int(np.sqrt(len(df) // 2))

    if num_pixels ** 2 != len(df) // 2:
        raise ValueError(&#34;Number of pixels does not match data length.&#34;)

    try:
        fwd, bwd = self.__get_forward_backward_counts(df[&#34;count_rates&#34;], num_pixels)
    except KeyError:
        try:
            fwd, bwd = self.__get_forward_backward_counts(df[&#34;Count Rates (cps)&#34;], num_pixels)
        except KeyError:
            # Support old data format
            fwd = df[&#34;forward (cps)&#34;].to_numpy().reshape(num_pixels, num_pixels)
            bwd = df[&#34;backward (cps)&#34;].to_numpy().reshape(num_pixels, num_pixels)

    fwd = pySPM.SPM_image(fwd, channel=&#34;Forward&#34;, _type=&#34;NV-PL&#34;)
    bwd = pySPM.SPM_image(bwd, channel=&#34;Backward&#34;, _type=&#34;NV-PL&#34;)
    return fwd, bwd</code></pre>
</details>
</dd>
<dt id="qudi_hira_analysis.IOHandler.read_pkl"><code class="name flex">
<span>def <span class="ident">read_pkl</span></span>(<span>self, filepath: pathlib.Path) ‑> dict</span>
</code></dt>
<dd>
<div class="desc"><p>Read pickle files into a dictionary.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@_add_base_read_path
@_check_extension(&#34;.pkl&#34;)
def read_pkl(self, filepath: Path) -&gt; dict:
    &#34;&#34;&#34; Read pickle files into a dictionary. &#34;&#34;&#34;
    with open(filepath, &#39;rb&#39;) as f:
        file = pickle.load(f)
    return file</code></pre>
</details>
</dd>
<dt id="qudi_hira_analysis.IOHandler.read_pys"><code class="name flex">
<span>def <span class="ident">read_pys</span></span>(<span>self, filepath: pathlib.Path) ‑> dict</span>
</code></dt>
<dd>
<div class="desc"><p>Read raw .pys data files into a dictionary.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@_add_base_read_path
@_check_extension(&#34;.pys&#34;)
def read_pys(self, filepath: Path) -&gt; dict:
    &#34;&#34;&#34; Read raw .pys data files into a dictionary. &#34;&#34;&#34;
    byte_dict = np.load(str(filepath), encoding=&#34;bytes&#34;, allow_pickle=True)
    # Convert byte string keys to normal strings
    return {key.decode(&#39;utf8&#39;): byte_dict.get(key) for key in byte_dict.keys()}</code></pre>
</details>
</dd>
<dt id="qudi_hira_analysis.IOHandler.read_qudi_parameters"><code class="name flex">
<span>def <span class="ident">read_qudi_parameters</span></span>(<span>self, filepath: pathlib.Path) ‑> dict</span>
</code></dt>
<dd>
<div class="desc"><p>Extract parameters from a qudi dat file.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>filepath</code></strong></dt>
<dd>Path to the qudi .dat file</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Dictionary of parameters</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@_add_base_read_path
@_check_extension(&#34;.dat&#34;)
def read_qudi_parameters(self, filepath: Path) -&gt; dict:
    &#34;&#34;&#34;Extract parameters from a qudi dat file.

    Args:
        filepath: Path to the qudi .dat file

    Returns:
        Dictionary of parameters
    &#34;&#34;&#34;
    params = {}
    with open(filepath) as file:
        for line in file:
            if line == &#39;#=====\n&#39;:
                break
            else:
                # noinspection PyBroadException
                try:
                    # Remove # from beginning of lines
                    line = line[1:]
                    if line.count(&#34;:&#34;) == 1:
                        # Add params to dictionary
                        label, value = line.split(&#34;:&#34;)
                        if value != &#34;\n&#34;:
                            params[label] = ast.literal_eval(inspect.cleandoc(value))
                    elif line.count(&#34;:&#34;) == 3:
                        # Handle files with timestamps in them
                        label = line.split(&#34;:&#34;)[0]
                        timestamp_str = &#34;&#34;.join(line.split(&#34;:&#34;)[1:]).strip()
                        datetime_str = datetime.datetime.strptime(timestamp_str, &#34;%d.%m.%Y %Hh%Mmin%Ss&#34;).replace(
                            tzinfo=datetime.timezone.utc)
                        params[label] = datetime_str
                except Exception as _:
                    pass
    return params</code></pre>
</details>
</dd>
<dt id="qudi_hira_analysis.IOHandler.save_df"><code class="name flex">
<span>def <span class="ident">save_df</span></span>(<span>self, filepath: pathlib.Path, df: pandas.core.frame.DataFrame)</span>
</code></dt>
<dd>
<div class="desc"><p>Save Dataframe as csv.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@_add_base_write_path
@_check_extension(&#34;.pys&#34;)
def save_df(self, filepath: Path, df: pd.DataFrame):
    &#34;&#34;&#34; Save Dataframe as csv. &#34;&#34;&#34;
    df.to_csv(filepath, sep=&#39;\t&#39;, encoding=&#39;utf-8&#39;)</code></pre>
</details>
</dd>
<dt id="qudi_hira_analysis.IOHandler.save_figures"><code class="name flex">
<span>def <span class="ident">save_figures</span></span>(<span>self, filepath: pathlib.Path, fig: matplotlib.figure.Figure, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Saves figures from matplotlib plot data.</p>
<p>By default, saves as jpg, png, pdf and svg.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>fig</code></strong></dt>
<dd>Matplotlib figure to save.</dd>
<dt><strong><code>filepath</code></strong></dt>
<dd>Name of figure to save.</dd>
<dt><strong><code>only_jpg</code></strong></dt>
<dd>If True, only save as jpg (default: False).</dd>
<dt><strong><code>only_pdf</code></strong></dt>
<dd>If True, only save as pdf (default: False).</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Keyword arguments passed to matplotlib.pyplot.savefig().</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@_add_base_write_path
def save_figures(self, filepath: Path, fig: plt.Figure, **kwargs):
    &#34;&#34;&#34;Saves figures from matplotlib plot data.

    By default, saves as jpg, png, pdf and svg.

    Args:
        fig: Matplotlib figure to save.
        filepath: Name of figure to save.
        only_jpg: If True, only save as jpg (default: False).
        only_pdf: If True, only save as pdf (default: False).
        **kwargs: Keyword arguments passed to matplotlib.pyplot.savefig().
    &#34;&#34;&#34;
    extensions = None
    if &#34;only_jpg&#34; in kwargs:
        if kwargs.get(&#34;only_jpg&#34;):
            extensions = [&#34;.jpg&#34;]
        kwargs.pop(&#34;only_jpg&#34;, None)
    elif &#34;only_pdf&#34; in kwargs:
        if kwargs.get(&#34;only_pdf&#34;):
            extensions = [&#34;.pdf&#34;]
        kwargs.pop(&#34;only_pdf&#34;, None)
    else:
        extensions = [&#34;.jpg&#34;, &#34;.pdf&#34;, &#34;.svg&#34;, &#34;.png&#34;]

    for ext in extensions:
        fig.savefig(filepath.with_suffix(ext), dpi=200, **kwargs)</code></pre>
</details>
</dd>
<dt id="qudi_hira_analysis.IOHandler.save_pkl"><code class="name flex">
<span>def <span class="ident">save_pkl</span></span>(<span>self, filepath: pathlib.Path, obj: object)</span>
</code></dt>
<dd>
<div class="desc"><p>Saves pickle files.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>filepath</code></strong></dt>
<dd>Path to the data file.</dd>
<dt><strong><code>obj</code></strong></dt>
<dd>Object to be saved.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@_add_base_write_path
@_check_extension(&#34;.pkl&#34;)
def save_pkl(self, filepath: Path, obj: object):
    &#34;&#34;&#34;Saves pickle files.

    Args:
        filepath: Path to the data file.
        obj: Object to be saved.
    &#34;&#34;&#34;
    with open(filepath, &#39;wb&#39;) as f:
        pickle.dump(obj, f)</code></pre>
</details>
</dd>
<dt id="qudi_hira_analysis.IOHandler.save_pys"><code class="name flex">
<span>def <span class="ident">save_pys</span></span>(<span>self, filepath: pathlib.Path, dictionary: dict)</span>
</code></dt>
<dd>
<div class="desc"><p>Saves .pys files.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>filepath</code></strong></dt>
<dd>Path to the data file.</dd>
<dt><strong><code>dictionary</code></strong></dt>
<dd>Dictionary to be saved.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@_add_base_write_path
@_check_extension(&#34;.pys&#34;)
def save_pys(self, filepath: Path, dictionary: dict):
    &#34;&#34;&#34;Saves .pys files.

    Args:
        filepath: Path to the data file.
        dictionary: Dictionary to be saved.
    &#34;&#34;&#34;
    with open(filepath, &#39;wb&#39;) as f:
        pickle.dump(dictionary, f, 1)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul>
<li><a href="#getting-started">Getting started</a></li>
<li><a href="#data-fitting">Data fitting</a></li>
<li><a href="#data-saving">Data saving</a></li>
<li><a href="#examples">Examples</a><ul>
<li><a href="#nv-odmr-map">NV-ODMR map</a></li>
<li><a href="#nv-pl-map">NV-PL map</a></li>
<li><a href="#nanonis-afm-measurements">Nanonis AFM measurements</a></li>
<li><a href="#g2-measurements-anti-bunching-fit">g(2) measurements (anti-bunching fit)</a></li>
<li><a href="#odmr-measurements-double-lorentzian-fit">ODMR measurements (double Lorentzian fit)</a></li>
<li><a href="#rabi-measurements-sine-exp-decay-fit">Rabi measurements (sine exp. decay fit)</a></li>
<li><a href="#temperature-measurements">Temperature measurements</a></li>
<li><a href="#bruker-mfm-measurements">Bruker MFM measurements</a></li>
<li><a href="#pys-data-pi3diamond-compatibility">PYS data (pi3diamond compatibility)</a></li>
</ul>
</li>
</ul>
</div>
<ul id="index">
<li><h3><a href="#header-submodules">Sub-modules</a></h3>
<ul>
<li><code><a title="qudi_hira_analysis.analysis_logic" href="analysis_logic.html">qudi_hira_analysis.analysis_logic</a></code></li>
<li><code><a title="qudi_hira_analysis.data_handler" href="data_handler.html">qudi_hira_analysis.data_handler</a></code></li>
<li><code><a title="qudi_hira_analysis.helper_functions" href="helper_functions.html">qudi_hira_analysis.helper_functions</a></code></li>
<li><code><a title="qudi_hira_analysis.io_handler" href="io_handler.html">qudi_hira_analysis.io_handler</a></code></li>
<li><code><a title="qudi_hira_analysis.measurement_dataclass" href="measurement_dataclass.html">qudi_hira_analysis.measurement_dataclass</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="qudi_hira_analysis.AnalysisLogic" href="#qudi_hira_analysis.AnalysisLogic">AnalysisLogic</a></code></h4>
<ul class="">
<li><code><a title="qudi_hira_analysis.AnalysisLogic.analyse_mean" href="#qudi_hira_analysis.AnalysisLogic.analyse_mean">analyse_mean</a></code></li>
<li><code><a title="qudi_hira_analysis.AnalysisLogic.analyse_mean_norm" href="#qudi_hira_analysis.AnalysisLogic.analyse_mean_norm">analyse_mean_norm</a></code></li>
<li><code><a title="qudi_hira_analysis.AnalysisLogic.analyse_mean_reference" href="#qudi_hira_analysis.AnalysisLogic.analyse_mean_reference">analyse_mean_reference</a></code></li>
<li><code><a title="qudi_hira_analysis.AnalysisLogic.analyze_mean" href="#qudi_hira_analysis.AnalysisLogic.analyze_mean">analyze_mean</a></code></li>
<li><code><a title="qudi_hira_analysis.AnalysisLogic.analyze_mean_norm" href="#qudi_hira_analysis.AnalysisLogic.analyze_mean_norm">analyze_mean_norm</a></code></li>
<li><code><a title="qudi_hira_analysis.AnalysisLogic.analyze_mean_reference" href="#qudi_hira_analysis.AnalysisLogic.analyze_mean_reference">analyze_mean_reference</a></code></li>
<li><code><a title="qudi_hira_analysis.AnalysisLogic.average_raster_odmr_pixels" href="#qudi_hira_analysis.AnalysisLogic.average_raster_odmr_pixels">average_raster_odmr_pixels</a></code></li>
<li><code><a title="qudi_hira_analysis.AnalysisLogic.fit" href="#qudi_hira_analysis.AnalysisLogic.fit">fit</a></code></li>
<li><code><a title="qudi_hira_analysis.AnalysisLogic.fit_function" href="#qudi_hira_analysis.AnalysisLogic.fit_function">fit_function</a></code></li>
<li><code><a title="qudi_hira_analysis.AnalysisLogic.fit_raster_odmr" href="#qudi_hira_analysis.AnalysisLogic.fit_raster_odmr">fit_raster_odmr</a></code></li>
<li><code><a title="qudi_hira_analysis.AnalysisLogic.get_all_fits" href="#qudi_hira_analysis.AnalysisLogic.get_all_fits">get_all_fits</a></code></li>
<li><code><a title="qudi_hira_analysis.AnalysisLogic.optimize_raster_odmr_params" href="#qudi_hira_analysis.AnalysisLogic.optimize_raster_odmr_params">optimize_raster_odmr_params</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="qudi_hira_analysis.DataHandler" href="#qudi_hira_analysis.DataHandler">DataHandler</a></code></h4>
<ul class="">
<li><code><a title="qudi_hira_analysis.DataHandler.data_folder_tree" href="#qudi_hira_analysis.DataHandler.data_folder_tree">data_folder_tree</a></code></li>
<li><code><a title="qudi_hira_analysis.DataHandler.figure_folder_tree" href="#qudi_hira_analysis.DataHandler.figure_folder_tree">figure_folder_tree</a></code></li>
<li><code><a title="qudi_hira_analysis.DataHandler.load_measurements" href="#qudi_hira_analysis.DataHandler.load_measurements">load_measurements</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="qudi_hira_analysis.FitMethodsAndEstimators" href="#qudi_hira_analysis.FitMethodsAndEstimators">FitMethodsAndEstimators</a></code></h4>
<ul class="">
<li><code><a title="qudi_hira_analysis.FitMethodsAndEstimators.antibunching" href="#qudi_hira_analysis.FitMethodsAndEstimators.antibunching">antibunching</a></code></li>
<li><code><a title="qudi_hira_analysis.FitMethodsAndEstimators.biexponential" href="#qudi_hira_analysis.FitMethodsAndEstimators.biexponential">biexponential</a></code></li>
<li><code><a title="qudi_hira_analysis.FitMethodsAndEstimators.decayexponential" href="#qudi_hira_analysis.FitMethodsAndEstimators.decayexponential">decayexponential</a></code></li>
<li><code><a title="qudi_hira_analysis.FitMethodsAndEstimators.decayexponentialstretched" href="#qudi_hira_analysis.FitMethodsAndEstimators.decayexponentialstretched">decayexponentialstretched</a></code></li>
<li><code><a title="qudi_hira_analysis.FitMethodsAndEstimators.gaussian" href="#qudi_hira_analysis.FitMethodsAndEstimators.gaussian">gaussian</a></code></li>
<li><code><a title="qudi_hira_analysis.FitMethodsAndEstimators.gaussiandouble" href="#qudi_hira_analysis.FitMethodsAndEstimators.gaussiandouble">gaussiandouble</a></code></li>
<li><code><a title="qudi_hira_analysis.FitMethodsAndEstimators.gaussianlinearoffset" href="#qudi_hira_analysis.FitMethodsAndEstimators.gaussianlinearoffset">gaussianlinearoffset</a></code></li>
<li><code><a title="qudi_hira_analysis.FitMethodsAndEstimators.hyperbolicsaturation" href="#qudi_hira_analysis.FitMethodsAndEstimators.hyperbolicsaturation">hyperbolicsaturation</a></code></li>
<li><code><a title="qudi_hira_analysis.FitMethodsAndEstimators.linear" href="#qudi_hira_analysis.FitMethodsAndEstimators.linear">linear</a></code></li>
<li><code><a title="qudi_hira_analysis.FitMethodsAndEstimators.lorentzian" href="#qudi_hira_analysis.FitMethodsAndEstimators.lorentzian">lorentzian</a></code></li>
<li><code><a title="qudi_hira_analysis.FitMethodsAndEstimators.lorentziandouble" href="#qudi_hira_analysis.FitMethodsAndEstimators.lorentziandouble">lorentziandouble</a></code></li>
<li><code><a title="qudi_hira_analysis.FitMethodsAndEstimators.lorentziantriple" href="#qudi_hira_analysis.FitMethodsAndEstimators.lorentziantriple">lorentziantriple</a></code></li>
<li><code><a title="qudi_hira_analysis.FitMethodsAndEstimators.sine" href="#qudi_hira_analysis.FitMethodsAndEstimators.sine">sine</a></code></li>
<li><code><a title="qudi_hira_analysis.FitMethodsAndEstimators.sinedouble" href="#qudi_hira_analysis.FitMethodsAndEstimators.sinedouble">sinedouble</a></code></li>
<li><code><a title="qudi_hira_analysis.FitMethodsAndEstimators.sinedoublewithexpdecay" href="#qudi_hira_analysis.FitMethodsAndEstimators.sinedoublewithexpdecay">sinedoublewithexpdecay</a></code></li>
<li><code><a title="qudi_hira_analysis.FitMethodsAndEstimators.sinedoublewithtwoexpdecay" href="#qudi_hira_analysis.FitMethodsAndEstimators.sinedoublewithtwoexpdecay">sinedoublewithtwoexpdecay</a></code></li>
<li><code><a title="qudi_hira_analysis.FitMethodsAndEstimators.sineexponentialdecay" href="#qudi_hira_analysis.FitMethodsAndEstimators.sineexponentialdecay">sineexponentialdecay</a></code></li>
<li><code><a title="qudi_hira_analysis.FitMethodsAndEstimators.sinestretchedexponentialdecay" href="#qudi_hira_analysis.FitMethodsAndEstimators.sinestretchedexponentialdecay">sinestretchedexponentialdecay</a></code></li>
<li><code><a title="qudi_hira_analysis.FitMethodsAndEstimators.sinetriple" href="#qudi_hira_analysis.FitMethodsAndEstimators.sinetriple">sinetriple</a></code></li>
<li><code><a title="qudi_hira_analysis.FitMethodsAndEstimators.sinetriplewithexpdecay" href="#qudi_hira_analysis.FitMethodsAndEstimators.sinetriplewithexpdecay">sinetriplewithexpdecay</a></code></li>
<li><code><a title="qudi_hira_analysis.FitMethodsAndEstimators.sinetriplewiththreeexpdecay" href="#qudi_hira_analysis.FitMethodsAndEstimators.sinetriplewiththreeexpdecay">sinetriplewiththreeexpdecay</a></code></li>
<li><code><a title="qudi_hira_analysis.FitMethodsAndEstimators.twoDgaussian" href="#qudi_hira_analysis.FitMethodsAndEstimators.twoDgaussian">twoDgaussian</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="qudi_hira_analysis.IOHandler" href="#qudi_hira_analysis.IOHandler">IOHandler</a></code></h4>
<ul class="">
<li><code><a title="qudi_hira_analysis.IOHandler.read_bruker_spm_data" href="#qudi_hira_analysis.IOHandler.read_bruker_spm_data">read_bruker_spm_data</a></code></li>
<li><code><a title="qudi_hira_analysis.IOHandler.read_confocal_into_dataframe" href="#qudi_hira_analysis.IOHandler.read_confocal_into_dataframe">read_confocal_into_dataframe</a></code></li>
<li><code><a title="qudi_hira_analysis.IOHandler.read_csv" href="#qudi_hira_analysis.IOHandler.read_csv">read_csv</a></code></li>
<li><code><a title="qudi_hira_analysis.IOHandler.read_excel" href="#qudi_hira_analysis.IOHandler.read_excel">read_excel</a></code></li>
<li><code><a title="qudi_hira_analysis.IOHandler.read_into_dataframe" href="#qudi_hira_analysis.IOHandler.read_into_dataframe">read_into_dataframe</a></code></li>
<li><code><a title="qudi_hira_analysis.IOHandler.read_into_ndarray" href="#qudi_hira_analysis.IOHandler.read_into_ndarray">read_into_ndarray</a></code></li>
<li><code><a title="qudi_hira_analysis.IOHandler.read_into_ndarray_transposed" href="#qudi_hira_analysis.IOHandler.read_into_ndarray_transposed">read_into_ndarray_transposed</a></code></li>
<li><code><a title="qudi_hira_analysis.IOHandler.read_lakeshore_data" href="#qudi_hira_analysis.IOHandler.read_lakeshore_data">read_lakeshore_data</a></code></li>
<li><code><a title="qudi_hira_analysis.IOHandler.read_nanonis_data" href="#qudi_hira_analysis.IOHandler.read_nanonis_data">read_nanonis_data</a></code></li>
<li><code><a title="qudi_hira_analysis.IOHandler.read_nanonis_parameters" href="#qudi_hira_analysis.IOHandler.read_nanonis_parameters">read_nanonis_parameters</a></code></li>
<li><code><a title="qudi_hira_analysis.IOHandler.read_nanonis_spm_data" href="#qudi_hira_analysis.IOHandler.read_nanonis_spm_data">read_nanonis_spm_data</a></code></li>
<li><code><a title="qudi_hira_analysis.IOHandler.read_oceanoptics_data" href="#qudi_hira_analysis.IOHandler.read_oceanoptics_data">read_oceanoptics_data</a></code></li>
<li><code><a title="qudi_hira_analysis.IOHandler.read_pfeiffer_data" href="#qudi_hira_analysis.IOHandler.read_pfeiffer_data">read_pfeiffer_data</a></code></li>
<li><code><a title="qudi_hira_analysis.IOHandler.read_pixelscanner_data" href="#qudi_hira_analysis.IOHandler.read_pixelscanner_data">read_pixelscanner_data</a></code></li>
<li><code><a title="qudi_hira_analysis.IOHandler.read_pkl" href="#qudi_hira_analysis.IOHandler.read_pkl">read_pkl</a></code></li>
<li><code><a title="qudi_hira_analysis.IOHandler.read_pys" href="#qudi_hira_analysis.IOHandler.read_pys">read_pys</a></code></li>
<li><code><a title="qudi_hira_analysis.IOHandler.read_qudi_parameters" href="#qudi_hira_analysis.IOHandler.read_qudi_parameters">read_qudi_parameters</a></code></li>
<li><code><a title="qudi_hira_analysis.IOHandler.save_df" href="#qudi_hira_analysis.IOHandler.save_df">save_df</a></code></li>
<li><code><a title="qudi_hira_analysis.IOHandler.save_figures" href="#qudi_hira_analysis.IOHandler.save_figures">save_figures</a></code></li>
<li><code><a title="qudi_hira_analysis.IOHandler.save_pkl" href="#qudi_hira_analysis.IOHandler.save_pkl">save_pkl</a></code></li>
<li><code><a title="qudi_hira_analysis.IOHandler.save_pys" href="#qudi_hira_analysis.IOHandler.save_pys">save_pys</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>